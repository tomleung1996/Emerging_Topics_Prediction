{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "## 仅供交叉验证 前馈神经网络（NNAR）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "from utils import *\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "random.seed(42)\n",
    "n_input = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the gene_editing array: (2643, 17, 10)\n",
      "Shape of the transplant array: (5141, 17, 10)\n"
     ]
    }
   ],
   "source": [
    "# gene_arr_path = r'../output/gene_editing/es_with_decay.array'\n",
    "# transplant_arr_path = r'../output/transplant/es_with_decay.array'\n",
    "\n",
    "# gene_arr = pickle.load(open(gene_arr_path, mode='rb'))\n",
    "# transplant_arr = pickle.load(open(transplant_arr_path, mode='rb'))\n",
    "\n",
    "# print('Shape of the gene_editing array:',gene_arr.shape)\n",
    "# print('Shape of the transplant array:',transplant_arr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 截断数据\n",
    "2019年为无效数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the gene_editing array: (2643, 16, 10)\n",
      "Shape of the transplant array: (5141, 16, 10)\n"
     ]
    }
   ],
   "source": [
    "# gene_arr = gene_arr[:, :-1, :]\n",
    "# transplant_arr = transplant_arr[:, :-1, :]\n",
    "\n",
    "# print('Shape of the gene_editing array:',gene_arr.shape)\n",
    "# print('Shape of the transplant array:',transplant_arr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 规范数据并获取5折交叉检验所需的训练集和验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler, data = scale_data(transplant_arr, 'standard')\n",
    "\n",
    "# # 用预测第二年的类别变量作为分成Kfold的依据，不支持浮点数\n",
    "# X, y, y_cat = data[:, :n_input, :], data[:, n_input:, -2],transplant_arr[:, n_input, -1]\n",
    "# kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return keras.backend.sqrt(keras.backend.mean(keras.backend.square(y_pred - y_true), axis=-1)) \n",
    "\n",
    "def build_direct_dnn_model():\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(5))\n",
    "    \n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-4)\n",
    "    model.compile(loss=root_mean_squared_error, optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 进行训练和评估\n",
    "使用EarlyStopping和Checkpoint做训练停止方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(X, y, y_cat, kfold, scaler):\n",
    "    overall_metrics = {\n",
    "        'mae':[],\n",
    "        'rmse':[],\n",
    "        'ndcg':[],\n",
    "        'mape':[],\n",
    "        'r2':[],\n",
    "        'pearson':[],\n",
    "        'acc':[]\n",
    "    }\n",
    "\n",
    "    annual_metrics = {\n",
    "        'mae':[],\n",
    "        'rmse':[],\n",
    "        'ndcg':[],\n",
    "        'mape':[],\n",
    "        'r2':[],\n",
    "        'pearson':[],\n",
    "        'acc':[]\n",
    "    }\n",
    "\n",
    "    for train, test in kfold.split(X, y_cat):\n",
    "        model = build_direct_dnn_model()\n",
    "        history = model.fit(X[train], y[train], epochs=100, batch_size=16, verbose=1, validation_data=(X[test], y[test]),\n",
    "                           callbacks=[\n",
    "                               EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='auto', restore_best_weights=True)\n",
    "                           ])\n",
    "\n",
    "        y_test = y[test]\n",
    "        y_pred = model.predict(X[test]).reshape(y[test].shape)\n",
    "\n",
    "        metrics = ['mae', 'rmse','ndcg', 'mape', 'r2', 'pearson', 'acc']\n",
    "        for m in metrics:\n",
    "            overall, annual = eval_model(m, y_test, y_pred, scaler)\n",
    "            overall_metrics[m].append(overall)\n",
    "            annual_metrics[m].append(annual)\n",
    "    \n",
    "    return overall_metrics, annual_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_pipeline():\n",
    "    gene_arr_path = r'../output/gene_editing/es_with_decay.array'\n",
    "    transplant_arr_path = r'../output/transplant/es_with_decay.array'\n",
    "\n",
    "    gene_arr = pickle.load(open(gene_arr_path, mode='rb'))\n",
    "    transplant_arr = pickle.load(open(transplant_arr_path, mode='rb'))\n",
    "    \n",
    "    gene_arr = gene_arr[:, :-1, :]\n",
    "    transplant_arr = transplant_arr[:, :-1, :]\n",
    "\n",
    "    print('Shape of the gene_editing array:',gene_arr.shape)\n",
    "    print('Shape of the transplant array:',transplant_arr.shape)\n",
    "    \n",
    "    metrics = {\n",
    "        'gene':{\n",
    "            'overall':{},\n",
    "            'annual':{}\n",
    "        },\n",
    "        'transplant':{\n",
    "            'overall':{},\n",
    "            'annual':{}\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for name, dataset in zip(['gene', 'transplant'], [gene_arr, transplant_arr]):\n",
    "        scaler, data = scale_data(dataset, 'standard')\n",
    "\n",
    "        # 用预测第二年的类别变量作为分成Kfold的依据，不支持浮点数\n",
    "        X, y, y_cat = data[:, :n_input, :], data[:, n_input:, -2], dataset[:, n_input, -1]\n",
    "        kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        \n",
    "        overall_metrics, annual_metrics = cross_validation(X, y, y_cat, kfold, scaler)\n",
    "        \n",
    "        for metric, value in overall_metrics.items():\n",
    "            metrics[name]['overall'][metric] = np.mean(value)\n",
    "        \n",
    "        for metric, value in annual_metrics.items():\n",
    "            metrics[name]['annual'][metric] = np.mean(np.array(value), axis=0)\n",
    "    \n",
    "    pickle.dump(metrics, open('mlp_metrics.dict', 'wb'))\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the gene_editing array: (2643, 16, 10)\n",
      "Shape of the transplant array: (5141, 16, 10)\n",
      "Train on 2112 samples, validate on 531 samples\n",
      "Epoch 1/100\n",
      "2112/2112 [==============================] - 1s 595us/sample - loss: 0.9757 - val_loss: 0.8962\n",
      "Epoch 2/100\n",
      "2112/2112 [==============================] - 0s 161us/sample - loss: 0.9062 - val_loss: 0.8870\n",
      "Epoch 3/100\n",
      "2112/2112 [==============================] - 0s 165us/sample - loss: 0.8989 - val_loss: 0.8839\n",
      "Epoch 4/100\n",
      "2112/2112 [==============================] - 0s 162us/sample - loss: 0.8955 - val_loss: 0.8843\n",
      "Epoch 5/100\n",
      "2112/2112 [==============================] - 0s 163us/sample - loss: 0.8923 - val_loss: 0.8842\n",
      "Epoch 6/100\n",
      "2112/2112 [==============================] - 0s 162us/sample - loss: 0.8901 - val_loss: 0.8838\n",
      "Epoch 7/100\n",
      "2112/2112 [==============================] - 0s 161us/sample - loss: 0.8883 - val_loss: 0.8837\n",
      "Epoch 8/100\n",
      "2112/2112 [==============================] - 0s 163us/sample - loss: 0.8862 - val_loss: 0.8845\n",
      "Epoch 9/100\n",
      "2112/2112 [==============================] - 0s 164us/sample - loss: 0.8844 - val_loss: 0.8817\n",
      "Epoch 10/100\n",
      "2112/2112 [==============================] - 0s 161us/sample - loss: 0.8823 - val_loss: 0.8830\n",
      "Epoch 11/100\n",
      "2112/2112 [==============================] - 0s 163us/sample - loss: 0.8808 - val_loss: 0.8824\n",
      "Epoch 12/100\n",
      "2112/2112 [==============================] - 0s 164us/sample - loss: 0.8792 - val_loss: 0.8829\n",
      "Epoch 13/100\n",
      "2112/2112 [==============================] - 0s 161us/sample - loss: 0.8778 - val_loss: 0.8825\n",
      "Epoch 14/100\n",
      "2112/2112 [==============================] - 0s 163us/sample - loss: 0.8761 - val_loss: 0.8832\n",
      "Epoch 15/100\n",
      "2112/2112 [==============================] - 0s 161us/sample - loss: 0.8746 - val_loss: 0.8826\n",
      "Epoch 16/100\n",
      "2112/2112 [==============================] - 0s 162us/sample - loss: 0.8744 - val_loss: 0.8806\n",
      "Epoch 17/100\n",
      "2112/2112 [==============================] - 0s 165us/sample - loss: 0.8718 - val_loss: 0.8813\n",
      "Epoch 18/100\n",
      "2112/2112 [==============================] - 0s 161us/sample - loss: 0.8705 - val_loss: 0.8811\n",
      "Epoch 19/100\n",
      "2112/2112 [==============================] - 0s 162us/sample - loss: 0.8689 - val_loss: 0.8815\n",
      "Epoch 20/100\n",
      "2112/2112 [==============================] - 0s 164us/sample - loss: 0.8679 - val_loss: 0.8839\n",
      "Epoch 21/100\n",
      "2112/2112 [==============================] - 0s 162us/sample - loss: 0.8667 - val_loss: 0.8814\n",
      "Epoch 22/100\n",
      "2112/2112 [==============================] - 0s 163us/sample - loss: 0.8649 - val_loss: 0.8816\n",
      "Epoch 23/100\n",
      "2112/2112 [==============================] - 0s 166us/sample - loss: 0.8639 - val_loss: 0.8814\n",
      "Epoch 24/100\n",
      "2112/2112 [==============================] - 0s 166us/sample - loss: 0.8623 - val_loss: 0.8806\n",
      "Epoch 25/100\n",
      "2112/2112 [==============================] - 0s 164us/sample - loss: 0.8612 - val_loss: 0.8808\n",
      "Epoch 26/100\n",
      "2112/2112 [==============================] - 0s 169us/sample - loss: 0.8600 - val_loss: 0.8791\n",
      "Epoch 27/100\n",
      "2112/2112 [==============================] - 0s 165us/sample - loss: 0.8589 - val_loss: 0.8820\n",
      "Epoch 28/100\n",
      "2112/2112 [==============================] - 0s 166us/sample - loss: 0.8582 - val_loss: 0.8795\n",
      "Epoch 29/100\n",
      "2112/2112 [==============================] - 0s 167us/sample - loss: 0.8561 - val_loss: 0.8804\n",
      "Epoch 30/100\n",
      "2112/2112 [==============================] - 0s 166us/sample - loss: 0.8549 - val_loss: 0.8796\n",
      "Epoch 31/100\n",
      "2112/2112 [==============================] - 0s 167us/sample - loss: 0.8543 - val_loss: 0.8799\n",
      "Epoch 32/100\n",
      "2112/2112 [==============================] - 0s 172us/sample - loss: 0.8526 - val_loss: 0.8778\n",
      "Epoch 33/100\n",
      "2112/2112 [==============================] - 0s 170us/sample - loss: 0.8514 - val_loss: 0.8810\n",
      "Epoch 34/100\n",
      "2112/2112 [==============================] - 0s 171us/sample - loss: 0.8510 - val_loss: 0.8767\n",
      "Epoch 35/100\n",
      "2112/2112 [==============================] - 0s 171us/sample - loss: 0.8496 - val_loss: 0.8788\n",
      "Epoch 36/100\n",
      "2112/2112 [==============================] - 0s 170us/sample - loss: 0.8485 - val_loss: 0.8799\n",
      "Epoch 37/100\n",
      "2112/2112 [==============================] - 0s 170us/sample - loss: 0.8470 - val_loss: 0.8795\n",
      "Epoch 38/100\n",
      "2112/2112 [==============================] - 0s 169us/sample - loss: 0.8459 - val_loss: 0.8821\n",
      "Epoch 39/100\n",
      "2112/2112 [==============================] - 0s 169us/sample - loss: 0.8449 - val_loss: 0.8783\n",
      "Epoch 40/100\n",
      "2112/2112 [==============================] - 0s 173us/sample - loss: 0.8454 - val_loss: 0.8802\n",
      "Epoch 41/100\n",
      "2112/2112 [==============================] - 0s 171us/sample - loss: 0.8434 - val_loss: 0.8808\n",
      "Epoch 42/100\n",
      "2112/2112 [==============================] - 0s 196us/sample - loss: 0.8432 - val_loss: 0.8803\n",
      "Epoch 43/100\n",
      "2112/2112 [==============================] - ETA: 0s - loss: 0.846 - 0s 204us/sample - loss: 0.8415 - val_loss: 0.8834\n",
      "Epoch 44/100\n",
      "2112/2112 [==============================] - 0s 206us/sample - loss: 0.8409 - val_loss: 0.8772\n",
      "Train on 2113 samples, validate on 530 samples\n",
      "Epoch 1/100\n",
      "2113/2113 [==============================] - 1s 521us/sample - loss: 0.9325 - val_loss: 0.9382\n",
      "Epoch 2/100\n",
      "2113/2113 [==============================] - 0s 213us/sample - loss: 0.8954 - val_loss: 0.9300\n",
      "Epoch 3/100\n",
      "2113/2113 [==============================] - 0s 220us/sample - loss: 0.8897 - val_loss: 0.9257\n",
      "Epoch 4/100\n",
      "2113/2113 [==============================] - 0s 213us/sample - loss: 0.8865 - val_loss: 0.9245\n",
      "Epoch 5/100\n",
      "2113/2113 [==============================] - 0s 216us/sample - loss: 0.8834 - val_loss: 0.9281\n",
      "Epoch 6/100\n",
      "2113/2113 [==============================] - 0s 215us/sample - loss: 0.8822 - val_loss: 0.9240\n",
      "Epoch 7/100\n",
      "2113/2113 [==============================] - 0s 215us/sample - loss: 0.8796 - val_loss: 0.9241\n",
      "Epoch 8/100\n",
      "2113/2113 [==============================] - 1s 244us/sample - loss: 0.8786 - val_loss: 0.9220\n",
      "Epoch 9/100\n",
      "2113/2113 [==============================] - 0s 214us/sample - loss: 0.8749 - val_loss: 0.9246\n",
      "Epoch 10/100\n",
      "2113/2113 [==============================] - 0s 219us/sample - loss: 0.8742 - val_loss: 0.9237\n",
      "Epoch 11/100\n",
      "2113/2113 [==============================] - 0s 213us/sample - loss: 0.8727 - val_loss: 0.9232\n",
      "Epoch 12/100\n",
      "2113/2113 [==============================] - 0s 214us/sample - loss: 0.8708 - val_loss: 0.9240\n",
      "Epoch 13/100\n",
      "2113/2113 [==============================] - 0s 216us/sample - loss: 0.8688 - val_loss: 0.9240\n",
      "Epoch 14/100\n",
      "2113/2113 [==============================] - 0s 217us/sample - loss: 0.8673 - val_loss: 0.9255\n",
      "Epoch 15/100\n",
      "2113/2113 [==============================] - 1s 237us/sample - loss: 0.8664 - val_loss: 0.9246\n",
      "Epoch 16/100\n",
      "2113/2113 [==============================] - 0s 218us/sample - loss: 0.8648 - val_loss: 0.9240\n",
      "Epoch 17/100\n",
      "2113/2113 [==============================] - 0s 214us/sample - loss: 0.8634 - val_loss: 0.9241\n",
      "Epoch 18/100\n",
      "2113/2113 [==============================] - 0s 219us/sample - loss: 0.8613 - val_loss: 0.9262\n",
      "Train on 2115 samples, validate on 528 samples\n",
      "Epoch 1/100\n",
      "2115/2115 [==============================] - 1s 555us/sample - loss: 0.9715 - val_loss: 0.9130\n",
      "Epoch 2/100\n",
      "2115/2115 [==============================] - 0s 212us/sample - loss: 0.9019 - val_loss: 0.9006\n",
      "Epoch 3/100\n",
      "2115/2115 [==============================] - 0s 212us/sample - loss: 0.8959 - val_loss: 0.8984\n",
      "Epoch 4/100\n",
      "2115/2115 [==============================] - 0s 213us/sample - loss: 0.8928 - val_loss: 0.8967\n",
      "Epoch 5/100\n",
      "2115/2115 [==============================] - 0s 211us/sample - loss: 0.8895 - val_loss: 0.8984\n",
      "Epoch 6/100\n",
      "2115/2115 [==============================] - 0s 210us/sample - loss: 0.8873 - val_loss: 0.8973\n",
      "Epoch 7/100\n",
      "2115/2115 [==============================] - 0s 212us/sample - loss: 0.8842 - val_loss: 0.8972\n",
      "Epoch 8/100\n",
      "2115/2115 [==============================] - 0s 215us/sample - loss: 0.8826 - val_loss: 0.8943\n",
      "Epoch 9/100\n",
      "2115/2115 [==============================] - 0s 211us/sample - loss: 0.8809 - val_loss: 0.8964\n",
      "Epoch 10/100\n",
      "2115/2115 [==============================] - 0s 210us/sample - loss: 0.8785 - val_loss: 0.8948\n",
      "Epoch 11/100\n",
      "2115/2115 [==============================] - 0s 210us/sample - loss: 0.8774 - val_loss: 0.8960\n",
      "Epoch 12/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2115/2115 [==============================] - 0s 207us/sample - loss: 0.8748 - val_loss: 0.8958\n",
      "Epoch 13/100\n",
      "2115/2115 [==============================] - 0s 207us/sample - loss: 0.8741 - val_loss: 0.8969\n",
      "Epoch 14/100\n",
      "2115/2115 [==============================] - 0s 203us/sample - loss: 0.8718 - val_loss: 0.8969\n",
      "Epoch 15/100\n",
      "2115/2115 [==============================] - 0s 204us/sample - loss: 0.8698 - val_loss: 0.8980\n",
      "Epoch 16/100\n",
      "2115/2115 [==============================] - 0s 202us/sample - loss: 0.8689 - val_loss: 0.8984\n",
      "Epoch 17/100\n",
      "2115/2115 [==============================] - 0s 203us/sample - loss: 0.8668 - val_loss: 0.8955\n",
      "Epoch 18/100\n",
      "2115/2115 [==============================] - 0s 206us/sample - loss: 0.8647 - val_loss: 0.9000\n",
      "Train on 2116 samples, validate on 527 samples\n",
      "Epoch 1/100\n",
      "2116/2116 [==============================] - 1s 509us/sample - loss: 0.9410 - val_loss: 0.9212\n",
      "Epoch 2/100\n",
      "2116/2116 [==============================] - 0s 204us/sample - loss: 0.8951 - val_loss: 0.9211\n",
      "Epoch 3/100\n",
      "2116/2116 [==============================] - 0s 204us/sample - loss: 0.8888 - val_loss: 0.9232\n",
      "Epoch 4/100\n",
      "2116/2116 [==============================] - 0s 206us/sample - loss: 0.8845 - val_loss: 0.9247\n",
      "Epoch 5/100\n",
      "2116/2116 [==============================] - 0s 207us/sample - loss: 0.8815 - val_loss: 0.9243\n",
      "Epoch 6/100\n",
      "2116/2116 [==============================] - 0s 205us/sample - loss: 0.8795 - val_loss: 0.9276\n",
      "Epoch 7/100\n",
      "2116/2116 [==============================] - 0s 205us/sample - loss: 0.8772 - val_loss: 0.9271\n",
      "Epoch 8/100\n",
      "2116/2116 [==============================] - 0s 205us/sample - loss: 0.8753 - val_loss: 0.9314\n",
      "Epoch 9/100\n",
      "2116/2116 [==============================] - 0s 204us/sample - loss: 0.8730 - val_loss: 0.9256\n",
      "Epoch 10/100\n",
      "2116/2116 [==============================] - 0s 205us/sample - loss: 0.8715 - val_loss: 0.9326\n",
      "Epoch 11/100\n",
      "2116/2116 [==============================] - 0s 204us/sample - loss: 0.8694 - val_loss: 0.9312\n",
      "Epoch 12/100\n",
      "2116/2116 [==============================] - 0s 207us/sample - loss: 0.8682 - val_loss: 0.9300\n",
      "Train on 2116 samples, validate on 527 samples\n",
      "Epoch 1/100\n",
      "2116/2116 [==============================] - 1s 564us/sample - loss: 0.9535 - val_loss: 0.8762\n",
      "Epoch 2/100\n",
      "2116/2116 [==============================] - 0s 211us/sample - loss: 0.9077 - val_loss: 0.8687\n",
      "Epoch 3/100\n",
      "2116/2116 [==============================] - 0s 208us/sample - loss: 0.9032 - val_loss: 0.8693\n",
      "Epoch 4/100\n",
      "2116/2116 [==============================] - 0s 210us/sample - loss: 0.9006 - val_loss: 0.8659\n",
      "Epoch 5/100\n",
      "2116/2116 [==============================] - 0s 209us/sample - loss: 0.8976 - val_loss: 0.8671\n",
      "Epoch 6/100\n",
      "2116/2116 [==============================] - 0s 210us/sample - loss: 0.8959 - val_loss: 0.8696\n",
      "Epoch 7/100\n",
      "2116/2116 [==============================] - 0s 210us/sample - loss: 0.8933 - val_loss: 0.8675\n",
      "Epoch 8/100\n",
      "2116/2116 [==============================] - 0s 207us/sample - loss: 0.8926 - val_loss: 0.8642\n",
      "Epoch 9/100\n",
      "2116/2116 [==============================] - 0s 211us/sample - loss: 0.8904 - val_loss: 0.8668\n",
      "Epoch 10/100\n",
      "2116/2116 [==============================] - 0s 233us/sample - loss: 0.8895 - val_loss: 0.8658\n",
      "Epoch 11/100\n",
      "2116/2116 [==============================] - 0s 208us/sample - loss: 0.8876 - val_loss: 0.8635\n",
      "Epoch 12/100\n",
      "2116/2116 [==============================] - 0s 209us/sample - loss: 0.8862 - val_loss: 0.8641\n",
      "Epoch 13/100\n",
      "2116/2116 [==============================] - 0s 209us/sample - loss: 0.8841 - val_loss: 0.8656\n",
      "Epoch 14/100\n",
      "2116/2116 [==============================] - 0s 210us/sample - loss: 0.8831 - val_loss: 0.8615\n",
      "Epoch 15/100\n",
      "2116/2116 [==============================] - 0s 210us/sample - loss: 0.8820 - val_loss: 0.8625\n",
      "Epoch 16/100\n",
      "2116/2116 [==============================] - 0s 209us/sample - loss: 0.8806 - val_loss: 0.8627\n",
      "Epoch 17/100\n",
      "2116/2116 [==============================] - 0s 212us/sample - loss: 0.8787 - val_loss: 0.8609\n",
      "Epoch 18/100\n",
      "2116/2116 [==============================] - 0s 211us/sample - loss: 0.8781 - val_loss: 0.8625\n",
      "Epoch 19/100\n",
      "2116/2116 [==============================] - 0s 209us/sample - loss: 0.8757 - val_loss: 0.8638\n",
      "Epoch 20/100\n",
      "2116/2116 [==============================] - 0s 208us/sample - loss: 0.8750 - val_loss: 0.8616\n",
      "Epoch 21/100\n",
      "2116/2116 [==============================] - 0s 211us/sample - loss: 0.8733 - val_loss: 0.8617\n",
      "Epoch 22/100\n",
      "2116/2116 [==============================] - 0s 208us/sample - loss: 0.8720 - val_loss: 0.8612\n",
      "Epoch 23/100\n",
      "2116/2116 [==============================] - 0s 208us/sample - loss: 0.8702 - val_loss: 0.8618\n",
      "Epoch 24/100\n",
      "2116/2116 [==============================] - 0s 210us/sample - loss: 0.8685 - val_loss: 0.8622\n",
      "Epoch 25/100\n",
      "2116/2116 [==============================] - 0s 210us/sample - loss: 0.8688 - val_loss: 0.8632\n",
      "Epoch 26/100\n",
      "2116/2116 [==============================] - 0s 209us/sample - loss: 0.8665 - val_loss: 0.8607\n",
      "Epoch 27/100\n",
      "2116/2116 [==============================] - 0s 208us/sample - loss: 0.8660 - val_loss: 0.8620\n",
      "Epoch 28/100\n",
      "2116/2116 [==============================] - 0s 219us/sample - loss: 0.8643 - val_loss: 0.8591\n",
      "Epoch 29/100\n",
      "2116/2116 [==============================] - 0s 212us/sample - loss: 0.8628 - val_loss: 0.8607\n",
      "Epoch 30/100\n",
      "2116/2116 [==============================] - 0s 220us/sample - loss: 0.8617 - val_loss: 0.8628\n",
      "Epoch 31/100\n",
      "2116/2116 [==============================] - 1s 238us/sample - loss: 0.8600 - val_loss: 0.8621\n",
      "Epoch 32/100\n",
      "2116/2116 [==============================] - 0s 222us/sample - loss: 0.8600 - val_loss: 0.8583\n",
      "Epoch 33/100\n",
      "2116/2116 [==============================] - 0s 213us/sample - loss: 0.8571 - val_loss: 0.8607\n",
      "Epoch 34/100\n",
      "2116/2116 [==============================] - 0s 212us/sample - loss: 0.8574 - val_loss: 0.8623\n",
      "Epoch 35/100\n",
      "2116/2116 [==============================] - 1s 238us/sample - loss: 0.8563 - val_loss: 0.8654\n",
      "Epoch 36/100\n",
      "2116/2116 [==============================] - 0s 211us/sample - loss: 0.8546 - val_loss: 0.8646\n",
      "Epoch 37/100\n",
      "2116/2116 [==============================] - 0s 214us/sample - loss: 0.8543 - val_loss: 0.8614\n",
      "Epoch 38/100\n",
      "2116/2116 [==============================] - 0s 211us/sample - loss: 0.8530 - val_loss: 0.8584\n",
      "Epoch 39/100\n",
      "2116/2116 [==============================] - 0s 212us/sample - loss: 0.8521 - val_loss: 0.8595\n",
      "Epoch 40/100\n",
      "2116/2116 [==============================] - 0s 210us/sample - loss: 0.8505 - val_loss: 0.8597\n",
      "Epoch 41/100\n",
      "2116/2116 [==============================] - 0s 210us/sample - loss: 0.8492 - val_loss: 0.8596\n",
      "Epoch 42/100\n",
      "2116/2116 [==============================] - 0s 212us/sample - loss: 0.8490 - val_loss: 0.8601\n",
      "Train on 4111 samples, validate on 1030 samples\n",
      "Epoch 1/100\n",
      "4111/4111 [==============================] - 1s 352us/sample - loss: 0.6486 - val_loss: 0.6165\n",
      "Epoch 2/100\n",
      "4111/4111 [==============================] - 1s 201us/sample - loss: 0.6074 - val_loss: 0.6191\n",
      "Epoch 3/100\n",
      "4111/4111 [==============================] - 1s 199us/sample - loss: 0.5965 - val_loss: 0.6079\n",
      "Epoch 4/100\n",
      "4111/4111 [==============================] - 1s 197us/sample - loss: 0.5899 - val_loss: 0.6087\n",
      "Epoch 5/100\n",
      "4111/4111 [==============================] - 1s 198us/sample - loss: 0.5838 - val_loss: 0.6085\n",
      "Epoch 6/100\n",
      "4111/4111 [==============================] - 1s 197us/sample - loss: 0.5809 - val_loss: 0.6070\n",
      "Epoch 7/100\n",
      "4111/4111 [==============================] - 1s 198us/sample - loss: 0.5755 - val_loss: 0.6105\n",
      "Epoch 8/100\n",
      "4111/4111 [==============================] - 1s 198us/sample - loss: 0.5717 - val_loss: 0.6084\n",
      "Epoch 9/100\n",
      "4111/4111 [==============================] - 1s 197us/sample - loss: 0.5693 - val_loss: 0.6079\n",
      "Epoch 10/100\n",
      "4111/4111 [==============================] - 1s 199us/sample - loss: 0.5657 - val_loss: 0.6085\n",
      "Epoch 11/100\n",
      "4111/4111 [==============================] - 1s 211us/sample - loss: 0.5636 - val_loss: 0.6112\n",
      "Epoch 12/100\n",
      "4111/4111 [==============================] - 1s 198us/sample - loss: 0.5594 - val_loss: 0.6137\n",
      "Epoch 13/100\n",
      "4111/4111 [==============================] - 1s 198us/sample - loss: 0.5570 - val_loss: 0.6060\n",
      "Epoch 14/100\n",
      "4111/4111 [==============================] - 1s 197us/sample - loss: 0.5546 - val_loss: 0.6060\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100\n",
      "4111/4111 [==============================] - 1s 199us/sample - loss: 0.5509 - val_loss: 0.6115\n",
      "Epoch 16/100\n",
      "4111/4111 [==============================] - 1s 197us/sample - loss: 0.5487 - val_loss: 0.6098\n",
      "Epoch 17/100\n",
      "4111/4111 [==============================] - 1s 195us/sample - loss: 0.5458 - val_loss: 0.6077\n",
      "Epoch 18/100\n",
      "4111/4111 [==============================] - 1s 196us/sample - loss: 0.5432 - val_loss: 0.6076\n",
      "Epoch 19/100\n",
      "4111/4111 [==============================] - 1s 195us/sample - loss: 0.5406 - val_loss: 0.6086\n",
      "Epoch 20/100\n",
      "4111/4111 [==============================] - 1s 195us/sample - loss: 0.5365 - val_loss: 0.6127\n",
      "Epoch 21/100\n",
      "4111/4111 [==============================] - 1s 197us/sample - loss: 0.5356 - val_loss: 0.6120\n",
      "Epoch 22/100\n",
      "4111/4111 [==============================] - 1s 196us/sample - loss: 0.5325 - val_loss: 0.6113\n",
      "Epoch 23/100\n",
      "4111/4111 [==============================] - 1s 197us/sample - loss: 0.5298 - val_loss: 0.6118\n",
      "Train on 4112 samples, validate on 1029 samples\n",
      "Epoch 1/100\n",
      "4112/4112 [==============================] - 1s 365us/sample - loss: 0.6401 - val_loss: 0.6151\n",
      "Epoch 2/100\n",
      "4112/4112 [==============================] - 1s 237us/sample - loss: 0.6034 - val_loss: 0.6121\n",
      "Epoch 3/100\n",
      "4112/4112 [==============================] - 1s 217us/sample - loss: 0.5942 - val_loss: 0.6053\n",
      "Epoch 4/100\n",
      "4112/4112 [==============================] - 1s 196us/sample - loss: 0.5876 - val_loss: 0.6057\n",
      "Epoch 5/100\n",
      "4112/4112 [==============================] - 1s 192us/sample - loss: 0.5827 - val_loss: 0.6054\n",
      "Epoch 6/100\n",
      "4112/4112 [==============================] - 1s 191us/sample - loss: 0.5781 - val_loss: 0.6042\n",
      "Epoch 7/100\n",
      "4112/4112 [==============================] - 1s 189us/sample - loss: 0.5735 - val_loss: 0.6067\n",
      "Epoch 8/100\n",
      "4112/4112 [==============================] - 1s 190us/sample - loss: 0.5700 - val_loss: 0.6027\n",
      "Epoch 9/100\n",
      "4112/4112 [==============================] - 1s 189us/sample - loss: 0.5659 - val_loss: 0.6040\n",
      "Epoch 10/100\n",
      "4112/4112 [==============================] - 1s 189us/sample - loss: 0.5636 - val_loss: 0.6037\n",
      "Epoch 11/100\n",
      "4112/4112 [==============================] - 1s 189us/sample - loss: 0.5603 - val_loss: 0.6039\n",
      "Epoch 12/100\n",
      "4112/4112 [==============================] - 1s 189us/sample - loss: 0.5565 - val_loss: 0.6068\n",
      "Epoch 13/100\n",
      "4112/4112 [==============================] - 1s 188us/sample - loss: 0.5535 - val_loss: 0.6034\n",
      "Epoch 14/100\n",
      "4112/4112 [==============================] - 1s 189us/sample - loss: 0.5499 - val_loss: 0.6037\n",
      "Epoch 15/100\n",
      "4112/4112 [==============================] - 1s 189us/sample - loss: 0.5468 - val_loss: 0.6045\n",
      "Epoch 16/100\n",
      "4112/4112 [==============================] - 1s 190us/sample - loss: 0.5443 - val_loss: 0.6047\n",
      "Epoch 17/100\n",
      "4112/4112 [==============================] - 1s 196us/sample - loss: 0.5429 - val_loss: 0.6054\n",
      "Epoch 18/100\n",
      "4112/4112 [==============================] - 1s 191us/sample - loss: 0.5385 - val_loss: 0.6064\n",
      "Train on 4112 samples, validate on 1029 samples\n",
      "Epoch 1/100\n",
      "4112/4112 [==============================] - 1s 360us/sample - loss: 0.6410 - val_loss: 0.6105\n",
      "Epoch 2/100\n",
      "4112/4112 [==============================] - 1s 191us/sample - loss: 0.6037 - val_loss: 0.6065\n",
      "Epoch 3/100\n",
      "4112/4112 [==============================] - 1s 193us/sample - loss: 0.5948 - val_loss: 0.6035\n",
      "Epoch 4/100\n",
      "4112/4112 [==============================] - 1s 191us/sample - loss: 0.5890 - val_loss: 0.6033\n",
      "Epoch 5/100\n",
      "4112/4112 [==============================] - 1s 191us/sample - loss: 0.5836 - val_loss: 0.6038\n",
      "Epoch 6/100\n",
      "4112/4112 [==============================] - 1s 191us/sample - loss: 0.5802 - val_loss: 0.6022\n",
      "Epoch 7/100\n",
      "4112/4112 [==============================] - 1s 205us/sample - loss: 0.5751 - val_loss: 0.6009\n",
      "Epoch 8/100\n",
      "4112/4112 [==============================] - 1s 192us/sample - loss: 0.5735 - val_loss: 0.6015\n",
      "Epoch 9/100\n",
      "4112/4112 [==============================] - 1s 192us/sample - loss: 0.5688 - val_loss: 0.6016\n",
      "Epoch 10/100\n",
      "4112/4112 [==============================] - 1s 191us/sample - loss: 0.5659 - val_loss: 0.6069\n",
      "Epoch 11/100\n",
      "4112/4112 [==============================] - 1s 192us/sample - loss: 0.5624 - val_loss: 0.6032\n",
      "Epoch 12/100\n",
      "4112/4112 [==============================] - 1s 191us/sample - loss: 0.5605 - val_loss: 0.6025\n",
      "Epoch 13/100\n",
      "4112/4112 [==============================] - 1s 191us/sample - loss: 0.5559 - val_loss: 0.6024\n",
      "Epoch 14/100\n",
      "4112/4112 [==============================] - 1s 192us/sample - loss: 0.5530 - val_loss: 0.6021\n",
      "Epoch 15/100\n",
      "4112/4112 [==============================] - 1s 190us/sample - loss: 0.5504 - val_loss: 0.6029\n",
      "Epoch 16/100\n",
      "4112/4112 [==============================] - 1s 191us/sample - loss: 0.5477 - val_loss: 0.6029\n",
      "Epoch 17/100\n",
      "4112/4112 [==============================] - 1s 198us/sample - loss: 0.5439 - val_loss: 0.6037\n",
      "Train on 4114 samples, validate on 1027 samples\n",
      "Epoch 1/100\n",
      "4114/4114 [==============================] - 2s 374us/sample - loss: 0.6468 - val_loss: 0.6094\n",
      "Epoch 2/100\n",
      "4114/4114 [==============================] - 1s 215us/sample - loss: 0.6101 - val_loss: 0.6079\n",
      "Epoch 3/100\n",
      "4114/4114 [==============================] - 1s 223us/sample - loss: 0.5983 - val_loss: 0.5961\n",
      "Epoch 4/100\n",
      "4114/4114 [==============================] - 1s 201us/sample - loss: 0.5914 - val_loss: 0.5984\n",
      "Epoch 5/100\n",
      "4114/4114 [==============================] - 1s 225us/sample - loss: 0.5867 - val_loss: 0.5984\n",
      "Epoch 6/100\n",
      "4114/4114 [==============================] - 1s 199us/sample - loss: 0.5824 - val_loss: 0.5948\n",
      "Epoch 7/100\n",
      "4114/4114 [==============================] - 1s 208us/sample - loss: 0.5795 - val_loss: 0.5951\n",
      "Epoch 8/100\n",
      "4114/4114 [==============================] - 1s 199us/sample - loss: 0.5765 - val_loss: 0.5977\n",
      "Epoch 9/100\n",
      "4114/4114 [==============================] - 1s 199us/sample - loss: 0.5714 - val_loss: 0.6003\n",
      "Epoch 10/100\n",
      "4114/4114 [==============================] - 1s 198us/sample - loss: 0.5676 - val_loss: 0.5973\n",
      "Epoch 11/100\n",
      "4114/4114 [==============================] - 1s 211us/sample - loss: 0.5659 - val_loss: 0.5916\n",
      "Epoch 12/100\n",
      "4114/4114 [==============================] - 1s 227us/sample - loss: 0.5621 - val_loss: 0.5994\n",
      "Epoch 13/100\n",
      "4114/4114 [==============================] - 1s 206us/sample - loss: 0.5595 - val_loss: 0.5923\n",
      "Epoch 14/100\n",
      "4114/4114 [==============================] - 1s 208us/sample - loss: 0.5557 - val_loss: 0.5958\n",
      "Epoch 15/100\n",
      "4114/4114 [==============================] - 1s 218us/sample - loss: 0.5538 - val_loss: 0.5953\n",
      "Epoch 16/100\n",
      "4114/4114 [==============================] - 1s 214us/sample - loss: 0.5507 - val_loss: 0.6002\n",
      "Epoch 17/100\n",
      "4114/4114 [==============================] - 1s 199us/sample - loss: 0.5479 - val_loss: 0.5955\n",
      "Epoch 18/100\n",
      "4114/4114 [==============================] - 1s 225us/sample - loss: 0.5441 - val_loss: 0.6007\n",
      "Epoch 19/100\n",
      "4114/4114 [==============================] - 1s 220us/sample - loss: 0.5430 - val_loss: 0.5965\n",
      "Epoch 20/100\n",
      "4114/4114 [==============================] - 1s 221us/sample - loss: 0.5402 - val_loss: 0.5959\n",
      "Epoch 21/100\n",
      "4114/4114 [==============================] - 1s 216us/sample - loss: 0.5374 - val_loss: 0.5975\n",
      "Train on 4115 samples, validate on 1026 samples\n",
      "Epoch 1/100\n",
      "4115/4115 [==============================] - 2s 381us/sample - loss: 0.6514 - val_loss: 0.6254\n",
      "Epoch 2/100\n",
      "4115/4115 [==============================] - 1s 206us/sample - loss: 0.6048 - val_loss: 0.6196\n",
      "Epoch 3/100\n",
      "4115/4115 [==============================] - 1s 212us/sample - loss: 0.5957 - val_loss: 0.6158\n",
      "Epoch 4/100\n",
      "4115/4115 [==============================] - 1s 206us/sample - loss: 0.5878 - val_loss: 0.6166\n",
      "Epoch 5/100\n",
      "4115/4115 [==============================] - 1s 226us/sample - loss: 0.5830 - val_loss: 0.6125\n",
      "Epoch 6/100\n",
      "4115/4115 [==============================] - 1s 218us/sample - loss: 0.5778 - val_loss: 0.6125\n",
      "Epoch 7/100\n",
      "4115/4115 [==============================] - 1s 214us/sample - loss: 0.5735 - val_loss: 0.6116\n",
      "Epoch 8/100\n",
      "4115/4115 [==============================] - 1s 216us/sample - loss: 0.5694 - val_loss: 0.6121\n",
      "Epoch 9/100\n",
      "4115/4115 [==============================] - 1s 217us/sample - loss: 0.5677 - val_loss: 0.6164\n",
      "Epoch 10/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4115/4115 [==============================] - 1s 204us/sample - loss: 0.5631 - val_loss: 0.6143\n",
      "Epoch 11/100\n",
      "4115/4115 [==============================] - 1s 216us/sample - loss: 0.5603 - val_loss: 0.6159\n",
      "Epoch 12/100\n",
      "4115/4115 [==============================] - 1s 215us/sample - loss: 0.5575 - val_loss: 0.6112\n",
      "Epoch 13/100\n",
      "4115/4115 [==============================] - 1s 202us/sample - loss: 0.5552 - val_loss: 0.6116\n",
      "Epoch 14/100\n",
      "4115/4115 [==============================] - 1s 225us/sample - loss: 0.5513 - val_loss: 0.6141\n",
      "Epoch 15/100\n",
      "4115/4115 [==============================] - 1s 205us/sample - loss: 0.5490 - val_loss: 0.6131\n",
      "Epoch 16/100\n",
      "4115/4115 [==============================] - 1s 210us/sample - loss: 0.5454 - val_loss: 0.6151\n",
      "Epoch 17/100\n",
      "4115/4115 [==============================] - 1s 223us/sample - loss: 0.5436 - val_loss: 0.6172\n",
      "Epoch 18/100\n",
      "4115/4115 [==============================] - 1s 214us/sample - loss: 0.5410 - val_loss: 0.6118\n",
      "Epoch 19/100\n",
      "4115/4115 [==============================] - 1s 201us/sample - loss: 0.5397 - val_loss: 0.6128\n",
      "Epoch 20/100\n",
      "4115/4115 [==============================] - 1s 206us/sample - loss: 0.5358 - val_loss: 0.6121\n",
      "Epoch 21/100\n",
      "4115/4115 [==============================] - 1s 218us/sample - loss: 0.5335 - val_loss: 0.6118\n",
      "Epoch 22/100\n",
      "4115/4115 [==============================] - 1s 202us/sample - loss: 0.5301 - val_loss: 0.6154\n"
     ]
    }
   ],
   "source": [
    "metrics = full_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gene': {'overall': {'mae': 0.7299490896746337,\n",
       "   'rmse': 1.2918968704150449,\n",
       "   'ndcg': 0.44835543717483367},\n",
       "  'annual': {'mae': array([0.41098335, 0.57326106, 0.75468637, 0.88329084, 1.02752384]),\n",
       "   'rmse': array([0.74612466, 0.97067574, 1.33166402, 1.48635163, 1.66849718]),\n",
       "   'ndcg': array([0.46505046, 0.36096419, 0.14908492, 0.19261638, 0.15743147])}},\n",
       " 'transplant': {'overall': {'mae': 0.7718043551135649,\n",
       "   'rmse': 1.2752554128409044,\n",
       "   'ndcg': 0.48122490028670706},\n",
       "  'annual': {'mae': array([0.75218725, 0.77410503, 0.74279975, 0.7759474 , 0.81398235]),\n",
       "   'rmse': array([1.30078556, 1.29846621, 1.21739863, 1.22831554, 1.31962679]),\n",
       "   'ndcg': array([0.09773368, 0.07247887, 0.02053076, 0.10604713, 0.11611737])}}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gene': {'overall': {'mae': 0.7299490896746337,\n",
       "   'rmse': 1.2918968704150449,\n",
       "   'ndcg': 0.44835543717483367,\n",
       "   'mape': 4.43813367441968,\n",
       "   'r2': 0.2366877334464476,\n",
       "   'pearson': 0.5213191822517468,\n",
       "   'acc': 0.34326412607594264},\n",
       "  'annual': {'mae': array([0.41098335, 0.57326106, 0.75468637, 0.88329084, 1.02752384]),\n",
       "   'rmse': array([0.74612466, 0.97067574, 1.33166402, 1.48635163, 1.66849718]),\n",
       "   'ndcg': array([0.46505046, 0.36096419, 0.14908492, 0.19261638, 0.15743147]),\n",
       "   'mape': array([3.48005609, 2.8328771 , 3.14869389, 6.23453968, 6.49450161]),\n",
       "   'r2': array([0.43897391, 0.27529658, 0.16778358, 0.05561811, 0.01792775]),\n",
       "   'pearson': array([0.67472678, 0.56063183, 0.46457533, 0.32847791, 0.26389998]),\n",
       "   'acc': array([0.60726591, 0.22830786, 0.23867794, 0.33481486, 0.30725406])}},\n",
       " 'transplant': {'overall': {'mae': 0.7718043551135649,\n",
       "   'rmse': 1.2752554128409044,\n",
       "   'ndcg': 0.48122490028670706,\n",
       "   'mape': 3.659302529390196,\n",
       "   'r2': 0.4193337345315111,\n",
       "   'pearson': 0.6565445696067178,\n",
       "   'acc': 0.33313833122132325},\n",
       "  'annual': {'mae': array([0.75218725, 0.77410503, 0.74279975, 0.7759474 , 0.81398235]),\n",
       "   'rmse': array([1.30078556, 1.29846621, 1.21739863, 1.22831554, 1.31962679]),\n",
       "   'ndcg': array([0.09773368, 0.07247887, 0.02053076, 0.10604713, 0.11611737]),\n",
       "   'mape': array([3.21496512, 3.61200224, 3.0389909 , 5.03109506, 3.39945933]),\n",
       "   'r2': array([0.42932613, 0.41840298, 0.40859382, 0.44062724, 0.40219277]),\n",
       "   'pearson': array([0.66477248, 0.65548223, 0.64643488, 0.67638026, 0.64780072]),\n",
       "   'acc': array([0.35187322, 0.33807755, 0.33828895, 0.32387197, 0.31357996])}}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
