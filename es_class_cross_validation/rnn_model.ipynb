{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "## 仅供交叉验证 Encoder-Decoder with LSTM cell-按总量分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from utils import *\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "random.seed(42)\n",
    "n_input = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the gene_editing array: (2643, 17, 10)\n",
      "Shape of the transplant array: (5141, 17, 10)\n"
     ]
    }
   ],
   "source": [
    "# gene_arr_path = r'../output/gene_editing/es_with_decay.array'\n",
    "# transplant_arr_path = r'../output/transplant/es_with_decay.array'\n",
    "\n",
    "# gene_arr = pickle.load(open(gene_arr_path, mode='rb'))\n",
    "# transplant_arr = pickle.load(open(transplant_arr_path, mode='rb'))\n",
    "\n",
    "# print('Shape of the gene_editing array:',gene_arr.shape)\n",
    "# print('Shape of the transplant array:',transplant_arr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 截断数据\n",
    "2019年为无效数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the gene_editing array: (2643, 16, 10)\n",
      "Shape of the transplant array: (5141, 16, 10)\n"
     ]
    }
   ],
   "source": [
    "# gene_arr = gene_arr[:, :-1, :]\n",
    "# transplant_arr = transplant_arr[:, :-1, :]\n",
    "\n",
    "# print('Shape of the gene_editing array:',gene_arr.shape)\n",
    "# print('Shape of the transplant array:',transplant_arr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 规范数据并获取5折交叉检验所需的训练集和验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler, data = scale_data(transplant_arr, 'standard')\n",
    "\n",
    "# # 用预测第二年的类别变量作为分成Kfold的依据，不支持浮点数\n",
    "# X, y, y_cat = data[:, :n_input, :], data[:, n_input:, -2],transplant_arr[:, n_input, -1]\n",
    "# kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 按总量切分数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_by_es(data, targets):\n",
    "    total_es = np.sum(data[:, :11, -2], axis=1)\n",
    "    sorted_index = np.argsort(total_es)\n",
    "    group_size = len(total_es) // 3\n",
    "    \n",
    "    data1, target1 = data[sorted_index[:group_size]], targets[sorted_index[:group_size]]\n",
    "    data2, target2 = data[sorted_index[group_size:2*group_size]], targets[sorted_index[group_size:2*group_size]]\n",
    "    data3, target3 = data[sorted_index[2*group_size:]], targets[sorted_index[2*group_size:]]\n",
    "    \n",
    "    return data1, target1, data2, target2, data3, target3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return keras.backend.sqrt(keras.backend.mean(keras.backend.square(y_pred - y_true), axis=-1)) \n",
    "\n",
    "# def build_encoder_decoder_model(lstm_units, dense_units, lr=1e-4):\n",
    "#     model = keras.models.Sequential()\n",
    "#     model.add(LSTM(lstm_units, activation='tanh', input_shape=(11, 10), return_sequences=True))\n",
    "#     model.add(LSTM(lstm_units, activation='tanh'))\n",
    "#     model.add(RepeatVector(5))\n",
    "#     model.add(LSTM(lstm_units, activation='tanh', return_sequences=True))\n",
    "#     model.add(LSTM(lstm_units, activation='tanh', return_sequences=True))\n",
    "#     model.add(TimeDistributed(Dense(dense_units, activation='relu')))\n",
    "#     model.add(TimeDistributed(Dense(1)))\n",
    "    \n",
    "#     optimizer=keras.optimizers.Adam(learning_rate=lr)\n",
    "#     model.compile(loss=root_mean_squared_error, optimizer=optimizer)\n",
    "#     return model\n",
    "\n",
    "def build_encoder_decoder_model(n_layers=2, n_units=256, lr=1e-4):\n",
    "    model = keras.models.Sequential()\n",
    "    if n_layers == 1:\n",
    "        model.add(LSTM(n_units, activation='tanh', input_shape=(11, 10)))\n",
    "    elif n_layers == 2:\n",
    "        model.add(LSTM(n_units, activation='tanh', input_shape=(11, 10), return_sequences=True))\n",
    "        model.add(LSTM(n_units, activation='tanh'))\n",
    "    if n_layers > 2:\n",
    "        for i in range(n_layers-1):\n",
    "            model.add(LSTM(n_units, activation='tanh', return_sequences=True))\n",
    "        model.add(LSTM(n_units, activation='tanh'))\n",
    "    model.add(RepeatVector(5))\n",
    "\n",
    "    for i in range(n_layers):\n",
    "        model.add(LSTM(n_units, activation='tanh', return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(n_units, activation='relu')))\n",
    "    model.add(TimeDistributed(Dense(1)))\n",
    "    \n",
    "    optimizer=keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(loss=root_mean_squared_error, optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 进行训练和评估\n",
    "使用EarlyStopping和Checkpoint做训练停止方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(X, y, y_cat, kfold, scaler, n_layers, n_units):\n",
    "    overall_metrics = {\n",
    "        'mae':[],\n",
    "        'rmse':[],\n",
    "        'ndcg':[],\n",
    "        'mape':[],\n",
    "        'r2':[],\n",
    "        'pearson':[],\n",
    "        'acc':[]\n",
    "    }\n",
    "\n",
    "    annual_metrics = {\n",
    "        'mae':[],\n",
    "        'rmse':[],\n",
    "        'ndcg':[],\n",
    "        'mape':[],\n",
    "        'r2':[],\n",
    "        'pearson':[],\n",
    "        'acc':[]\n",
    "    }\n",
    "    \n",
    "    for train, test in kfold.split(X, y_cat):\n",
    "        X_train = X[train]\n",
    "        y_train = y[train]\n",
    "        X_test = X[test]\n",
    "        y_test = y[test]\n",
    "        models = []\n",
    "        \n",
    "        # 按总量划分数据集\n",
    "        X_train1, y_train1, X_train2, y_train2, X_train3, y_train3 = split_data_by_es(X_train, y_train)\n",
    "        train_xs = [X_train1, X_train2, X_train3]\n",
    "        train_ys = [y_train1, y_train2, y_train3]\n",
    "        \n",
    "        X_test1, y_test1, X_test2, y_test2, X_test3, y_test3 = split_data_by_es(X_test, y_test)\n",
    "        test_xs = [X_test1, X_test2, X_test3]\n",
    "        test_ys = [y_test1, y_test2, y_test3]\n",
    "        i_s = [1, 2, 3]\n",
    "        \n",
    "        # 训练\n",
    "        for i in range(len(i_s)):\n",
    "            model = build_encoder_decoder_model(n_layers, n_units, 1e-4)\n",
    "            history = model.fit(train_xs[i], train_ys[i], epochs=300, batch_size=16, verbose=1, validation_data=(test_xs[i], test_ys[i]),\n",
    "                           callbacks=[\n",
    "                               EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='auto', restore_best_weights=True)\n",
    "                           ])\n",
    "            models.append(model)\n",
    "        \n",
    "        # 预测\n",
    "        y_test = []\n",
    "        y_pred = []\n",
    "        for i in range(len(i_s)):\n",
    "            y_test.append(test_ys[i])\n",
    "            y_pred.append(models[i].predict(test_xs[i]).reshape(test_ys[i].shape))\n",
    "        \n",
    "        y_test = np.concatenate(y_test)\n",
    "        y_pred = np.concatenate(y_pred)\n",
    "\n",
    "        metrics = ['mae', 'rmse','ndcg', 'mape', 'r2', 'pearson', 'acc']\n",
    "        for m in metrics:\n",
    "            overall, annual = eval_model(m, y_test, y_pred, scaler)\n",
    "            overall_metrics[m].append(overall)\n",
    "            annual_metrics[m].append(annual)\n",
    "    \n",
    "    return overall_metrics, annual_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_pipeline():\n",
    "    gene_arr_path = r'../output/gene_editing/es_with_decay.array'\n",
    "    transplant_arr_path = r'../output/transplant/es_with_decay.array'\n",
    "\n",
    "    gene_arr = pickle.load(open(gene_arr_path, mode='rb'))\n",
    "    transplant_arr = pickle.load(open(transplant_arr_path, mode='rb'))\n",
    "    \n",
    "    gene_arr = gene_arr[:, :-1, :]\n",
    "    transplant_arr = transplant_arr[:, :-1, :]\n",
    "\n",
    "    print('Shape of the gene_editing array:',gene_arr.shape)\n",
    "    print('Shape of the transplant array:',transplant_arr.shape)\n",
    "    \n",
    "    metrics = {\n",
    "        'gene':{\n",
    "            'overall':{},\n",
    "            'annual':{}\n",
    "        },\n",
    "        'transplant':{\n",
    "            'overall':{},\n",
    "            'annual':{}\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for name, dataset in zip(['gene', 'transplant'], [gene_arr, transplant_arr]):\n",
    "        scaler, data = scale_data(dataset, 'standard')\n",
    "\n",
    "        # 用预测第二年的类别变量作为分成Kfold的依据，不支持浮点数\n",
    "        X, y, y_cat = data[:, :n_input, :], data[:, n_input:, -2], dataset[:, n_input, -1]\n",
    "        kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        \n",
    "#         overall_metrics, annual_metrics = cross_validation(X, y, y_cat, kfold, scaler)\n",
    "        if name == 'gene':\n",
    "            overall_metrics, annual_metrics = cross_validation(X, y, y_cat, kfold, scaler, 5, 512)\n",
    "        elif name == 'transplant':\n",
    "            overall_metrics, annual_metrics = cross_validation(X, y, y_cat, kfold, scaler, 1, 512)\n",
    "            \n",
    "        for metric, value in overall_metrics.items():\n",
    "            metrics[name]['overall'][metric] = np.mean(value)\n",
    "        \n",
    "        for metric, value in annual_metrics.items():\n",
    "            metrics[name]['annual'][metric] = np.mean(np.array(value), axis=0)\n",
    "    \n",
    "    pickle.dump(metrics, open('rnn_metrics.dict', 'wb'))\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the gene_editing array: (2643, 16, 10)\n",
      "Shape of the transplant array: (5141, 16, 10)\n",
      "Train on 704 samples, validate on 177 samples\n",
      "Epoch 1/300\n",
      "320/704 [============>.................] - ETA: 46s - loss: 0.9912WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-054a19eaaa18>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmetrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfull_pipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-2be3156c05be>\u001b[0m in \u001b[0;36mfull_pipeline\u001b[1;34m()\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;31m#         overall_metrics, annual_metrics = cross_validation(X, y, y_cat, kfold, scaler)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'gene'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m             \u001b[0moverall_metrics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mannual_metrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_validation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_cat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkfold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m512\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'transplant'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[0moverall_metrics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mannual_metrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_validation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_cat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkfold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m512\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-a5e5561df57c>\u001b[0m in \u001b[0;36mcross_validation\u001b[1;34m(X, y, y_cat, kfold, scaler, n_layers, n_units)\u001b[0m\n\u001b[0;32m     42\u001b[0m             history = model.fit(train_xs[i], train_ys[i], epochs=300, batch_size=16, verbose=1, validation_data=(test_xs[i], test_ys[i]),\n\u001b[0;32m     43\u001b[0m                            callbacks=[\n\u001b[1;32m---> 44\u001b[1;33m                                \u001b[0mEarlyStopping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'auto'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrestore_best_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m                            ])\n\u001b[0;32m     46\u001b[0m             \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 728\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    122\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 86\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    485\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 487\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    488\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    489\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1821\u001b[0m     \u001b[1;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1823\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1824\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1825\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1141\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1143\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1222\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[1;32m-> 1224\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1225\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 511\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    512\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "metrics = full_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gene': {'overall': {'mae': 0.730133398463071,\n",
       "   'rmse': 1.2883865069292366,\n",
       "   'ndcg': 0.4595261782617205,\n",
       "   'mape': 4.236438889071694,\n",
       "   'r2': 0.24111341230011735,\n",
       "   'pearson': 0.5244351917116782,\n",
       "   'acc': 0.3585302695937836},\n",
       "  'annual': {'mae': array([0.4229301 , 0.57894903, 0.7480632 , 0.88135736, 1.0193673 ]),\n",
       "   'rmse': array([0.77587887, 0.97472237, 1.31649913, 1.47399919, 1.66308958]),\n",
       "   'ndcg': array([0.51792661, 0.46710419, 0.16252051, 0.27382767, 0.1715413 ]),\n",
       "   'mape': array([3.00807798, 2.71817289, 3.09897554, 5.97081394, 6.3861541 ]),\n",
       "   'r2': array([0.39648012, 0.27090543, 0.18840093, 0.06966091, 0.02438854]),\n",
       "   'pearson': array([0.65189546, 0.55748179, 0.47756879, 0.33801751, 0.28149024]),\n",
       "   'acc': array([0.61641129, 0.2852523 , 0.24175685, 0.33479559, 0.31443532])}},\n",
       " 'transplant': {'overall': {'mae': 0.758002721263924,\n",
       "   'rmse': 1.2550889729706527,\n",
       "   'ndcg': 0.5085506950352213,\n",
       "   'mape': 3.6212566596977545,\n",
       "   'r2': 0.4375413865835555,\n",
       "   'pearson': 0.6694910991705629,\n",
       "   'acc': 0.3430393232493611},\n",
       "  'annual': {'mae': array([0.75363844, 0.76184934, 0.72673293, 0.75062862, 0.79716427]),\n",
       "   'rmse': array([1.29663055, 1.28344223, 1.18514967, 1.19959487, 1.29548363]),\n",
       "   'ndcg': array([0.02790316, 0.01808545, 0.02161034, 0.07049631, 0.05358012]),\n",
       "   'mape': array([3.36047711, 3.76802715, 3.07736233, 4.52672757, 3.37368914]),\n",
       "   'r2': array([0.433252  , 0.43173871, 0.43978169, 0.46714086, 0.42351524]),\n",
       "   'pearson': array([0.66462342, 0.66477923, 0.66877474, 0.6943663 , 0.66214033]),\n",
       "   'acc': array([0.34175399, 0.34700181, 0.35498698, 0.34447452, 0.32697932])}}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mae': [0.7593994116060747,\n",
       "  0.7671028942138409,\n",
       "  0.7561637531690426,\n",
       "  0.7418184505992778,\n",
       "  0.754409714862091],\n",
       " 'rmse': [1.2462465377385148,\n",
       "  1.2449577006458816,\n",
       "  1.2305761200091225,\n",
       "  1.2152881768708477,\n",
       "  1.3016089605016066],\n",
       " 'ndcg': [0.12383840192943663,\n",
       "  0.8889930008127779,\n",
       "  0.5801052823760275,\n",
       "  0.7835049919448109,\n",
       "  0.07865601010951555]}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
