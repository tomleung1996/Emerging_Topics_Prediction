{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "## 仅供交叉验证 前馈神经网络（NNAR）-按总量分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "from utils import *\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "random.seed(42)\n",
    "n_input = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the gene_editing array: (2643, 17, 10)\n",
      "Shape of the transplant array: (5141, 17, 10)\n"
     ]
    }
   ],
   "source": [
    "# gene_arr_path = r'../output/gene_editing/es_with_decay.array'\n",
    "# transplant_arr_path = r'../output/transplant/es_with_decay.array'\n",
    "\n",
    "# gene_arr = pickle.load(open(gene_arr_path, mode='rb'))\n",
    "# transplant_arr = pickle.load(open(transplant_arr_path, mode='rb'))\n",
    "\n",
    "# print('Shape of the gene_editing array:',gene_arr.shape)\n",
    "# print('Shape of the transplant array:',transplant_arr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 截断数据\n",
    "2019年为无效数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the gene_editing array: (2643, 16, 10)\n",
      "Shape of the transplant array: (5141, 16, 10)\n"
     ]
    }
   ],
   "source": [
    "# gene_arr = gene_arr[:, :-1, :]\n",
    "# transplant_arr = transplant_arr[:, :-1, :]\n",
    "\n",
    "# print('Shape of the gene_editing array:',gene_arr.shape)\n",
    "# print('Shape of the transplant array:',transplant_arr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 规范数据并获取5折交叉检验所需的训练集和验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler, data = scale_data(transplant_arr, 'standard')\n",
    "\n",
    "# # 用预测第二年的类别变量作为分成Kfold的依据，不支持浮点数\n",
    "# X, y, y_cat = data[:, :n_input, :], data[:, n_input:, -2],transplant_arr[:, n_input, -1]\n",
    "# kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 按总量切分数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_by_es(data, targets):\n",
    "    total_es = np.sum(data[:, :11, -2], axis=1)\n",
    "    sorted_index = np.argsort(total_es)\n",
    "    group_size = len(total_es) // 3\n",
    "    \n",
    "    data1, target1 = data[sorted_index[:group_size]], targets[sorted_index[:group_size]]\n",
    "    data2, target2 = data[sorted_index[group_size:2*group_size]], targets[sorted_index[group_size:2*group_size]]\n",
    "    data3, target3 = data[sorted_index[2*group_size:]], targets[sorted_index[2*group_size:]]\n",
    "    \n",
    "    return data1, target1, data2, target2, data3, target3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return keras.backend.sqrt(keras.backend.mean(keras.backend.square(y_pred - y_true), axis=-1)) \n",
    "\n",
    "# def build_direct_dnn_model():\n",
    "#     model = keras.models.Sequential()\n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dense(256, activation='relu'))\n",
    "#     model.add(Dense(256, activation='relu'))\n",
    "#     model.add(Dense(5))\n",
    "    \n",
    "#     optimizer=keras.optimizers.Adam(learning_rate=1e-4)\n",
    "#     model.compile(loss=root_mean_squared_error, optimizer=optimizer)\n",
    "#     return model\n",
    "def build_direct_dnn_model(n_layers=2, n_units=256):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(Flatten())\n",
    "    for i in range(n_layers):\n",
    "        model.add(Dense(n_units, activation='relu'))\n",
    "    model.add(Dense(5))\n",
    "    \n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-4)\n",
    "    model.compile(loss=root_mean_squared_error, optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 进行训练和评估\n",
    "使用EarlyStopping和Checkpoint做训练停止方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(X, y, y_cat, kfold, scaler, n_layers, n_units):\n",
    "    overall_metrics = {\n",
    "        'mae':[],\n",
    "        'rmse':[],\n",
    "        'ndcg':[],\n",
    "        'mape':[],\n",
    "        'r2':[],\n",
    "        'pearson':[],\n",
    "        'acc':[]\n",
    "    }\n",
    "\n",
    "    annual_metrics = {\n",
    "        'mae':[],\n",
    "        'rmse':[],\n",
    "        'ndcg':[],\n",
    "        'mape':[],\n",
    "        'r2':[],\n",
    "        'pearson':[],\n",
    "        'acc':[]\n",
    "    }\n",
    "    \n",
    "    for train, test in kfold.split(X, y_cat):\n",
    "        X_train = X[train]\n",
    "        y_train = y[train]\n",
    "        X_test = X[test]\n",
    "        y_test = y[test]\n",
    "        models = []\n",
    "        \n",
    "        # 按总量划分数据集\n",
    "        X_train1, y_train1, X_train2, y_train2, X_train3, y_train3 = split_data_by_es(X_train, y_train)\n",
    "        train_xs = [X_train1, X_train2, X_train3]\n",
    "        train_ys = [y_train1, y_train2, y_train3]\n",
    "        \n",
    "        X_test1, y_test1, X_test2, y_test2, X_test3, y_test3 = split_data_by_es(X_test, y_test)\n",
    "        test_xs = [X_test1, X_test2, X_test3]\n",
    "        test_ys = [y_test1, y_test2, y_test3]\n",
    "        i_s = [1, 2, 3]\n",
    "        \n",
    "        # 训练\n",
    "        for i in range(len(i_s)):\n",
    "            model = build_direct_dnn_model(n_layers, n_units)\n",
    "            history = model.fit(train_xs[i], train_ys[i], epochs=300, batch_size=16, verbose=1, validation_data=(test_xs[i], test_ys[i]),\n",
    "                           callbacks=[\n",
    "                               EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='auto', restore_best_weights=True)\n",
    "                           ])\n",
    "            models.append(model)\n",
    "        \n",
    "        # 预测\n",
    "        y_test = []\n",
    "        y_pred = []\n",
    "        for i in range(len(i_s)):\n",
    "            y_test.append(test_ys[i])\n",
    "            y_pred.append(models[i].predict(test_xs[i]).reshape(test_ys[i].shape))\n",
    "        \n",
    "        y_test = np.concatenate(y_test)\n",
    "        y_pred = np.concatenate(y_pred)\n",
    "\n",
    "        metrics = ['mae', 'rmse','ndcg', 'mape', 'r2', 'pearson', 'acc']\n",
    "        for m in metrics:\n",
    "            overall, annual = eval_model(m, y_test, y_pred, scaler)\n",
    "            overall_metrics[m].append(overall)\n",
    "            annual_metrics[m].append(annual)\n",
    "    \n",
    "    return overall_metrics, annual_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_pipeline():\n",
    "    gene_arr_path = r'../output/gene_editing/es_with_decay.array'\n",
    "    transplant_arr_path = r'../output/transplant/es_with_decay.array'\n",
    "\n",
    "    gene_arr = pickle.load(open(gene_arr_path, mode='rb'))\n",
    "    transplant_arr = pickle.load(open(transplant_arr_path, mode='rb'))\n",
    "    \n",
    "    gene_arr = gene_arr[:, :-1, :]\n",
    "    transplant_arr = transplant_arr[:, :-1, :]\n",
    "\n",
    "    print('Shape of the gene_editing array:',gene_arr.shape)\n",
    "    print('Shape of the transplant array:',transplant_arr.shape)\n",
    "    \n",
    "    metrics = {\n",
    "        'gene':{\n",
    "            'overall':{},\n",
    "            'annual':{}\n",
    "        },\n",
    "        'transplant':{\n",
    "            'overall':{},\n",
    "            'annual':{}\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for name, dataset in zip(['gene', 'transplant'], [gene_arr, transplant_arr]):\n",
    "        scaler, data = scale_data(dataset, 'standard')\n",
    "\n",
    "        # 用预测第二年的类别变量作为分成Kfold的依据，不支持浮点数\n",
    "        X, y, y_cat = data[:, :n_input, :], data[:, n_input:, -2], dataset[:, n_input, -1]\n",
    "        kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        \n",
    "        if name == 'gene':\n",
    "            overall_metrics, annual_metrics = cross_validation(X, y, y_cat, kfold, scaler, 3, 128)\n",
    "        elif name == 'transplant':\n",
    "            overall_metrics, annual_metrics = cross_validation(X, y, y_cat, kfold, scaler, 5, 256)\n",
    "\n",
    "        \n",
    "        for metric, value in overall_metrics.items():\n",
    "            metrics[name]['overall'][metric] = np.mean(value)\n",
    "        \n",
    "        for metric, value in annual_metrics.items():\n",
    "            metrics[name]['annual'][metric] = np.mean(np.array(value), axis=0)\n",
    "    \n",
    "    pickle.dump(metrics, open('mlp_metrics.dict', 'wb'))\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the gene_editing array: (2643, 16, 10)\n",
      "Shape of the transplant array: (5141, 16, 10)\n",
      "Train on 704 samples, validate on 177 samples\n",
      "Epoch 1/300\n",
      "704/704 [==============================] - 1s 2ms/sample - loss: 0.8943 - val_loss: 0.6881\n",
      "Epoch 2/300\n",
      "704/704 [==============================] - 0s 207us/sample - loss: 0.7863 - val_loss: 0.6646\n",
      "Epoch 3/300\n",
      "704/704 [==============================] - 0s 198us/sample - loss: 0.7767 - val_loss: 0.6652\n",
      "Epoch 4/300\n",
      "704/704 [==============================] - 0s 208us/sample - loss: 0.7769 - val_loss: 0.6652\n",
      "Epoch 5/300\n",
      "704/704 [==============================] - 0s 198us/sample - loss: 0.7770 - val_loss: 0.6649\n",
      "Epoch 6/300\n",
      "704/704 [==============================] - 0s 207us/sample - loss: 0.7768 - val_loss: 0.6649\n",
      "Epoch 7/300\n",
      "704/704 [==============================] - 0s 205us/sample - loss: 0.7770 - val_loss: 0.6649\n",
      "Epoch 8/300\n",
      "704/704 [==============================] - 0s 197us/sample - loss: 0.7772 - val_loss: 0.6653\n",
      "Epoch 9/300\n",
      "704/704 [==============================] - 0s 195us/sample - loss: 0.7769 - val_loss: 0.6649\n",
      "Epoch 10/300\n",
      "704/704 [==============================] - 0s 194us/sample - loss: 0.7769 - val_loss: 0.6651\n",
      "Epoch 11/300\n",
      "704/704 [==============================] - 0s 200us/sample - loss: 0.7771 - val_loss: 0.6650\n",
      "Epoch 12/300\n",
      "704/704 [==============================] - 0s 200us/sample - loss: 0.7767 - val_loss: 0.6644\n",
      "Epoch 13/300\n",
      "704/704 [==============================] - 0s 200us/sample - loss: 0.7768 - val_loss: 0.6642\n",
      "Epoch 14/300\n",
      "704/704 [==============================] - 0s 200us/sample - loss: 0.7771 - val_loss: 0.6651\n",
      "Epoch 15/300\n",
      "704/704 [==============================] - 0s 237us/sample - loss: 0.7768 - val_loss: 0.6650\n",
      "Epoch 16/300\n",
      "704/704 [==============================] - 0s 208us/sample - loss: 0.7771 - val_loss: 0.6650\n",
      "Epoch 17/300\n",
      "704/704 [==============================] - 0s 198us/sample - loss: 0.7771 - val_loss: 0.6644\n",
      "Epoch 18/300\n",
      "704/704 [==============================] - 0s 211us/sample - loss: 0.7771 - val_loss: 0.6646\n",
      "Epoch 19/300\n",
      "704/704 [==============================] - 0s 207us/sample - loss: 0.7770 - val_loss: 0.6646\n",
      "Epoch 20/300\n",
      "704/704 [==============================] - 0s 208us/sample - loss: 0.7769 - val_loss: 0.6644\n",
      "Epoch 21/300\n",
      "704/704 [==============================] - 0s 214us/sample - loss: 0.7769 - val_loss: 0.6652\n",
      "Epoch 22/300\n",
      "704/704 [==============================] - 0s 207us/sample - loss: 0.7770 - val_loss: 0.6646\n",
      "Epoch 23/300\n",
      "704/704 [==============================] - 0s 201us/sample - loss: 0.7768 - val_loss: 0.6651\n",
      "Train on 704 samples, validate on 177 samples\n",
      "Epoch 1/300\n",
      "704/704 [==============================] - 1s 1ms/sample - loss: 0.9140 - val_loss: 0.9054\n",
      "Epoch 2/300\n",
      "704/704 [==============================] - 0s 198us/sample - loss: 0.8208 - val_loss: 0.8470\n",
      "Epoch 3/300\n",
      "704/704 [==============================] - 0s 205us/sample - loss: 0.8012 - val_loss: 0.8414\n",
      "Epoch 4/300\n",
      "704/704 [==============================] - 0s 217us/sample - loss: 0.7977 - val_loss: 0.8388\n",
      "Epoch 5/300\n",
      "704/704 [==============================] - 0s 203us/sample - loss: 0.7952 - val_loss: 0.8361\n",
      "Epoch 6/300\n",
      "704/704 [==============================] - 0s 207us/sample - loss: 0.7937 - val_loss: 0.8342\n",
      "Epoch 7/300\n",
      "704/704 [==============================] - 0s 208us/sample - loss: 0.7921 - val_loss: 0.8332\n",
      "Epoch 8/300\n",
      "704/704 [==============================] - 0s 221us/sample - loss: 0.7909 - val_loss: 0.8320\n",
      "Epoch 9/300\n",
      "704/704 [==============================] - 0s 201us/sample - loss: 0.7900 - val_loss: 0.8327\n",
      "Epoch 10/300\n",
      "704/704 [==============================] - 0s 207us/sample - loss: 0.7900 - val_loss: 0.8324\n",
      "Epoch 11/300\n",
      "704/704 [==============================] - 0s 207us/sample - loss: 0.7892 - val_loss: 0.8323\n",
      "Epoch 12/300\n",
      "704/704 [==============================] - 0s 210us/sample - loss: 0.7897 - val_loss: 0.8323\n",
      "Epoch 13/300\n",
      "704/704 [==============================] - 0s 228us/sample - loss: 0.7893 - val_loss: 0.8319\n",
      "Epoch 14/300\n",
      "704/704 [==============================] - 0s 214us/sample - loss: 0.7888 - val_loss: 0.8318\n",
      "Epoch 15/300\n",
      "704/704 [==============================] - 0s 207us/sample - loss: 0.7889 - val_loss: 0.8324\n",
      "Epoch 16/300\n",
      "704/704 [==============================] - 0s 207us/sample - loss: 0.7885 - val_loss: 0.8321\n",
      "Epoch 17/300\n",
      "704/704 [==============================] - 0s 212us/sample - loss: 0.7889 - val_loss: 0.8334\n",
      "Epoch 18/300\n",
      "704/704 [==============================] - 0s 218us/sample - loss: 0.7885 - val_loss: 0.8333\n",
      "Epoch 19/300\n",
      "704/704 [==============================] - 0s 262us/sample - loss: 0.7890 - val_loss: 0.8319\n",
      "Epoch 20/300\n",
      "704/704 [==============================] - 0s 239us/sample - loss: 0.7882 - val_loss: 0.8323\n",
      "Epoch 21/300\n",
      "704/704 [==============================] - 0s 214us/sample - loss: 0.7882 - val_loss: 0.8330\n",
      "Epoch 22/300\n",
      "704/704 [==============================] - 0s 208us/sample - loss: 0.7883 - val_loss: 0.8331\n",
      "Epoch 23/300\n",
      "704/704 [==============================] - 0s 211us/sample - loss: 0.7880 - val_loss: 0.8319\n",
      "Epoch 24/300\n",
      "704/704 [==============================] - 0s 214us/sample - loss: 0.7878 - val_loss: 0.8319\n",
      "Train on 704 samples, validate on 177 samples\n",
      "Epoch 1/300\n",
      "704/704 [==============================] - 1s 1ms/sample - loss: 1.4322 - val_loss: 1.3758\n",
      "Epoch 2/300\n",
      "704/704 [==============================] - 0s 208us/sample - loss: 1.2581 - val_loss: 1.2562\n",
      "Epoch 3/300\n",
      "704/704 [==============================] - 0s 237us/sample - loss: 1.1926 - val_loss: 1.2027\n",
      "Epoch 4/300\n",
      "704/704 [==============================] - 0s 231us/sample - loss: 1.1573 - val_loss: 1.1689\n",
      "Epoch 5/300\n",
      "704/704 [==============================] - 0s 234us/sample - loss: 1.1379 - val_loss: 1.1545\n",
      "Epoch 6/300\n",
      "704/704 [==============================] - 0s 208us/sample - loss: 1.1280 - val_loss: 1.1474\n",
      "Epoch 7/300\n",
      "704/704 [==============================] - 0s 212us/sample - loss: 1.1210 - val_loss: 1.1447\n",
      "Epoch 8/300\n",
      "704/704 [==============================] - 0s 214us/sample - loss: 1.1165 - val_loss: 1.1410\n",
      "Epoch 9/300\n",
      "704/704 [==============================] - 0s 207us/sample - loss: 1.1124 - val_loss: 1.1415\n",
      "Epoch 10/300\n",
      "704/704 [==============================] - 0s 211us/sample - loss: 1.1084 - val_loss: 1.1427\n",
      "Epoch 11/300\n",
      "704/704 [==============================] - 0s 208us/sample - loss: 1.1040 - val_loss: 1.1408\n",
      "Epoch 12/300\n",
      "704/704 [==============================] - 0s 208us/sample - loss: 1.1002 - val_loss: 1.1416\n",
      "Epoch 13/300\n",
      "704/704 [==============================] - 0s 214us/sample - loss: 1.0974 - val_loss: 1.1414\n",
      "Epoch 14/300\n",
      "704/704 [==============================] - 0s 214us/sample - loss: 1.0936 - val_loss: 1.1402\n",
      "Epoch 15/300\n",
      "704/704 [==============================] - 0s 210us/sample - loss: 1.0913 - val_loss: 1.1404\n",
      "Epoch 16/300\n",
      "704/704 [==============================] - 0s 214us/sample - loss: 1.0877 - val_loss: 1.1418\n",
      "Epoch 17/300\n",
      "704/704 [==============================] - 0s 231us/sample - loss: 1.0869 - val_loss: 1.1395\n",
      "Epoch 18/300\n",
      "704/704 [==============================] - 0s 228us/sample - loss: 1.0826 - val_loss: 1.1418\n",
      "Epoch 19/300\n",
      "704/704 [==============================] - 0s 210us/sample - loss: 1.0802 - val_loss: 1.1421\n",
      "Epoch 20/300\n",
      "704/704 [==============================] - 0s 212us/sample - loss: 1.0766 - val_loss: 1.1408\n",
      "Epoch 21/300\n",
      "704/704 [==============================] - 0s 214us/sample - loss: 1.0733 - val_loss: 1.1400\n",
      "Epoch 22/300\n",
      "704/704 [==============================] - 0s 238us/sample - loss: 1.0716 - val_loss: 1.1398\n",
      "Epoch 23/300\n",
      "704/704 [==============================] - 0s 217us/sample - loss: 1.0685 - val_loss: 1.1399\n",
      "Epoch 24/300\n",
      "704/704 [==============================] - 0s 213us/sample - loss: 1.0661 - val_loss: 1.1410\n",
      "Epoch 25/300\n",
      "704/704 [==============================] - 0s 217us/sample - loss: 1.0639 - val_loss: 1.1409\n",
      "Epoch 26/300\n",
      "704/704 [==============================] - 0s 251us/sample - loss: 1.0610 - val_loss: 1.1399\n",
      "Epoch 27/300\n",
      "704/704 [==============================] - 0s 218us/sample - loss: 1.0579 - val_loss: 1.1401\n",
      "Train on 704 samples, validate on 176 samples\n",
      "Epoch 1/300\n",
      "704/704 [==============================] - 1s 1ms/sample - loss: 0.8648 - val_loss: 0.7718\n",
      "Epoch 2/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "704/704 [==============================] - 0s 205us/sample - loss: 0.7585 - val_loss: 0.7483\n",
      "Epoch 3/300\n",
      "704/704 [==============================] - 0s 214us/sample - loss: 0.7508 - val_loss: 0.7486\n",
      "Epoch 4/300\n",
      "704/704 [==============================] - 0s 227us/sample - loss: 0.7511 - val_loss: 0.7484\n",
      "Epoch 5/300\n",
      "704/704 [==============================] - 0s 225us/sample - loss: 0.7513 - val_loss: 0.7486\n",
      "Epoch 6/300\n",
      "704/704 [==============================] - 0s 217us/sample - loss: 0.7513 - val_loss: 0.7473\n",
      "Epoch 7/300\n",
      "704/704 [==============================] - 0s 263us/sample - loss: 0.7513 - val_loss: 0.7483\n",
      "Epoch 8/300\n",
      "704/704 [==============================] - 0s 229us/sample - loss: 0.7509 - val_loss: 0.7483\n",
      "Epoch 9/300\n",
      "704/704 [==============================] - 0s 225us/sample - loss: 0.7510 - val_loss: 0.7484\n",
      "Epoch 10/300\n",
      "704/704 [==============================] - 0s 210us/sample - loss: 0.7513 - val_loss: 0.7489\n",
      "Epoch 11/300\n",
      "704/704 [==============================] - 0s 214us/sample - loss: 0.7511 - val_loss: 0.7481\n",
      "Epoch 12/300\n",
      "704/704 [==============================] - 0s 212us/sample - loss: 0.7512 - val_loss: 0.7486\n",
      "Epoch 13/300\n",
      "704/704 [==============================] - 0s 205us/sample - loss: 0.7508 - val_loss: 0.7480\n",
      "Epoch 14/300\n",
      "704/704 [==============================] - 0s 204us/sample - loss: 0.7512 - val_loss: 0.7489\n",
      "Epoch 15/300\n",
      "704/704 [==============================] - 0s 214us/sample - loss: 0.7512 - val_loss: 0.7480\n",
      "Epoch 16/300\n",
      "704/704 [==============================] - 0s 221us/sample - loss: 0.7512 - val_loss: 0.7486\n",
      "Train on 704 samples, validate on 176 samples\n",
      "Epoch 1/300\n",
      "704/704 [==============================] - 1s 1ms/sample - loss: 0.8940 - val_loss: 0.8902\n",
      "Epoch 2/300\n",
      "704/704 [==============================] - 0s 214us/sample - loss: 0.8165 - val_loss: 0.8453\n",
      "Epoch 3/300\n",
      "704/704 [==============================] - 0s 214us/sample - loss: 0.8024 - val_loss: 0.8403\n",
      "Epoch 4/300\n",
      "704/704 [==============================] - 0s 232us/sample - loss: 0.7992 - val_loss: 0.8384\n",
      "Epoch 5/300\n",
      "704/704 [==============================] - 0s 323us/sample - loss: 0.7970 - val_loss: 0.8383\n",
      "Epoch 6/300\n",
      "704/704 [==============================] - 0s 235us/sample - loss: 0.7956 - val_loss: 0.8368\n",
      "Epoch 7/300\n",
      "704/704 [==============================] - 0s 215us/sample - loss: 0.7949 - val_loss: 0.8380\n",
      "Epoch 8/300\n",
      "704/704 [==============================] - 0s 217us/sample - loss: 0.7941 - val_loss: 0.8368\n",
      "Epoch 9/300\n",
      "704/704 [==============================] - 0s 224us/sample - loss: 0.7932 - val_loss: 0.8382\n",
      "Epoch 10/300\n",
      "704/704 [==============================] - 0s 229us/sample - loss: 0.7931 - val_loss: 0.8375\n",
      "Epoch 11/300\n",
      "704/704 [==============================] - 0s 248us/sample - loss: 0.7931 - val_loss: 0.8383\n",
      "Epoch 12/300\n",
      "704/704 [==============================] - 0s 251us/sample - loss: 0.7927 - val_loss: 0.8375\n",
      "Epoch 13/300\n",
      "704/704 [==============================] - 0s 252us/sample - loss: 0.7927 - val_loss: 0.8375\n",
      "Epoch 14/300\n",
      "704/704 [==============================] - 0s 246us/sample - loss: 0.7924 - val_loss: 0.8396\n",
      "Epoch 15/300\n",
      "704/704 [==============================] - 0s 247us/sample - loss: 0.7926 - val_loss: 0.8369\n",
      "Epoch 16/300\n",
      "704/704 [==============================] - 0s 251us/sample - loss: 0.7927 - val_loss: 0.8391\n",
      "Train on 705 samples, validate on 178 samples\n",
      "Epoch 1/300\n",
      "705/705 [==============================] - 1s 1ms/sample - loss: 1.3478 - val_loss: 1.3470\n",
      "Epoch 2/300\n",
      "705/705 [==============================] - 0s 291us/sample - loss: 1.2161 - val_loss: 1.2708\n",
      "Epoch 3/300\n",
      "705/705 [==============================] - 0s 279us/sample - loss: 1.1593 - val_loss: 1.2359\n",
      "Epoch 4/300\n",
      "705/705 [==============================] - 0s 277us/sample - loss: 1.1361 - val_loss: 1.2213\n",
      "Epoch 5/300\n",
      "705/705 [==============================] - 0s 293us/sample - loss: 1.1234 - val_loss: 1.2130\n",
      "Epoch 6/300\n",
      "705/705 [==============================] - 0s 291us/sample - loss: 1.1158 - val_loss: 1.2079\n",
      "Epoch 7/300\n",
      "705/705 [==============================] - 0s 283us/sample - loss: 1.1106 - val_loss: 1.2022\n",
      "Epoch 8/300\n",
      "705/705 [==============================] - 0s 291us/sample - loss: 1.1057 - val_loss: 1.2001\n",
      "Epoch 9/300\n",
      "705/705 [==============================] - 0s 270us/sample - loss: 1.1012 - val_loss: 1.1960\n",
      "Epoch 10/300\n",
      "705/705 [==============================] - 0s 283us/sample - loss: 1.0979 - val_loss: 1.1953\n",
      "Epoch 11/300\n",
      "705/705 [==============================] - 0s 310us/sample - loss: 1.0941 - val_loss: 1.1940\n",
      "Epoch 12/300\n",
      "705/705 [==============================] - 0s 297us/sample - loss: 1.0908 - val_loss: 1.1927\n",
      "Epoch 13/300\n",
      "705/705 [==============================] - 0s 267us/sample - loss: 1.0878 - val_loss: 1.1909\n",
      "Epoch 14/300\n",
      "705/705 [==============================] - 0s 284us/sample - loss: 1.0863 - val_loss: 1.1909\n",
      "Epoch 15/300\n",
      "705/705 [==============================] - 0s 293us/sample - loss: 1.0811 - val_loss: 1.1887\n",
      "Epoch 16/300\n",
      "705/705 [==============================] - 0s 270us/sample - loss: 1.0794 - val_loss: 1.1866\n",
      "Epoch 17/300\n",
      "705/705 [==============================] - 0s 276us/sample - loss: 1.0759 - val_loss: 1.1861\n",
      "Epoch 18/300\n",
      "705/705 [==============================] - 0s 272us/sample - loss: 1.0726 - val_loss: 1.1892\n",
      "Epoch 19/300\n",
      "705/705 [==============================] - 0s 272us/sample - loss: 1.0703 - val_loss: 1.1889\n",
      "Epoch 20/300\n",
      "705/705 [==============================] - 0s 269us/sample - loss: 1.0674 - val_loss: 1.1872\n",
      "Epoch 21/300\n",
      "705/705 [==============================] - 0s 270us/sample - loss: 1.0641 - val_loss: 1.1847\n",
      "Epoch 22/300\n",
      "705/705 [==============================] - 0s 265us/sample - loss: 1.0614 - val_loss: 1.1865\n",
      "Epoch 23/300\n",
      "705/705 [==============================] - 0s 313us/sample - loss: 1.0597 - val_loss: 1.1847\n",
      "Epoch 24/300\n",
      "705/705 [==============================] - 0s 266us/sample - loss: 1.0559 - val_loss: 1.1830\n",
      "Epoch 25/300\n",
      "705/705 [==============================] - 0s 266us/sample - loss: 1.0543 - val_loss: 1.1849\n",
      "Epoch 26/300\n",
      "705/705 [==============================] - 0s 279us/sample - loss: 1.0568 - val_loss: 1.1851\n",
      "Epoch 27/300\n",
      "705/705 [==============================] - 0s 272us/sample - loss: 1.0508 - val_loss: 1.1855\n",
      "Epoch 28/300\n",
      "705/705 [==============================] - 0s 284us/sample - loss: 1.0459 - val_loss: 1.1830\n",
      "Epoch 29/300\n",
      "705/705 [==============================] - 0s 272us/sample - loss: 1.0435 - val_loss: 1.1830\n",
      "Epoch 30/300\n",
      "705/705 [==============================] - 0s 280us/sample - loss: 1.0405 - val_loss: 1.1828\n",
      "Epoch 31/300\n",
      "705/705 [==============================] - 0s 304us/sample - loss: 1.0371 - val_loss: 1.1821\n",
      "Epoch 32/300\n",
      "705/705 [==============================] - 0s 280us/sample - loss: 1.0359 - val_loss: 1.1811\n",
      "Epoch 33/300\n",
      "705/705 [==============================] - 0s 272us/sample - loss: 1.0323 - val_loss: 1.1816\n",
      "Epoch 34/300\n",
      "705/705 [==============================] - 0s 274us/sample - loss: 1.0312 - val_loss: 1.1808\n",
      "Epoch 35/300\n",
      "705/705 [==============================] - 0s 270us/sample - loss: 1.0269 - val_loss: 1.1806\n",
      "Epoch 36/300\n",
      "705/705 [==============================] - 0s 259us/sample - loss: 1.0259 - val_loss: 1.1824\n",
      "Epoch 37/300\n",
      "705/705 [==============================] - 0s 263us/sample - loss: 1.0216 - val_loss: 1.1805\n",
      "Epoch 38/300\n",
      "705/705 [==============================] - 0s 259us/sample - loss: 1.0192 - val_loss: 1.1815\n",
      "Epoch 39/300\n",
      "705/705 [==============================] - 0s 269us/sample - loss: 1.0194 - val_loss: 1.1806\n",
      "Epoch 40/300\n",
      "705/705 [==============================] - 0s 274us/sample - loss: 1.0150 - val_loss: 1.1796\n",
      "Epoch 41/300\n",
      "705/705 [==============================] - 0s 263us/sample - loss: 1.0135 - val_loss: 1.1797\n",
      "Epoch 42/300\n",
      "705/705 [==============================] - 0s 262us/sample - loss: 1.0112 - val_loss: 1.1799\n",
      "Epoch 43/300\n",
      "705/705 [==============================] - 0s 252us/sample - loss: 1.0081 - val_loss: 1.1807\n",
      "Epoch 44/300\n",
      "705/705 [==============================] - 0s 270us/sample - loss: 1.0071 - val_loss: 1.1787\n",
      "Epoch 45/300\n",
      "705/705 [==============================] - 0s 267us/sample - loss: 1.0044 - val_loss: 1.1788\n",
      "Epoch 46/300\n",
      "705/705 [==============================] - 0s 270us/sample - loss: 1.0024 - val_loss: 1.1773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/300\n",
      "705/705 [==============================] - 0s 260us/sample - loss: 0.9994 - val_loss: 1.1780\n",
      "Epoch 48/300\n",
      "705/705 [==============================] - 0s 265us/sample - loss: 1.0007 - val_loss: 1.1767\n",
      "Epoch 49/300\n",
      "705/705 [==============================] - 0s 293us/sample - loss: 0.9964 - val_loss: 1.1774\n",
      "Epoch 50/300\n",
      "705/705 [==============================] - 0s 267us/sample - loss: 0.9950 - val_loss: 1.1775\n",
      "Epoch 51/300\n",
      "705/705 [==============================] - 0s 272us/sample - loss: 0.9926 - val_loss: 1.1788\n",
      "Epoch 52/300\n",
      "705/705 [==============================] - 0s 266us/sample - loss: 0.9903 - val_loss: 1.1777\n",
      "Epoch 53/300\n",
      "705/705 [==============================] - 0s 259us/sample - loss: 0.9884 - val_loss: 1.1791\n",
      "Epoch 54/300\n",
      "705/705 [==============================] - 0s 266us/sample - loss: 0.9869 - val_loss: 1.1772\n",
      "Epoch 55/300\n",
      "705/705 [==============================] - 0s 263us/sample - loss: 0.9851 - val_loss: 1.1815\n",
      "Epoch 56/300\n",
      "705/705 [==============================] - 0s 284us/sample - loss: 0.9816 - val_loss: 1.1790\n",
      "Epoch 57/300\n",
      "705/705 [==============================] - 0s 262us/sample - loss: 0.9803 - val_loss: 1.1796\n",
      "Epoch 58/300\n",
      "705/705 [==============================] - 0s 276us/sample - loss: 0.9785 - val_loss: 1.1783\n",
      "Train on 705 samples, validate on 176 samples\n",
      "Epoch 1/300\n",
      "705/705 [==============================] - 1s 1ms/sample - loss: 0.8838 - val_loss: 0.7998\n",
      "Epoch 2/300\n",
      "705/705 [==============================] - 0s 293us/sample - loss: 0.7840 - val_loss: 0.7636\n",
      "Epoch 3/300\n",
      "705/705 [==============================] - 0s 298us/sample - loss: 0.7688 - val_loss: 0.7644\n",
      "Epoch 4/300\n",
      "705/705 [==============================] - 0s 298us/sample - loss: 0.7682 - val_loss: 0.7629\n",
      "Epoch 5/300\n",
      "705/705 [==============================] - 0s 306us/sample - loss: 0.7687 - val_loss: 0.7636\n",
      "Epoch 6/300\n",
      "705/705 [==============================] - 0s 260us/sample - loss: 0.7682 - val_loss: 0.7633\n",
      "Epoch 7/300\n",
      "705/705 [==============================] - 0s 260us/sample - loss: 0.7681 - val_loss: 0.7629\n",
      "Epoch 8/300\n",
      "705/705 [==============================] - 0s 252us/sample - loss: 0.7681 - val_loss: 0.7634\n",
      "Epoch 9/300\n",
      "705/705 [==============================] - 0s 255us/sample - loss: 0.7682 - val_loss: 0.7628\n",
      "Epoch 10/300\n",
      "705/705 [==============================] - 0s 252us/sample - loss: 0.7689 - val_loss: 0.7634\n",
      "Epoch 11/300\n",
      "705/705 [==============================] - 0s 249us/sample - loss: 0.7680 - val_loss: 0.7634\n",
      "Epoch 12/300\n",
      "705/705 [==============================] - 0s 260us/sample - loss: 0.7684 - val_loss: 0.7639\n",
      "Epoch 13/300\n",
      "705/705 [==============================] - 0s 265us/sample - loss: 0.7684 - val_loss: 0.7632\n",
      "Epoch 14/300\n",
      "705/705 [==============================] - 0s 259us/sample - loss: 0.7685 - val_loss: 0.7636\n",
      "Epoch 15/300\n",
      "705/705 [==============================] - 0s 266us/sample - loss: 0.7685 - val_loss: 0.7631\n",
      "Epoch 16/300\n",
      "705/705 [==============================] - 0s 262us/sample - loss: 0.7686 - val_loss: 0.7633\n",
      "Epoch 17/300\n",
      "705/705 [==============================] - 0s 263us/sample - loss: 0.7686 - val_loss: 0.7636\n",
      "Epoch 18/300\n",
      "705/705 [==============================] - 0s 265us/sample - loss: 0.7682 - val_loss: 0.7635\n",
      "Epoch 19/300\n",
      "705/705 [==============================] - 0s 255us/sample - loss: 0.7682 - val_loss: 0.7631\n",
      "Train on 705 samples, validate on 176 samples\n",
      "Epoch 1/300\n",
      "705/705 [==============================] - 1s 1ms/sample - loss: 0.8945 - val_loss: 0.8405\n",
      "Epoch 2/300\n",
      "705/705 [==============================] - 0s 255us/sample - loss: 0.7942 - val_loss: 0.8139\n",
      "Epoch 3/300\n",
      "705/705 [==============================] - 0s 265us/sample - loss: 0.7871 - val_loss: 0.8109\n",
      "Epoch 4/300\n",
      "705/705 [==============================] - 0s 274us/sample - loss: 0.7843 - val_loss: 0.8089\n",
      "Epoch 5/300\n",
      "705/705 [==============================] - 0s 262us/sample - loss: 0.7830 - val_loss: 0.8068\n",
      "Epoch 6/300\n",
      "705/705 [==============================] - 0s 260us/sample - loss: 0.7817 - val_loss: 0.8065\n",
      "Epoch 7/300\n",
      "705/705 [==============================] - ETA: 0s - loss: 0.781 - 0s 263us/sample - loss: 0.7822 - val_loss: 0.8062\n",
      "Epoch 8/300\n",
      "705/705 [==============================] - 0s 262us/sample - loss: 0.7814 - val_loss: 0.8059\n",
      "Epoch 9/300\n",
      "705/705 [==============================] - 0s 263us/sample - loss: 0.7814 - val_loss: 0.8054\n",
      "Epoch 10/300\n",
      "705/705 [==============================] - 0s 255us/sample - loss: 0.7805 - val_loss: 0.8052\n",
      "Epoch 11/300\n",
      "705/705 [==============================] - 0s 257us/sample - loss: 0.7804 - val_loss: 0.8055\n",
      "Epoch 12/300\n",
      "705/705 [==============================] - 0s 252us/sample - loss: 0.7813 - val_loss: 0.8056\n",
      "Epoch 13/300\n",
      "705/705 [==============================] - 0s 255us/sample - loss: 0.7809 - val_loss: 0.8060\n",
      "Epoch 14/300\n",
      "705/705 [==============================] - 0s 252us/sample - loss: 0.7803 - val_loss: 0.8060\n",
      "Epoch 15/300\n",
      "705/705 [==============================] - 0s 297us/sample - loss: 0.7809 - val_loss: 0.8063\n",
      "Epoch 16/300\n",
      "705/705 [==============================] - 0s 252us/sample - loss: 0.7803 - val_loss: 0.8054\n",
      "Epoch 17/300\n",
      "705/705 [==============================] - 0s 249us/sample - loss: 0.7808 - val_loss: 0.8055\n",
      "Epoch 18/300\n",
      "705/705 [==============================] - 0s 260us/sample - loss: 0.7801 - val_loss: 0.8056\n",
      "Epoch 19/300\n",
      "705/705 [==============================] - 0s 255us/sample - loss: 0.7798 - val_loss: 0.8054\n",
      "Epoch 20/300\n",
      "705/705 [==============================] - 0s 259us/sample - loss: 0.7800 - val_loss: 0.8051\n",
      "Epoch 21/300\n",
      "705/705 [==============================] - 0s 260us/sample - loss: 0.7801 - val_loss: 0.8054\n",
      "Epoch 22/300\n",
      "705/705 [==============================] - 0s 259us/sample - loss: 0.7800 - val_loss: 0.8046\n",
      "Epoch 23/300\n",
      "705/705 [==============================] - 0s 260us/sample - loss: 0.7806 - val_loss: 0.8051\n",
      "Epoch 24/300\n",
      "705/705 [==============================] - 0s 263us/sample - loss: 0.7801 - val_loss: 0.8044\n",
      "Epoch 25/300\n",
      "705/705 [==============================] - 0s 252us/sample - loss: 0.7801 - val_loss: 0.8050\n",
      "Epoch 26/300\n",
      "705/705 [==============================] - 0s 257us/sample - loss: 0.7801 - val_loss: 0.8045\n",
      "Epoch 27/300\n",
      "705/705 [==============================] - 0s 253us/sample - loss: 0.7795 - val_loss: 0.8045\n",
      "Epoch 28/300\n",
      "705/705 [==============================] - 0s 248us/sample - loss: 0.7800 - val_loss: 0.8046\n",
      "Epoch 29/300\n",
      "705/705 [==============================] - 0s 253us/sample - loss: 0.7790 - val_loss: 0.8046\n",
      "Epoch 30/300\n",
      "705/705 [==============================] - 0s 257us/sample - loss: 0.7794 - val_loss: 0.8051\n",
      "Epoch 31/300\n",
      "705/705 [==============================] - 0s 250us/sample - loss: 0.7794 - val_loss: 0.8037\n",
      "Epoch 32/300\n",
      "705/705 [==============================] - 0s 257us/sample - loss: 0.7795 - val_loss: 0.8040\n",
      "Epoch 33/300\n",
      "705/705 [==============================] - 0s 250us/sample - loss: 0.7791 - val_loss: 0.8048\n",
      "Epoch 34/300\n",
      "705/705 [==============================] - 0s 255us/sample - loss: 0.7795 - val_loss: 0.8046\n",
      "Epoch 35/300\n",
      "705/705 [==============================] - 0s 274us/sample - loss: 0.7790 - val_loss: 0.8045\n",
      "Epoch 36/300\n",
      "705/705 [==============================] - 0s 267us/sample - loss: 0.7786 - val_loss: 0.8043\n",
      "Epoch 37/300\n",
      "705/705 [==============================] - 0s 252us/sample - loss: 0.7784 - val_loss: 0.8043\n",
      "Epoch 38/300\n",
      "705/705 [==============================] - 0s 257us/sample - loss: 0.7784 - val_loss: 0.8047\n",
      "Epoch 39/300\n",
      "705/705 [==============================] - 0s 253us/sample - loss: 0.7787 - val_loss: 0.8046\n",
      "Epoch 40/300\n",
      "705/705 [==============================] - 0s 250us/sample - loss: 0.7779 - val_loss: 0.8041\n",
      "Epoch 41/300\n",
      "705/705 [==============================] - 0s 257us/sample - loss: 0.7787 - val_loss: 0.8047\n",
      "Train on 705 samples, validate on 176 samples\n",
      "Epoch 1/300\n",
      "705/705 [==============================] - 1s 2ms/sample - loss: 1.3846 - val_loss: 1.2785\n",
      "Epoch 2/300\n",
      "705/705 [==============================] - 0s 300us/sample - loss: 1.2404 - val_loss: 1.1784\n",
      "Epoch 3/300\n",
      "705/705 [==============================] - 0s 263us/sample - loss: 1.1727 - val_loss: 1.1466\n",
      "Epoch 4/300\n",
      "705/705 [==============================] - 0s 263us/sample - loss: 1.1499 - val_loss: 1.1342\n",
      "Epoch 5/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "705/705 [==============================] - 0s 257us/sample - loss: 1.1380 - val_loss: 1.1294\n",
      "Epoch 6/300\n",
      "705/705 [==============================] - 0s 263us/sample - loss: 1.1313 - val_loss: 1.1259\n",
      "Epoch 7/300\n",
      "705/705 [==============================] - 0s 272us/sample - loss: 1.1243 - val_loss: 1.1217\n",
      "Epoch 8/300\n",
      "705/705 [==============================] - 0s 257us/sample - loss: 1.1184 - val_loss: 1.1228\n",
      "Epoch 9/300\n",
      "705/705 [==============================] - 0s 263us/sample - loss: 1.1140 - val_loss: 1.1212\n",
      "Epoch 10/300\n",
      "705/705 [==============================] - 0s 267us/sample - loss: 1.1095 - val_loss: 1.1226\n",
      "Epoch 11/300\n",
      "705/705 [==============================] - 0s 267us/sample - loss: 1.1053 - val_loss: 1.1190\n",
      "Epoch 12/300\n",
      "705/705 [==============================] - 0s 250us/sample - loss: 1.1006 - val_loss: 1.1199\n",
      "Epoch 13/300\n",
      "705/705 [==============================] - 0s 257us/sample - loss: 1.0969 - val_loss: 1.1211\n",
      "Epoch 14/300\n",
      "705/705 [==============================] - 0s 256us/sample - loss: 1.0942 - val_loss: 1.1190\n",
      "Epoch 15/300\n",
      "705/705 [==============================] - 0s 256us/sample - loss: 1.0915 - val_loss: 1.1207\n",
      "Epoch 16/300\n",
      "705/705 [==============================] - 0s 256us/sample - loss: 1.0873 - val_loss: 1.1186\n",
      "Epoch 17/300\n",
      "705/705 [==============================] - 0s 250us/sample - loss: 1.0835 - val_loss: 1.1195\n",
      "Epoch 18/300\n",
      "705/705 [==============================] - 0s 255us/sample - loss: 1.0800 - val_loss: 1.1173\n",
      "Epoch 19/300\n",
      "705/705 [==============================] - 0s 249us/sample - loss: 1.0798 - val_loss: 1.1205\n",
      "Epoch 20/300\n",
      "705/705 [==============================] - 0s 253us/sample - loss: 1.0752 - val_loss: 1.1138\n",
      "Epoch 21/300\n",
      "705/705 [==============================] - 0s 260us/sample - loss: 1.0714 - val_loss: 1.1163\n",
      "Epoch 22/300\n",
      "705/705 [==============================] - 0s 255us/sample - loss: 1.0682 - val_loss: 1.1167\n",
      "Epoch 23/300\n",
      "705/705 [==============================] - 0s 252us/sample - loss: 1.0654 - val_loss: 1.1167\n",
      "Epoch 24/300\n",
      "705/705 [==============================] - 0s 248us/sample - loss: 1.0620 - val_loss: 1.1141\n",
      "Epoch 25/300\n",
      "705/705 [==============================] - 0s 249us/sample - loss: 1.0589 - val_loss: 1.1140\n",
      "Epoch 26/300\n",
      "705/705 [==============================] - 0s 252us/sample - loss: 1.0660 - val_loss: 1.1152\n",
      "Epoch 27/300\n",
      "705/705 [==============================] - 0s 253us/sample - loss: 1.0564 - val_loss: 1.1163\n",
      "Epoch 28/300\n",
      "705/705 [==============================] - 0s 250us/sample - loss: 1.0513 - val_loss: 1.1151\n",
      "Epoch 29/300\n",
      "705/705 [==============================] - 0s 256us/sample - loss: 1.0481 - val_loss: 1.1168\n",
      "Epoch 30/300\n",
      "705/705 [==============================] - 0s 257us/sample - loss: 1.0456 - val_loss: 1.1160\n",
      "Train on 705 samples, validate on 175 samples\n",
      "Epoch 1/300\n",
      "705/705 [==============================] - 1s 1ms/sample - loss: 0.9320 - val_loss: 0.8768\n",
      "Epoch 2/300\n",
      "705/705 [==============================] - 0s 253us/sample - loss: 0.7544 - val_loss: 0.8111\n",
      "Epoch 3/300\n",
      "705/705 [==============================] - ETA: 0s - loss: 0.719 - 0s 259us/sample - loss: 0.7366 - val_loss: 0.8104\n",
      "Epoch 4/300\n",
      "705/705 [==============================] - 0s 257us/sample - loss: 0.7366 - val_loss: 0.8107\n",
      "Epoch 5/300\n",
      "705/705 [==============================] - 0s 259us/sample - loss: 0.7365 - val_loss: 0.8100\n",
      "Epoch 6/300\n",
      "705/705 [==============================] - 0s 265us/sample - loss: 0.7368 - val_loss: 0.8102\n",
      "Epoch 7/300\n",
      "705/705 [==============================] - 0s 267us/sample - loss: 0.7369 - val_loss: 0.8103\n",
      "Epoch 8/300\n",
      "705/705 [==============================] - 0s 293us/sample - loss: 0.7371 - val_loss: 0.8105\n",
      "Epoch 9/300\n",
      "705/705 [==============================] - 0s 279us/sample - loss: 0.7366 - val_loss: 0.8110\n",
      "Epoch 10/300\n",
      "705/705 [==============================] - 0s 280us/sample - loss: 0.7368 - val_loss: 0.8109\n",
      "Epoch 11/300\n",
      "705/705 [==============================] - 0s 266us/sample - loss: 0.7368 - val_loss: 0.8101\n",
      "Epoch 12/300\n",
      "705/705 [==============================] - 0s 267us/sample - loss: 0.7369 - val_loss: 0.8110\n",
      "Epoch 13/300\n",
      "705/705 [==============================] - 0s 259us/sample - loss: 0.7367 - val_loss: 0.8102\n",
      "Epoch 14/300\n",
      "705/705 [==============================] - 0s 330us/sample - loss: 0.7366 - val_loss: 0.8100\n",
      "Epoch 15/300\n",
      "705/705 [==============================] - 0s 260us/sample - loss: 0.7369 - val_loss: 0.8106\n",
      "Train on 705 samples, validate on 175 samples\n",
      "Epoch 1/300\n",
      "705/705 [==============================] - 1s 2ms/sample - loss: 0.8987 - val_loss: 0.8422\n",
      "Epoch 2/300\n",
      "705/705 [==============================] - 0s 249us/sample - loss: 0.8132 - val_loss: 0.8226\n",
      "Epoch 3/300\n",
      "705/705 [==============================] - 0s 250us/sample - loss: 0.8057 - val_loss: 0.8193\n",
      "Epoch 4/300\n",
      "705/705 [==============================] - 0s 259us/sample - loss: 0.8028 - val_loss: 0.8168\n",
      "Epoch 5/300\n",
      "705/705 [==============================] - 0s 263us/sample - loss: 0.8008 - val_loss: 0.8147\n",
      "Epoch 6/300\n",
      "705/705 [==============================] - 0s 256us/sample - loss: 0.7982 - val_loss: 0.8135\n",
      "Epoch 7/300\n",
      "705/705 [==============================] - 0s 267us/sample - loss: 0.7971 - val_loss: 0.8126\n",
      "Epoch 8/300\n",
      "705/705 [==============================] - 0s 279us/sample - loss: 0.7965 - val_loss: 0.8121\n",
      "Epoch 9/300\n",
      "705/705 [==============================] - 0s 269us/sample - loss: 0.7951 - val_loss: 0.8113\n",
      "Epoch 10/300\n",
      "705/705 [==============================] - 0s 283us/sample - loss: 0.7952 - val_loss: 0.8112\n",
      "Epoch 11/300\n",
      "705/705 [==============================] - 0s 266us/sample - loss: 0.7951 - val_loss: 0.8106\n",
      "Epoch 12/300\n",
      "705/705 [==============================] - 0s 255us/sample - loss: 0.7950 - val_loss: 0.8114\n",
      "Epoch 13/300\n",
      "705/705 [==============================] - 0s 276us/sample - loss: 0.7945 - val_loss: 0.8104\n",
      "Epoch 14/300\n",
      "705/705 [==============================] - 0s 259us/sample - loss: 0.7938 - val_loss: 0.8107\n",
      "Epoch 15/300\n",
      "705/705 [==============================] - 0s 253us/sample - loss: 0.7939 - val_loss: 0.8109\n",
      "Epoch 16/300\n",
      "705/705 [==============================] - 0s 265us/sample - loss: 0.7939 - val_loss: 0.8112\n",
      "Epoch 17/300\n",
      "705/705 [==============================] - 0s 257us/sample - loss: 0.7936 - val_loss: 0.8124\n",
      "Epoch 18/300\n",
      "705/705 [==============================] - 0s 260us/sample - loss: 0.7933 - val_loss: 0.8101\n",
      "Epoch 19/300\n",
      "705/705 [==============================] - 0s 269us/sample - loss: 0.7934 - val_loss: 0.8109\n",
      "Epoch 20/300\n",
      "705/705 [==============================] - 0s 259us/sample - loss: 0.7929 - val_loss: 0.8107\n",
      "Epoch 21/300\n",
      "705/705 [==============================] - 0s 265us/sample - loss: 0.7926 - val_loss: 0.8104\n",
      "Epoch 22/300\n",
      "705/705 [==============================] - 0s 256us/sample - loss: 0.7927 - val_loss: 0.8116\n",
      "Epoch 23/300\n",
      "705/705 [==============================] - 0s 265us/sample - loss: 0.7927 - val_loss: 0.8107\n",
      "Epoch 24/300\n",
      "705/705 [==============================] - 0s 274us/sample - loss: 0.7923 - val_loss: 0.8123\n",
      "Epoch 25/300\n",
      "705/705 [==============================] - 0s 276us/sample - loss: 0.7929 - val_loss: 0.8121\n",
      "Epoch 26/300\n",
      "705/705 [==============================] - 0s 273us/sample - loss: 0.7922 - val_loss: 0.8122\n",
      "Epoch 27/300\n",
      "705/705 [==============================] - 0s 270us/sample - loss: 0.7927 - val_loss: 0.8123\n",
      "Epoch 28/300\n",
      "705/705 [==============================] - 0s 267us/sample - loss: 0.7923 - val_loss: 0.8123\n",
      "Train on 706 samples, validate on 177 samples\n",
      "Epoch 1/300\n",
      "706/706 [==============================] - 1s 2ms/sample - loss: 1.4045 - val_loss: 1.2847\n",
      "Epoch 2/300\n",
      "706/706 [==============================] - 0s 420us/sample - loss: 1.2676 - val_loss: 1.1943\n",
      "Epoch 3/300\n",
      "706/706 [==============================] - 0s 301us/sample - loss: 1.2029 - val_loss: 1.1574\n",
      "Epoch 4/300\n",
      "706/706 [==============================] - 0s 339us/sample - loss: 1.1753 - val_loss: 1.1435\n",
      "Epoch 5/300\n",
      "706/706 [==============================] - 0s 264us/sample - loss: 1.1564 - val_loss: 1.1393\n",
      "Epoch 6/300\n",
      "706/706 [==============================] - 0s 268us/sample - loss: 1.1431 - val_loss: 1.1404\n",
      "Epoch 7/300\n",
      "706/706 [==============================] - 0s 309us/sample - loss: 1.1342 - val_loss: 1.1516\n",
      "Epoch 8/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "706/706 [==============================] - 0s 333us/sample - loss: 1.1289 - val_loss: 1.1475\n",
      "Epoch 9/300\n",
      "706/706 [==============================] - 0s 283us/sample - loss: 1.1232 - val_loss: 1.1520\n",
      "Epoch 10/300\n",
      "706/706 [==============================] - 0s 283us/sample - loss: 1.1176 - val_loss: 1.1493\n",
      "Epoch 11/300\n",
      "706/706 [==============================] - 0s 275us/sample - loss: 1.1151 - val_loss: 1.1503\n",
      "Epoch 12/300\n",
      "706/706 [==============================] - 0s 277us/sample - loss: 1.1113 - val_loss: 1.1557\n",
      "Epoch 13/300\n",
      "706/706 [==============================] - 0s 288us/sample - loss: 1.1071 - val_loss: 1.1606\n",
      "Epoch 14/300\n",
      "706/706 [==============================] - 0s 283us/sample - loss: 1.1041 - val_loss: 1.1590\n",
      "Epoch 15/300\n",
      "706/706 [==============================] - 0s 288us/sample - loss: 1.1012 - val_loss: 1.1655\n",
      "Train on 705 samples, validate on 175 samples\n",
      "Epoch 1/300\n",
      "705/705 [==============================] - 1s 2ms/sample - loss: 0.8756 - val_loss: 0.7078\n",
      "Epoch 2/300\n",
      "705/705 [==============================] - 0s 351us/sample - loss: 0.7764 - val_loss: 0.6904\n",
      "Epoch 3/300\n",
      "705/705 [==============================] - 0s 291us/sample - loss: 0.7750 - val_loss: 0.6899\n",
      "Epoch 4/300\n",
      "705/705 [==============================] - 0s 273us/sample - loss: 0.7748 - val_loss: 0.6911\n",
      "Epoch 5/300\n",
      "705/705 [==============================] - 0s 324us/sample - loss: 0.7755 - val_loss: 0.6903\n",
      "Epoch 6/300\n",
      "705/705 [==============================] - 0s 276us/sample - loss: 0.7748 - val_loss: 0.6902\n",
      "Epoch 7/300\n",
      "705/705 [==============================] - 0s 279us/sample - loss: 0.7750 - val_loss: 0.6917\n",
      "Epoch 8/300\n",
      "705/705 [==============================] - 0s 291us/sample - loss: 0.7752 - val_loss: 0.6912\n",
      "Epoch 9/300\n",
      "705/705 [==============================] - 0s 276us/sample - loss: 0.7746 - val_loss: 0.6904\n",
      "Epoch 10/300\n",
      "705/705 [==============================] - 0s 289us/sample - loss: 0.7751 - val_loss: 0.6895\n",
      "Epoch 11/300\n",
      "705/705 [==============================] - 0s 296us/sample - loss: 0.7749 - val_loss: 0.6901\n",
      "Epoch 12/300\n",
      "705/705 [==============================] - 0s 289us/sample - loss: 0.7747 - val_loss: 0.6898\n",
      "Epoch 13/300\n",
      "705/705 [==============================] - 0s 273us/sample - loss: 0.7744 - val_loss: 0.6901\n",
      "Epoch 14/300\n",
      "705/705 [==============================] - 0s 270us/sample - loss: 0.7751 - val_loss: 0.6902\n",
      "Epoch 15/300\n",
      "705/705 [==============================] - 0s 277us/sample - loss: 0.7754 - val_loss: 0.6900\n",
      "Epoch 16/300\n",
      "705/705 [==============================] - 0s 284us/sample - loss: 0.7754 - val_loss: 0.6915\n",
      "Epoch 17/300\n",
      "705/705 [==============================] - 0s 315us/sample - loss: 0.7745 - val_loss: 0.6903\n",
      "Epoch 18/300\n",
      "705/705 [==============================] - 0s 351us/sample - loss: 0.7747 - val_loss: 0.6893\n",
      "Epoch 19/300\n",
      "705/705 [==============================] - 0s 340us/sample - loss: 0.7746 - val_loss: 0.6916\n",
      "Epoch 20/300\n",
      "705/705 [==============================] - 0s 283us/sample - loss: 0.7749 - val_loss: 0.6911\n",
      "Epoch 21/300\n",
      "705/705 [==============================] - 0s 260us/sample - loss: 0.7751 - val_loss: 0.6913\n",
      "Epoch 22/300\n",
      "705/705 [==============================] - 0s 260us/sample - loss: 0.7748 - val_loss: 0.6900\n",
      "Epoch 23/300\n",
      "705/705 [==============================] - 0s 283us/sample - loss: 0.7745 - val_loss: 0.6899\n",
      "Epoch 24/300\n",
      "705/705 [==============================] - 0s 265us/sample - loss: 0.7752 - val_loss: 0.6895\n",
      "Epoch 25/300\n",
      "705/705 [==============================] - 0s 262us/sample - loss: 0.7751 - val_loss: 0.6905\n",
      "Epoch 26/300\n",
      "705/705 [==============================] - 0s 265us/sample - loss: 0.7749 - val_loss: 0.6919\n",
      "Epoch 27/300\n",
      "705/705 [==============================] - 0s 311us/sample - loss: 0.7752 - val_loss: 0.6910\n",
      "Epoch 28/300\n",
      "705/705 [==============================] - 0s 284us/sample - loss: 0.7748 - val_loss: 0.6905\n",
      "Train on 705 samples, validate on 175 samples\n",
      "Epoch 1/300\n",
      "705/705 [==============================] - 1s 1ms/sample - loss: 0.8942 - val_loss: 0.8198\n",
      "Epoch 2/300\n",
      "705/705 [==============================] - 0s 274us/sample - loss: 0.8102 - val_loss: 0.8143\n",
      "Epoch 3/300\n",
      "705/705 [==============================] - 0s 279us/sample - loss: 0.8029 - val_loss: 0.8133\n",
      "Epoch 4/300\n",
      "705/705 [==============================] - 0s 284us/sample - loss: 0.7995 - val_loss: 0.8137\n",
      "Epoch 5/300\n",
      "705/705 [==============================] - 0s 282us/sample - loss: 0.7968 - val_loss: 0.8128\n",
      "Epoch 6/300\n",
      "705/705 [==============================] - 0s 259us/sample - loss: 0.7955 - val_loss: 0.8142\n",
      "Epoch 7/300\n",
      "705/705 [==============================] - 0s 265us/sample - loss: 0.7955 - val_loss: 0.8130\n",
      "Epoch 8/300\n",
      "705/705 [==============================] - 0s 287us/sample - loss: 0.7940 - val_loss: 0.8149\n",
      "Epoch 9/300\n",
      "705/705 [==============================] - 0s 291us/sample - loss: 0.7934 - val_loss: 0.8177\n",
      "Epoch 10/300\n",
      "705/705 [==============================] - 0s 267us/sample - loss: 0.7930 - val_loss: 0.8181\n",
      "Epoch 11/300\n",
      "705/705 [==============================] - 0s 265us/sample - loss: 0.7933 - val_loss: 0.8180\n",
      "Epoch 12/300\n",
      "705/705 [==============================] - 0s 289us/sample - loss: 0.7930 - val_loss: 0.8152\n",
      "Epoch 13/300\n",
      "705/705 [==============================] - 0s 291us/sample - loss: 0.7930 - val_loss: 0.8155\n",
      "Epoch 14/300\n",
      "705/705 [==============================] - 0s 267us/sample - loss: 0.7927 - val_loss: 0.8171\n",
      "Epoch 15/300\n",
      "705/705 [==============================] - 0s 260us/sample - loss: 0.7923 - val_loss: 0.8157\n",
      "Train on 706 samples, validate on 177 samples\n",
      "Epoch 1/300\n",
      "706/706 [==============================] - 1s 1ms/sample - loss: 1.4116 - val_loss: 1.2305\n",
      "Epoch 2/300\n",
      "706/706 [==============================] - 0s 256us/sample - loss: 1.2660 - val_loss: 1.1527\n",
      "Epoch 3/300\n",
      "706/706 [==============================] - 0s 271us/sample - loss: 1.1944 - val_loss: 1.1218\n",
      "Epoch 4/300\n",
      "706/706 [==============================] - 0s 274us/sample - loss: 1.1645 - val_loss: 1.1066\n",
      "Epoch 5/300\n",
      "706/706 [==============================] - 0s 270us/sample - loss: 1.1494 - val_loss: 1.0997\n",
      "Epoch 6/300\n",
      "706/706 [==============================] - 0s 281us/sample - loss: 1.1400 - val_loss: 1.0942\n",
      "Epoch 7/300\n",
      "706/706 [==============================] - 0s 271us/sample - loss: 1.1333 - val_loss: 1.0908\n",
      "Epoch 8/300\n",
      "706/706 [==============================] - 0s 273us/sample - loss: 1.1286 - val_loss: 1.0880\n",
      "Epoch 9/300\n",
      "706/706 [==============================] - 0s 298us/sample - loss: 1.1234 - val_loss: 1.0873\n",
      "Epoch 10/300\n",
      "706/706 [==============================] - 0s 322us/sample - loss: 1.1206 - val_loss: 1.0866\n",
      "Epoch 11/300\n",
      "706/706 [==============================] - 0s 298us/sample - loss: 1.1164 - val_loss: 1.0844\n",
      "Epoch 12/300\n",
      "706/706 [==============================] - 0s 285us/sample - loss: 1.1134 - val_loss: 1.0845\n",
      "Epoch 13/300\n",
      "706/706 [==============================] - 0s 302us/sample - loss: 1.1107 - val_loss: 1.0834\n",
      "Epoch 14/300\n",
      "706/706 [==============================] - 0s 290us/sample - loss: 1.1069 - val_loss: 1.0833\n",
      "Epoch 15/300\n",
      "706/706 [==============================] - 0s 299us/sample - loss: 1.1044 - val_loss: 1.0821\n",
      "Epoch 16/300\n",
      "706/706 [==============================] - 0s 290us/sample - loss: 1.1016 - val_loss: 1.0815\n",
      "Epoch 17/300\n",
      "706/706 [==============================] - 0s 267us/sample - loss: 1.0989 - val_loss: 1.0808\n",
      "Epoch 18/300\n",
      "706/706 [==============================] - 0s 268us/sample - loss: 1.0977 - val_loss: 1.0807\n",
      "Epoch 19/300\n",
      "706/706 [==============================] - 0s 266us/sample - loss: 1.0938 - val_loss: 1.0801\n",
      "Epoch 20/300\n",
      "706/706 [==============================] - 0s 280us/sample - loss: 1.0907 - val_loss: 1.0808\n",
      "Epoch 21/300\n",
      "706/706 [==============================] - 0s 275us/sample - loss: 1.0879 - val_loss: 1.0804\n",
      "Epoch 22/300\n",
      "706/706 [==============================] - 0s 271us/sample - loss: 1.0854 - val_loss: 1.0797\n",
      "Epoch 23/300\n",
      "706/706 [==============================] - 0s 274us/sample - loss: 1.0830 - val_loss: 1.0794\n",
      "Epoch 24/300\n",
      "706/706 [==============================] - 0s 277us/sample - loss: 1.0805 - val_loss: 1.0796\n",
      "Epoch 25/300\n",
      "706/706 [==============================] - 0s 268us/sample - loss: 1.0770 - val_loss: 1.0780\n",
      "Epoch 26/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "706/706 [==============================] - 0s 275us/sample - loss: 1.0761 - val_loss: 1.0772\n",
      "Epoch 27/300\n",
      "706/706 [==============================] - 0s 274us/sample - loss: 1.0730 - val_loss: 1.0779\n",
      "Epoch 28/300\n",
      "706/706 [==============================] - 0s 274us/sample - loss: 1.0708 - val_loss: 1.0771\n",
      "Epoch 29/300\n",
      "706/706 [==============================] - 0s 267us/sample - loss: 1.0673 - val_loss: 1.0766\n",
      "Epoch 30/300\n",
      "706/706 [==============================] - 0s 264us/sample - loss: 1.0648 - val_loss: 1.0764\n",
      "Epoch 31/300\n",
      "706/706 [==============================] - 0s 259us/sample - loss: 1.0628 - val_loss: 1.0768\n",
      "Epoch 32/300\n",
      "706/706 [==============================] - 0s 274us/sample - loss: 1.0607 - val_loss: 1.0763\n",
      "Epoch 33/300\n",
      "706/706 [==============================] - 0s 302us/sample - loss: 1.0576 - val_loss: 1.0765\n",
      "Epoch 34/300\n",
      "706/706 [==============================] - 0s 294us/sample - loss: 1.0545 - val_loss: 1.0763\n",
      "Epoch 35/300\n",
      "706/706 [==============================] - 0s 267us/sample - loss: 1.0518 - val_loss: 1.0762\n",
      "Epoch 36/300\n",
      "706/706 [==============================] - 0s 287us/sample - loss: 1.0494 - val_loss: 1.0756\n",
      "Epoch 37/300\n",
      "706/706 [==============================] - 0s 270us/sample - loss: 1.0476 - val_loss: 1.0765\n",
      "Epoch 38/300\n",
      "706/706 [==============================] - 0s 266us/sample - loss: 1.0453 - val_loss: 1.0755\n",
      "Epoch 39/300\n",
      "706/706 [==============================] - 0s 283us/sample - loss: 1.0415 - val_loss: 1.0751\n",
      "Epoch 40/300\n",
      "706/706 [==============================] - 0s 271us/sample - loss: 1.0385 - val_loss: 1.0736\n",
      "Epoch 41/300\n",
      "706/706 [==============================] - 0s 277us/sample - loss: 1.0359 - val_loss: 1.0769\n",
      "Epoch 42/300\n",
      "706/706 [==============================] - 0s 270us/sample - loss: 1.0342 - val_loss: 1.0731\n",
      "Epoch 43/300\n",
      "706/706 [==============================] - 0s 268us/sample - loss: 1.0332 - val_loss: 1.0749\n",
      "Epoch 44/300\n",
      "706/706 [==============================] - 0s 285us/sample - loss: 1.0306 - val_loss: 1.0757\n",
      "Epoch 45/300\n",
      "706/706 [==============================] - 0s 273us/sample - loss: 1.0265 - val_loss: 1.0747\n",
      "Epoch 46/300\n",
      "706/706 [==============================] - 0s 268us/sample - loss: 1.0242 - val_loss: 1.0745\n",
      "Epoch 47/300\n",
      "706/706 [==============================] - 0s 270us/sample - loss: 1.0214 - val_loss: 1.0744\n",
      "Epoch 48/300\n",
      "706/706 [==============================] - 0s 264us/sample - loss: 1.0198 - val_loss: 1.0740\n",
      "Epoch 49/300\n",
      "706/706 [==============================] - 0s 263us/sample - loss: 1.0174 - val_loss: 1.0741\n",
      "Epoch 50/300\n",
      "706/706 [==============================] - 0s 271us/sample - loss: 1.0149 - val_loss: 1.0738\n",
      "Epoch 51/300\n",
      "706/706 [==============================] - 0s 283us/sample - loss: 1.0125 - val_loss: 1.0718\n",
      "Epoch 52/300\n",
      "706/706 [==============================] - 0s 268us/sample - loss: 1.0099 - val_loss: 1.0733\n",
      "Epoch 53/300\n",
      "706/706 [==============================] - 0s 271us/sample - loss: 1.0075 - val_loss: 1.0729\n",
      "Epoch 54/300\n",
      "706/706 [==============================] - 0s 275us/sample - loss: 1.0043 - val_loss: 1.0742\n",
      "Epoch 55/300\n",
      "706/706 [==============================] - 0s 271us/sample - loss: 1.0030 - val_loss: 1.0732\n",
      "Epoch 56/300\n",
      "706/706 [==============================] - 0s 268us/sample - loss: 1.0001 - val_loss: 1.0730\n",
      "Epoch 57/300\n",
      "706/706 [==============================] - 0s 260us/sample - loss: 0.9981 - val_loss: 1.0714\n",
      "Epoch 58/300\n",
      "706/706 [==============================] - 0s 274us/sample - loss: 0.9971 - val_loss: 1.0717\n",
      "Epoch 59/300\n",
      "706/706 [==============================] - 0s 273us/sample - loss: 0.9932 - val_loss: 1.0723\n",
      "Epoch 60/300\n",
      "706/706 [==============================] - 0s 264us/sample - loss: 0.9915 - val_loss: 1.0712\n",
      "Epoch 61/300\n",
      "706/706 [==============================] - 0s 263us/sample - loss: 1.0034 - val_loss: 1.0716\n",
      "Epoch 62/300\n",
      "706/706 [==============================] - 0s 259us/sample - loss: 0.9912 - val_loss: 1.0729\n",
      "Epoch 63/300\n",
      "706/706 [==============================] - 0s 263us/sample - loss: 0.9865 - val_loss: 1.0723\n",
      "Epoch 64/300\n",
      "706/706 [==============================] - ETA: 0s - loss: 0.967 - 0s 263us/sample - loss: 0.9840 - val_loss: 1.0726\n",
      "Epoch 65/300\n",
      "706/706 [==============================] - 0s 268us/sample - loss: 0.9823 - val_loss: 1.0710\n",
      "Epoch 66/300\n",
      "706/706 [==============================] - 0s 263us/sample - loss: 0.9796 - val_loss: 1.0697\n",
      "Epoch 67/300\n",
      "706/706 [==============================] - 0s 283us/sample - loss: 0.9773 - val_loss: 1.0724\n",
      "Epoch 68/300\n",
      "706/706 [==============================] - 0s 267us/sample - loss: 0.9741 - val_loss: 1.0694\n",
      "Epoch 69/300\n",
      "706/706 [==============================] - 0s 264us/sample - loss: 0.9732 - val_loss: 1.0692\n",
      "Epoch 70/300\n",
      "706/706 [==============================] - 0s 263us/sample - loss: 0.9714 - val_loss: 1.0705\n",
      "Epoch 71/300\n",
      "706/706 [==============================] - 0s 263us/sample - loss: 0.9692 - val_loss: 1.0725\n",
      "Epoch 72/300\n",
      "706/706 [==============================] - 0s 280us/sample - loss: 0.9667 - val_loss: 1.0695\n",
      "Epoch 73/300\n",
      "706/706 [==============================] - 0s 267us/sample - loss: 0.9647 - val_loss: 1.0714\n",
      "Epoch 74/300\n",
      "706/706 [==============================] - 0s 268us/sample - loss: 0.9635 - val_loss: 1.0705\n",
      "Epoch 75/300\n",
      "706/706 [==============================] - 0s 267us/sample - loss: 0.9621 - val_loss: 1.0706\n",
      "Epoch 76/300\n",
      "706/706 [==============================] - 0s 267us/sample - loss: 0.9589 - val_loss: 1.0710\n",
      "Epoch 77/300\n",
      "706/706 [==============================] - 0s 277us/sample - loss: 0.9571 - val_loss: 1.0703\n",
      "Epoch 78/300\n",
      "706/706 [==============================] - 0s 263us/sample - loss: 0.9540 - val_loss: 1.0701\n",
      "Epoch 79/300\n",
      "706/706 [==============================] - 0s 267us/sample - loss: 0.9536 - val_loss: 1.0717\n",
      "Train on 1370 samples, validate on 343 samples\n",
      "Epoch 1/300\n",
      "1370/1370 [==============================] - 1s 1ms/sample - loss: 0.4201 - val_loss: 0.4111\n",
      "Epoch 2/300\n",
      "1370/1370 [==============================] - 0s 320us/sample - loss: 0.3937 - val_loss: 0.4091\n",
      "Epoch 3/300\n",
      "1370/1370 [==============================] - 0s 339us/sample - loss: 0.3893 - val_loss: 0.4047\n",
      "Epoch 4/300\n",
      "1370/1370 [==============================] - 0s 328us/sample - loss: 0.3852 - val_loss: 0.4033\n",
      "Epoch 5/300\n",
      "1370/1370 [==============================] - 0s 317us/sample - loss: 0.3829 - val_loss: 0.4043\n",
      "Epoch 6/300\n",
      "1370/1370 [==============================] - 0s 328us/sample - loss: 0.3807 - val_loss: 0.4022\n",
      "Epoch 7/300\n",
      "1370/1370 [==============================] - 0s 319us/sample - loss: 0.3779 - val_loss: 0.4036\n",
      "Epoch 8/300\n",
      "1370/1370 [==============================] - 0s 342us/sample - loss: 0.3754 - val_loss: 0.4069\n",
      "Epoch 9/300\n",
      "1370/1370 [==============================] - 0s 309us/sample - loss: 0.3732 - val_loss: 0.4040\n",
      "Epoch 10/300\n",
      "1370/1370 [==============================] - 0s 319us/sample - loss: 0.3708 - val_loss: 0.4053\n",
      "Epoch 11/300\n",
      "1370/1370 [==============================] - 0s 317us/sample - loss: 0.3679 - val_loss: 0.4121\n",
      "Epoch 12/300\n",
      "1370/1370 [==============================] - 0s 329us/sample - loss: 0.3670 - val_loss: 0.4039\n",
      "Epoch 13/300\n",
      "1370/1370 [==============================] - 0s 328us/sample - loss: 0.3629 - val_loss: 0.4077\n",
      "Epoch 14/300\n",
      "1370/1370 [==============================] - 0s 320us/sample - loss: 0.3617 - val_loss: 0.4066\n",
      "Epoch 15/300\n",
      "1370/1370 [==============================] - 0s 322us/sample - loss: 0.3577 - val_loss: 0.4107\n",
      "Epoch 16/300\n",
      "1370/1370 [==============================] - 0s 320us/sample - loss: 0.3562 - val_loss: 0.4090\n",
      "Train on 1370 samples, validate on 343 samples\n",
      "Epoch 1/300\n",
      "1370/1370 [==============================] - 1s 973us/sample - loss: 0.4803 - val_loss: 0.4152\n",
      "Epoch 2/300\n",
      "1370/1370 [==============================] - 0s 320us/sample - loss: 0.4345 - val_loss: 0.4068\n",
      "Epoch 3/300\n",
      "1370/1370 [==============================] - 0s 318us/sample - loss: 0.4239 - val_loss: 0.4059\n",
      "Epoch 4/300\n",
      "1370/1370 [==============================] - 0s 317us/sample - loss: 0.4166 - val_loss: 0.4065\n",
      "Epoch 5/300\n",
      "1370/1370 [==============================] - 0s 327us/sample - loss: 0.4094 - val_loss: 0.4056\n",
      "Epoch 6/300\n",
      "1370/1370 [==============================] - 0s 325us/sample - loss: 0.4035 - val_loss: 0.4080\n",
      "Epoch 7/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1370/1370 [==============================] - 0s 321us/sample - loss: 0.3964 - val_loss: 0.4081\n",
      "Epoch 8/300\n",
      "1370/1370 [==============================] - 0s 317us/sample - loss: 0.3879 - val_loss: 0.4118\n",
      "Epoch 9/300\n",
      "1370/1370 [==============================] - 0s 317us/sample - loss: 0.3804 - val_loss: 0.4084\n",
      "Epoch 10/300\n",
      "1370/1370 [==============================] - 0s 312us/sample - loss: 0.3726 - val_loss: 0.4085\n",
      "Epoch 11/300\n",
      "1370/1370 [==============================] - 0s 324us/sample - loss: 0.3618 - val_loss: 0.4135\n",
      "Epoch 12/300\n",
      "1370/1370 [==============================] - 0s 363us/sample - loss: 0.3538 - val_loss: 0.4106\n",
      "Epoch 13/300\n",
      "1370/1370 [==============================] - 0s 361us/sample - loss: 0.3436 - val_loss: 0.4187\n",
      "Epoch 14/300\n",
      "1370/1370 [==============================] - 0s 331us/sample - loss: 0.3367 - val_loss: 0.4123\n",
      "Epoch 15/300\n",
      "1370/1370 [==============================] - 0s 324us/sample - loss: 0.3263 - val_loss: 0.4189\n",
      "Train on 1371 samples, validate on 344 samples\n",
      "Epoch 1/300\n",
      "1371/1371 [==============================] - 1s 1ms/sample - loss: 1.0691 - val_loss: 0.9902\n",
      "Epoch 2/300\n",
      "1371/1371 [==============================] - 0s 308us/sample - loss: 0.9729 - val_loss: 0.9807\n",
      "Epoch 3/300\n",
      "1371/1371 [==============================] - 0s 306us/sample - loss: 0.9474 - val_loss: 0.9841\n",
      "Epoch 4/300\n",
      "1371/1371 [==============================] - 0s 309us/sample - loss: 0.9351 - val_loss: 0.9829\n",
      "Epoch 5/300\n",
      "1371/1371 [==============================] - 0s 306us/sample - loss: 0.9235 - val_loss: 0.9815\n",
      "Epoch 6/300\n",
      "1371/1371 [==============================] - 0s 309us/sample - loss: 0.9098 - val_loss: 0.9823\n",
      "Epoch 7/300\n",
      "1371/1371 [==============================] - 0s 309us/sample - loss: 0.8976 - val_loss: 0.9795\n",
      "Epoch 8/300\n",
      "1371/1371 [==============================] - 0s 309us/sample - loss: 0.8873 - val_loss: 0.9793\n",
      "Epoch 9/300\n",
      "1371/1371 [==============================] - 0s 307us/sample - loss: 0.8717 - val_loss: 0.9836\n",
      "Epoch 10/300\n",
      "1371/1371 [==============================] - 0s 306us/sample - loss: 0.8641 - val_loss: 0.9875\n",
      "Epoch 11/300\n",
      "1371/1371 [==============================] - 0s 307us/sample - loss: 0.8411 - val_loss: 0.9877\n",
      "Epoch 12/300\n",
      "1371/1371 [==============================] - 0s 307us/sample - loss: 0.8268 - val_loss: 0.9849\n",
      "Epoch 13/300\n",
      "1371/1371 [==============================] - 0s 320us/sample - loss: 0.8118 - val_loss: 0.9884\n",
      "Epoch 14/300\n",
      "1371/1371 [==============================] - 0s 311us/sample - loss: 0.7941 - val_loss: 0.9981\n",
      "Epoch 15/300\n",
      "1371/1371 [==============================] - 0s 309us/sample - loss: 0.7753 - val_loss: 0.9984\n",
      "Epoch 16/300\n",
      "1371/1371 [==============================] - 0s 311us/sample - loss: 0.7588 - val_loss: 1.0073\n",
      "Epoch 17/300\n",
      "1371/1371 [==============================] - 0s 308us/sample - loss: 0.7427 - val_loss: 1.0047\n",
      "Epoch 18/300\n",
      "1371/1371 [==============================] - 0s 312us/sample - loss: 0.7219 - val_loss: 1.0170\n",
      "Train on 1370 samples, validate on 343 samples\n",
      "Epoch 1/300\n",
      "1370/1370 [==============================] - 1s 906us/sample - loss: 0.4216 - val_loss: 0.3783\n",
      "Epoch 2/300\n",
      "1370/1370 [==============================] - 0s 309us/sample - loss: 0.4003 - val_loss: 0.3756\n",
      "Epoch 3/300\n",
      "1370/1370 [==============================] - 0s 339us/sample - loss: 0.3954 - val_loss: 0.3755\n",
      "Epoch 4/300\n",
      "1370/1370 [==============================] - 0s 312us/sample - loss: 0.3927 - val_loss: 0.3752\n",
      "Epoch 5/300\n",
      "1370/1370 [==============================] - 0s 314us/sample - loss: 0.3891 - val_loss: 0.3746\n",
      "Epoch 6/300\n",
      "1370/1370 [==============================] - 0s 316us/sample - loss: 0.3869 - val_loss: 0.3726\n",
      "Epoch 7/300\n",
      "1370/1370 [==============================] - 0s 314us/sample - loss: 0.3844 - val_loss: 0.3733\n",
      "Epoch 8/300\n",
      "1370/1370 [==============================] - 0s 313us/sample - loss: 0.3819 - val_loss: 0.3730\n",
      "Epoch 9/300\n",
      "1370/1370 [==============================] - 0s 312us/sample - loss: 0.3787 - val_loss: 0.3729\n",
      "Epoch 10/300\n",
      "1370/1370 [==============================] - 0s 310us/sample - loss: 0.3772 - val_loss: 0.3750\n",
      "Epoch 11/300\n",
      "1370/1370 [==============================] - 0s 312us/sample - loss: 0.3746 - val_loss: 0.3746\n",
      "Epoch 12/300\n",
      "1370/1370 [==============================] - 0s 310us/sample - loss: 0.3721 - val_loss: 0.3739\n",
      "Epoch 13/300\n",
      "1370/1370 [==============================] - 0s 317us/sample - loss: 0.3686 - val_loss: 0.3758\n",
      "Epoch 14/300\n",
      "1370/1370 [==============================] - 0s 331us/sample - loss: 0.3663 - val_loss: 0.3756\n",
      "Epoch 15/300\n",
      "1370/1370 [==============================] - 0s 322us/sample - loss: 0.3650 - val_loss: 0.3788\n",
      "Epoch 16/300\n",
      "1370/1370 [==============================] - 0s 328us/sample - loss: 0.3611 - val_loss: 0.3804\n",
      "Train on 1370 samples, validate on 343 samples\n",
      "Epoch 1/300\n",
      "1370/1370 [==============================] - 2s 1ms/sample - loss: 0.4709 - val_loss: 0.4576\n",
      "Epoch 2/300\n",
      "1370/1370 [==============================] - 0s 333us/sample - loss: 0.4292 - val_loss: 0.4499\n",
      "Epoch 3/300\n",
      "1370/1370 [==============================] - 0s 353us/sample - loss: 0.4192 - val_loss: 0.4487\n",
      "Epoch 4/300\n",
      "1370/1370 [==============================] - 0s 325us/sample - loss: 0.4120 - val_loss: 0.4481\n",
      "Epoch 5/300\n",
      "1370/1370 [==============================] - 1s 481us/sample - loss: 0.4051 - val_loss: 0.4472\n",
      "Epoch 6/300\n",
      "1370/1370 [==============================] - 0s 325us/sample - loss: 0.3991 - val_loss: 0.4485\n",
      "Epoch 7/300\n",
      "1370/1370 [==============================] - 0s 316us/sample - loss: 0.3920 - val_loss: 0.4483\n",
      "Epoch 8/300\n",
      "1370/1370 [==============================] - 1s 403us/sample - loss: 0.3873 - val_loss: 0.4511\n",
      "Epoch 9/300\n",
      "1370/1370 [==============================] - 1s 394us/sample - loss: 0.3792 - val_loss: 0.4488\n",
      "Epoch 10/300\n",
      "1370/1370 [==============================] - 0s 358us/sample - loss: 0.3695 - val_loss: 0.4501\n",
      "Epoch 11/300\n",
      "1370/1370 [==============================] - 0s 363us/sample - loss: 0.3614 - val_loss: 0.4522\n",
      "Epoch 12/300\n",
      "1370/1370 [==============================] - 0s 352us/sample - loss: 0.3503 - val_loss: 0.4527\n",
      "Epoch 13/300\n",
      "1370/1370 [==============================] - 0s 323us/sample - loss: 0.3416 - val_loss: 0.4543\n",
      "Epoch 14/300\n",
      "1370/1370 [==============================] - 0s 365us/sample - loss: 0.3327 - val_loss: 0.4582\n",
      "Epoch 15/300\n",
      "1370/1370 [==============================] - 0s 328us/sample - loss: 0.3234 - val_loss: 0.4565\n",
      "Train on 1372 samples, validate on 343 samples\n",
      "Epoch 1/300\n",
      "1372/1372 [==============================] - 1s 953us/sample - loss: 1.0737 - val_loss: 1.0227\n",
      "Epoch 2/300\n",
      "1372/1372 [==============================] - 0s 313us/sample - loss: 0.9800 - val_loss: 0.9908\n",
      "Epoch 3/300\n",
      "1372/1372 [==============================] - 0s 330us/sample - loss: 0.9559 - val_loss: 0.9855\n",
      "Epoch 4/300\n",
      "1372/1372 [==============================] - 0s 339us/sample - loss: 0.9439 - val_loss: 0.9755\n",
      "Epoch 5/300\n",
      "1372/1372 [==============================] - 0s 323us/sample - loss: 0.9307 - val_loss: 0.9729\n",
      "Epoch 6/300\n",
      "1372/1372 [==============================] - 0s 323us/sample - loss: 0.9218 - val_loss: 0.9723\n",
      "Epoch 7/300\n",
      "1372/1372 [==============================] - 0s 336us/sample - loss: 0.9076 - val_loss: 0.9736\n",
      "Epoch 8/300\n",
      "1372/1372 [==============================] - 0s 339us/sample - loss: 0.8972 - val_loss: 0.9758\n",
      "Epoch 9/300\n",
      "1372/1372 [==============================] - 0s 327us/sample - loss: 0.8816 - val_loss: 0.9830\n",
      "Epoch 10/300\n",
      "1372/1372 [==============================] - 0s 334us/sample - loss: 0.8685 - val_loss: 0.9783\n",
      "Epoch 11/300\n",
      "1372/1372 [==============================] - 0s 327us/sample - loss: 0.8490 - val_loss: 0.9826\n",
      "Epoch 12/300\n",
      "1372/1372 [==============================] - 0s 334us/sample - loss: 0.8341 - val_loss: 0.9924\n",
      "Epoch 13/300\n",
      "1372/1372 [==============================] - 0s 335us/sample - loss: 0.8145 - val_loss: 0.9886\n",
      "Epoch 14/300\n",
      "1372/1372 [==============================] - 0s 325us/sample - loss: 0.7953 - val_loss: 1.0030\n",
      "Epoch 15/300\n",
      "1372/1372 [==============================] - 0s 323us/sample - loss: 0.7757 - val_loss: 1.0010\n",
      "Epoch 16/300\n",
      "1372/1372 [==============================] - 0s 329us/sample - loss: 0.7606 - val_loss: 1.0056\n",
      "Train on 1370 samples, validate on 343 samples\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1370/1370 [==============================] - 2s 1ms/sample - loss: 0.4157 - val_loss: 0.4015\n",
      "Epoch 2/300\n",
      "1370/1370 [==============================] - 0s 320us/sample - loss: 0.3947 - val_loss: 0.3976\n",
      "Epoch 3/300\n",
      "1370/1370 [==============================] - 0s 338us/sample - loss: 0.3907 - val_loss: 0.3949\n",
      "Epoch 4/300\n",
      "1370/1370 [==============================] - 0s 315us/sample - loss: 0.3862 - val_loss: 0.3936\n",
      "Epoch 5/300\n",
      "1370/1370 [==============================] - 0s 341us/sample - loss: 0.3839 - val_loss: 0.3934\n",
      "Epoch 6/300\n",
      "1370/1370 [==============================] - 0s 346us/sample - loss: 0.3802 - val_loss: 0.3949\n",
      "Epoch 7/300\n",
      "1370/1370 [==============================] - 0s 357us/sample - loss: 0.3793 - val_loss: 0.3977\n",
      "Epoch 8/300\n",
      "1370/1370 [==============================] - 0s 334us/sample - loss: 0.3766 - val_loss: 0.3921\n",
      "Epoch 9/300\n",
      "1370/1370 [==============================] - 0s 313us/sample - loss: 0.3732 - val_loss: 0.3979\n",
      "Epoch 10/300\n",
      "1370/1370 [==============================] - 0s 331us/sample - loss: 0.3717 - val_loss: 0.3965\n",
      "Epoch 11/300\n",
      "1370/1370 [==============================] - 0s 336us/sample - loss: 0.3694 - val_loss: 0.3937\n",
      "Epoch 12/300\n",
      "1370/1370 [==============================] - 0s 327us/sample - loss: 0.3651 - val_loss: 0.3967\n",
      "Epoch 13/300\n",
      "1370/1370 [==============================] - 0s 334us/sample - loss: 0.3641 - val_loss: 0.4086\n",
      "Epoch 14/300\n",
      "1370/1370 [==============================] - 0s 333us/sample - loss: 0.3632 - val_loss: 0.3964\n",
      "Epoch 15/300\n",
      "1370/1370 [==============================] - 0s 332us/sample - loss: 0.3598 - val_loss: 0.3983\n",
      "Epoch 16/300\n",
      "1370/1370 [==============================] - 0s 327us/sample - loss: 0.3565 - val_loss: 0.4016\n",
      "Epoch 17/300\n",
      "1370/1370 [==============================] - 0s 341us/sample - loss: 0.3571 - val_loss: 0.3988\n",
      "Epoch 18/300\n",
      "1370/1370 [==============================] - 0s 338us/sample - loss: 0.3526 - val_loss: 0.4004\n",
      "Train on 1370 samples, validate on 343 samples\n",
      "Epoch 1/300\n",
      "1370/1370 [==============================] - 1s 968us/sample - loss: 0.4728 - val_loss: 0.4803\n",
      "Epoch 2/300\n",
      "1370/1370 [==============================] - 0s 323us/sample - loss: 0.4267 - val_loss: 0.4687\n",
      "Epoch 3/300\n",
      "1370/1370 [==============================] - 0s 335us/sample - loss: 0.4165 - val_loss: 0.4646\n",
      "Epoch 4/300\n",
      "1370/1370 [==============================] - 0s 336us/sample - loss: 0.4101 - val_loss: 0.4632\n",
      "Epoch 5/300\n",
      "1370/1370 [==============================] - 0s 333us/sample - loss: 0.4034 - val_loss: 0.4633\n",
      "Epoch 6/300\n",
      "1370/1370 [==============================] - 0s 327us/sample - loss: 0.3958 - val_loss: 0.4655\n",
      "Epoch 7/300\n",
      "1370/1370 [==============================] - 0s 327us/sample - loss: 0.3894 - val_loss: 0.4634\n",
      "Epoch 8/300\n",
      "1370/1370 [==============================] - 0s 324us/sample - loss: 0.3818 - val_loss: 0.4642\n",
      "Epoch 9/300\n",
      "1370/1370 [==============================] - 0s 333us/sample - loss: 0.3726 - val_loss: 0.4666\n",
      "Epoch 10/300\n",
      "1370/1370 [==============================] - 0s 333us/sample - loss: 0.3661 - val_loss: 0.4656\n",
      "Epoch 11/300\n",
      "1370/1370 [==============================] - 1s 402us/sample - loss: 0.3552 - val_loss: 0.4673\n",
      "Epoch 12/300\n",
      "1370/1370 [==============================] - 1s 376us/sample - loss: 0.3471 - val_loss: 0.4707\n",
      "Epoch 13/300\n",
      "1370/1370 [==============================] - 1s 455us/sample - loss: 0.3368 - val_loss: 0.4720\n",
      "Epoch 14/300\n",
      "1370/1370 [==============================] - 0s 355us/sample - loss: 0.3248 - val_loss: 0.4703\n",
      "Train on 1372 samples, validate on 343 samples\n",
      "Epoch 1/300\n",
      "1372/1372 [==============================] - 2s 1ms/sample - loss: 1.1079 - val_loss: 0.9681\n",
      "Epoch 2/300\n",
      "1372/1372 [==============================] - 1s 393us/sample - loss: 0.9878 - val_loss: 0.9578\n",
      "Epoch 3/300\n",
      "1372/1372 [==============================] - 1s 411us/sample - loss: 0.9625 - val_loss: 0.9524\n",
      "Epoch 4/300\n",
      "1372/1372 [==============================] - 1s 381us/sample - loss: 0.9467 - val_loss: 0.9569\n",
      "Epoch 5/300\n",
      "1372/1372 [==============================] - 1s 370us/sample - loss: 0.9286 - val_loss: 0.9499\n",
      "Epoch 6/300\n",
      "1372/1372 [==============================] - 0s 356us/sample - loss: 0.9194 - val_loss: 0.9611\n",
      "Epoch 7/300\n",
      "1372/1372 [==============================] - 1s 377us/sample - loss: 0.9039 - val_loss: 0.9594\n",
      "Epoch 8/300\n",
      "1372/1372 [==============================] - 1s 395us/sample - loss: 0.8913 - val_loss: 0.9532\n",
      "Epoch 9/300\n",
      "1372/1372 [==============================] - 0s 361us/sample - loss: 0.8750 - val_loss: 0.9551\n",
      "Epoch 10/300\n",
      "1372/1372 [==============================] - 1s 439us/sample - loss: 0.8596 - val_loss: 0.9598\n",
      "Epoch 11/300\n",
      "1372/1372 [==============================] - 0s 352us/sample - loss: 0.8431 - val_loss: 0.9639\n",
      "Epoch 12/300\n",
      "1372/1372 [==============================] - 0s 336us/sample - loss: 0.8286 - val_loss: 0.9674\n",
      "Epoch 13/300\n",
      "1372/1372 [==============================] - 0s 349us/sample - loss: 0.8101 - val_loss: 0.9726\n",
      "Epoch 14/300\n",
      "1372/1372 [==============================] - 0s 345us/sample - loss: 0.7983 - val_loss: 0.9774\n",
      "Epoch 15/300\n",
      "1372/1372 [==============================] - 0s 359us/sample - loss: 0.7753 - val_loss: 0.9732\n",
      "Train on 1371 samples, validate on 342 samples\n",
      "Epoch 1/300\n",
      "1371/1371 [==============================] - 2s 1ms/sample - loss: 0.4263 - val_loss: 0.3705\n",
      "Epoch 2/300\n",
      "1371/1371 [==============================] - 0s 339us/sample - loss: 0.4011 - val_loss: 0.3648\n",
      "Epoch 3/300\n",
      "1371/1371 [==============================] - 0s 331us/sample - loss: 0.3949 - val_loss: 0.3683\n",
      "Epoch 4/300\n",
      "1371/1371 [==============================] - 0s 338us/sample - loss: 0.3945 - val_loss: 0.3659\n",
      "Epoch 5/300\n",
      "1371/1371 [==============================] - 0s 328us/sample - loss: 0.3896 - val_loss: 0.3649\n",
      "Epoch 6/300\n",
      "1371/1371 [==============================] - 0s 332us/sample - loss: 0.3859 - val_loss: 0.3659\n",
      "Epoch 7/300\n",
      "1371/1371 [==============================] - 0s 334us/sample - loss: 0.3840 - val_loss: 0.3654\n",
      "Epoch 8/300\n",
      "1371/1371 [==============================] - 1s 416us/sample - loss: 0.3805 - val_loss: 0.3789\n",
      "Epoch 9/300\n",
      "1371/1371 [==============================] - 0s 351us/sample - loss: 0.3786 - val_loss: 0.3668\n",
      "Epoch 10/300\n",
      "1371/1371 [==============================] - 0s 345us/sample - loss: 0.3761 - val_loss: 0.3670\n",
      "Epoch 11/300\n",
      "1371/1371 [==============================] - 0s 348us/sample - loss: 0.3743 - val_loss: 0.3692\n",
      "Epoch 12/300\n",
      "1371/1371 [==============================] - 0s 353us/sample - loss: 0.3712 - val_loss: 0.3673\n",
      "Train on 1371 samples, validate on 342 samples\n",
      "Epoch 1/300\n",
      "1371/1371 [==============================] - 1s 1ms/sample - loss: 0.4739 - val_loss: 0.4493\n",
      "Epoch 2/300\n",
      "1371/1371 [==============================] - 0s 328us/sample - loss: 0.4337 - val_loss: 0.4411\n",
      "Epoch 3/300\n",
      "1371/1371 [==============================] - 0s 338us/sample - loss: 0.4239 - val_loss: 0.4380\n",
      "Epoch 4/300\n",
      "1371/1371 [==============================] - 0s 345us/sample - loss: 0.4165 - val_loss: 0.4370\n",
      "Epoch 5/300\n",
      "1371/1371 [==============================] - 0s 347us/sample - loss: 0.4117 - val_loss: 0.4379\n",
      "Epoch 6/300\n",
      "1371/1371 [==============================] - 0s 332us/sample - loss: 0.4039 - val_loss: 0.4393\n",
      "Epoch 7/300\n",
      "1371/1371 [==============================] - 1s 367us/sample - loss: 0.3960 - val_loss: 0.4417\n",
      "Epoch 8/300\n",
      "1371/1371 [==============================] - 1s 400us/sample - loss: 0.3896 - val_loss: 0.4436\n",
      "Epoch 9/300\n",
      "1371/1371 [==============================] - 0s 363us/sample - loss: 0.3823 - val_loss: 0.4413\n",
      "Epoch 10/300\n",
      "1371/1371 [==============================] - 1s 468us/sample - loss: 0.3738 - val_loss: 0.4489\n",
      "Epoch 11/300\n",
      "1371/1371 [==============================] - 1s 368us/sample - loss: 0.3647 - val_loss: 0.4471\n",
      "Epoch 12/300\n",
      "1371/1371 [==============================] - 1s 391us/sample - loss: 0.3581 - val_loss: 0.4468\n",
      "Epoch 13/300\n",
      "1371/1371 [==============================] - 1s 368us/sample - loss: 0.3487 - val_loss: 0.4557\n",
      "Epoch 14/300\n",
      "1371/1371 [==============================] - 0s 364us/sample - loss: 0.3410 - val_loss: 0.4507\n",
      "Train on 1372 samples, validate on 343 samples\n",
      "Epoch 1/300\n",
      "1372/1372 [==============================] - 1s 966us/sample - loss: 1.0793 - val_loss: 1.0127\n",
      "Epoch 2/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1372/1372 [==============================] - 0s 358us/sample - loss: 0.9858 - val_loss: 0.9713\n",
      "Epoch 3/300\n",
      "1372/1372 [==============================] - 0s 326us/sample - loss: 0.9625 - val_loss: 0.9716\n",
      "Epoch 4/300\n",
      "1372/1372 [==============================] - 0s 355us/sample - loss: 0.9447 - val_loss: 0.9669\n",
      "Epoch 5/300\n",
      "1372/1372 [==============================] - 0s 342us/sample - loss: 0.9315 - val_loss: 0.9597\n",
      "Epoch 6/300\n",
      "1372/1372 [==============================] - 0s 343us/sample - loss: 0.9191 - val_loss: 0.9585\n",
      "Epoch 7/300\n",
      "1372/1372 [==============================] - 1s 370us/sample - loss: 0.9082 - val_loss: 0.9608\n",
      "Epoch 8/300\n",
      "1372/1372 [==============================] - 0s 327us/sample - loss: 0.8951 - val_loss: 0.9573\n",
      "Epoch 9/300\n",
      "1372/1372 [==============================] - 0s 326us/sample - loss: 0.8832 - val_loss: 0.9576\n",
      "Epoch 10/300\n",
      "1372/1372 [==============================] - 1s 454us/sample - loss: 0.8665 - val_loss: 0.9643\n",
      "Epoch 11/300\n",
      "1372/1372 [==============================] - 0s 341us/sample - loss: 0.8531 - val_loss: 0.9622\n",
      "Epoch 12/300\n",
      "1372/1372 [==============================] - 0s 354us/sample - loss: 0.8367 - val_loss: 0.9743\n",
      "Epoch 13/300\n",
      "1372/1372 [==============================] - 0s 357us/sample - loss: 0.8202 - val_loss: 0.9745\n",
      "Epoch 14/300\n",
      "1372/1372 [==============================] - 1s 377us/sample - loss: 0.8068 - val_loss: 0.9826\n",
      "Epoch 15/300\n",
      "1372/1372 [==============================] - 0s 353us/sample - loss: 0.7886 - val_loss: 0.9888\n",
      "Epoch 16/300\n",
      "1372/1372 [==============================] - 0s 357us/sample - loss: 0.7718 - val_loss: 0.9912\n",
      "Epoch 17/300\n",
      "1372/1372 [==============================] - 0s 354us/sample - loss: 0.7555 - val_loss: 0.9801\n",
      "Epoch 18/300\n",
      "1372/1372 [==============================] - 0s 337us/sample - loss: 0.7343 - val_loss: 0.9900\n",
      "Train on 1371 samples, validate on 342 samples\n",
      "Epoch 1/300\n",
      "1371/1371 [==============================] - 1s 1ms/sample - loss: 0.4022 - val_loss: 0.4246\n",
      "Epoch 2/300\n",
      "1371/1371 [==============================] - 0s 338us/sample - loss: 0.3877 - val_loss: 0.4221\n",
      "Epoch 3/300\n",
      "1371/1371 [==============================] - 0s 330us/sample - loss: 0.3831 - val_loss: 0.4204\n",
      "Epoch 4/300\n",
      "1371/1371 [==============================] - 0s 325us/sample - loss: 0.3811 - val_loss: 0.4192\n",
      "Epoch 5/300\n",
      "1371/1371 [==============================] - 1s 433us/sample - loss: 0.3775 - val_loss: 0.4214\n",
      "Epoch 6/300\n",
      "1371/1371 [==============================] - 0s 343us/sample - loss: 0.3744 - val_loss: 0.4189\n",
      "Epoch 7/300\n",
      "1371/1371 [==============================] - 0s 333us/sample - loss: 0.3728 - val_loss: 0.4192\n",
      "Epoch 8/300\n",
      "1371/1371 [==============================] - 0s 342us/sample - loss: 0.3700 - val_loss: 0.4231\n",
      "Epoch 9/300\n",
      "1371/1371 [==============================] - 1s 365us/sample - loss: 0.3687 - val_loss: 0.4187\n",
      "Epoch 10/300\n",
      "1371/1371 [==============================] - 0s 336us/sample - loss: 0.3642 - val_loss: 0.4176\n",
      "Epoch 11/300\n",
      "1371/1371 [==============================] - 0s 363us/sample - loss: 0.3617 - val_loss: 0.4267\n",
      "Epoch 12/300\n",
      "1371/1371 [==============================] - 0s 322us/sample - loss: 0.3616 - val_loss: 0.4194\n",
      "Epoch 13/300\n",
      "1371/1371 [==============================] - 0s 322us/sample - loss: 0.3568 - val_loss: 0.4237\n",
      "Epoch 14/300\n",
      "1371/1371 [==============================] - 0s 324us/sample - loss: 0.3531 - val_loss: 0.4186\n",
      "Epoch 15/300\n",
      "1371/1371 [==============================] - ETA: 0s - loss: 0.346 - 0s 327us/sample - loss: 0.3531 - val_loss: 0.4231\n",
      "Epoch 16/300\n",
      "1371/1371 [==============================] - 0s 323us/sample - loss: 0.3487 - val_loss: 0.4201\n",
      "Epoch 17/300\n",
      "1371/1371 [==============================] - 0s 325us/sample - loss: 0.3470 - val_loss: 0.4227\n",
      "Epoch 18/300\n",
      "1371/1371 [==============================] - 0s 321us/sample - loss: 0.3441 - val_loss: 0.4225\n",
      "Epoch 19/300\n",
      "1371/1371 [==============================] - 0s 323us/sample - loss: 0.3443 - val_loss: 0.4226\n",
      "Epoch 20/300\n",
      "1371/1371 [==============================] - 0s 332us/sample - loss: 0.3392 - val_loss: 0.4231\n",
      "Train on 1371 samples, validate on 342 samples\n",
      "Epoch 1/300\n",
      "1371/1371 [==============================] - 1s 1ms/sample - loss: 0.4710 - val_loss: 0.4215\n",
      "Epoch 2/300\n",
      "1371/1371 [==============================] - 0s 332us/sample - loss: 0.4354 - val_loss: 0.4171\n",
      "Epoch 3/300\n",
      "1371/1371 [==============================] - 0s 346us/sample - loss: 0.4259 - val_loss: 0.4146\n",
      "Epoch 4/300\n",
      "1371/1371 [==============================] - 0s 351us/sample - loss: 0.4188 - val_loss: 0.4152\n",
      "Epoch 5/300\n",
      "1371/1371 [==============================] - 0s 336us/sample - loss: 0.4121 - val_loss: 0.4153\n",
      "Epoch 6/300\n",
      "1371/1371 [==============================] - 0s 346us/sample - loss: 0.4049 - val_loss: 0.4160\n",
      "Epoch 7/300\n",
      "1371/1371 [==============================] - 0s 327us/sample - loss: 0.3977 - val_loss: 0.4164\n",
      "Epoch 8/300\n",
      "1371/1371 [==============================] - 1s 368us/sample - loss: 0.3897 - val_loss: 0.4207\n",
      "Epoch 9/300\n",
      "1371/1371 [==============================] - 0s 334us/sample - loss: 0.3826 - val_loss: 0.4184\n",
      "Epoch 10/300\n",
      "1371/1371 [==============================] - 0s 337us/sample - loss: 0.3740 - val_loss: 0.4164\n",
      "Epoch 11/300\n",
      "1371/1371 [==============================] - 0s 362us/sample - loss: 0.3642 - val_loss: 0.4183\n",
      "Epoch 12/300\n",
      "1371/1371 [==============================] - 0s 362us/sample - loss: 0.3537 - val_loss: 0.4186\n",
      "Epoch 13/300\n",
      "1371/1371 [==============================] - 1s 473us/sample - loss: 0.3455 - val_loss: 0.4214\n",
      "Train on 1373 samples, validate on 342 samples\n",
      "Epoch 1/300\n",
      "1373/1373 [==============================] - 2s 1ms/sample - loss: 1.0720 - val_loss: 1.0162\n",
      "Epoch 2/300\n",
      "1373/1373 [==============================] - 0s 356us/sample - loss: 0.9791 - val_loss: 1.0159\n",
      "Epoch 3/300\n",
      "1373/1373 [==============================] - 1s 373us/sample - loss: 0.9552 - val_loss: 0.9912\n",
      "Epoch 4/300\n",
      "1373/1373 [==============================] - 0s 327us/sample - loss: 0.9406 - val_loss: 0.9948\n",
      "Epoch 5/300\n",
      "1373/1373 [==============================] - 0s 359us/sample - loss: 0.9288 - val_loss: 0.9961\n",
      "Epoch 6/300\n",
      "1373/1373 [==============================] - 0s 364us/sample - loss: 0.9188 - val_loss: 0.9877\n",
      "Epoch 7/300\n",
      "1373/1373 [==============================] - 1s 373us/sample - loss: 0.9051 - val_loss: 0.9999\n",
      "Epoch 8/300\n",
      "1373/1373 [==============================] - 1s 387us/sample - loss: 0.8933 - val_loss: 0.9897\n",
      "Epoch 9/300\n",
      "1373/1373 [==============================] - 1s 367us/sample - loss: 0.8807 - val_loss: 0.9899\n",
      "Epoch 10/300\n",
      "1373/1373 [==============================] - 1s 380us/sample - loss: 0.8668 - val_loss: 0.9905\n",
      "Epoch 11/300\n",
      "1373/1373 [==============================] - 0s 336us/sample - loss: 0.8559 - val_loss: 0.9941\n",
      "Epoch 12/300\n",
      "1373/1373 [==============================] - 0s 339us/sample - loss: 0.8436 - val_loss: 0.9947\n",
      "Epoch 13/300\n",
      "1373/1373 [==============================] - 1s 476us/sample - loss: 0.8240 - val_loss: 1.0048\n",
      "Epoch 14/300\n",
      "1373/1373 [==============================] - 1s 366us/sample - loss: 0.8114 - val_loss: 0.9976\n",
      "Epoch 15/300\n",
      "1373/1373 [==============================] - 0s 361us/sample - loss: 0.7915 - val_loss: 1.0067\n",
      "Epoch 16/300\n",
      "1373/1373 [==============================] - 1s 384us/sample - loss: 0.7769 - val_loss: 1.0083\n"
     ]
    }
   ],
   "source": [
    "metrics = full_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gene': {'overall': {'mae': 0.729087191762223,\n",
       "   'rmse': 1.2839040305460443,\n",
       "   'ndcg': 0.45843922112693863,\n",
       "   'mape': 4.479670800712651,\n",
       "   'r2': 0.24605467970300313,\n",
       "   'pearson': 0.5256146778643038,\n",
       "   'acc': 0.3389295492412082},\n",
       "  'annual': {'mae': array([0.40826308, 0.57645428, 0.75528763, 0.88053546, 1.02489551]),\n",
       "   'rmse': array([0.74224636, 0.9580917 , 1.31857998, 1.48048021, 1.66244252]),\n",
       "   'ndcg': array([0.48617829, 0.43226526, 0.16332732, 0.24416067, 0.16687128]),\n",
       "   'mape': array([3.20163363, 2.79357391, 3.35044904, 6.30414468, 6.74855274]),\n",
       "   'r2': array([0.44522414, 0.29500761, 0.18427405, 0.06208507, 0.02513317]),\n",
       "   'pearson': array([0.68097563, 0.5689953 , 0.47024685, 0.33002525, 0.27093228]),\n",
       "   'acc': array([0.60837437, 0.19376762, 0.23379079, 0.34087837, 0.31783659])}},\n",
       " 'transplant': {'overall': {'mae': 0.7670352568953716,\n",
       "   'rmse': 1.267691327415828,\n",
       "   'ndcg': 0.48531922717613984,\n",
       "   'mape': 3.808742305883883,\n",
       "   'r2': 0.4261475457845016,\n",
       "   'pearson': 0.6617118326680259,\n",
       "   'acc': 0.33631710989730734},\n",
       "  'annual': {'mae': array([0.75451946, 0.77310202, 0.73617678, 0.76385661, 0.80752141]),\n",
       "   'rmse': array([1.30790612, 1.29118932, 1.2090014 , 1.20871731, 1.30798478]),\n",
       "   'ndcg': array([0.068681  , 0.02775149, 0.02037272, 0.10714045, 0.10805537]),\n",
       "   'mape': array([3.36988022, 3.78893268, 3.14724539, 5.45189784, 3.2857554 ]),\n",
       "   'r2': array([0.42300624, 0.42488245, 0.41684172, 0.45883101, 0.41210529]),\n",
       "   'pearson': array([0.65923608, 0.66172565, 0.65415675, 0.68695122, 0.65408657]),\n",
       "   'acc': array([0.34450215, 0.33708491, 0.35520521, 0.32638885, 0.31840443])}}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gene': {'overall': {'mae': 0.7323172166659129,\n",
       "   'rmse': 1.296150959780827,\n",
       "   'ndcg': 0.39375426995433405,\n",
       "   'mape': 4.407860713666826,\n",
       "   'r2': 0.23216118467795743,\n",
       "   'pearson': 0.5154964653492857,\n",
       "   'acc': 0.34331252699477943},\n",
       "  'annual': {'mae': array([0.41667306, 0.57881098, 0.75487069, 0.88225787, 1.02897349]),\n",
       "   'rmse': array([0.75489929, 0.98624051, 1.31984687, 1.49443502, 1.67494569]),\n",
       "   'ndcg': array([0.51731997, 0.21897933, 0.15831747, 0.16538815, 0.11366666]),\n",
       "   'mape': array([3.19242965, 2.7242381 , 3.31875722, 6.27535841, 6.52852019]),\n",
       "   'r2': array([0.42758378, 0.25620273, 0.18235494, 0.04360206, 0.01077944]),\n",
       "   'pearson': array([0.66936208, 0.53171588, 0.47137955, 0.31838994, 0.25930228]),\n",
       "   'acc': array([0.60385537, 0.21832896, 0.24209849, 0.33745923, 0.31482059])}},\n",
       " 'transplant': {'overall': {'mae': 0.7708396953327952,\n",
       "   'rmse': 1.2728291969176666,\n",
       "   'ndcg': 0.49547971873051616,\n",
       "   'mape': 3.814376280019359,\n",
       "   'r2': 0.42156821867225547,\n",
       "   'pearson': 0.6577014921087001,\n",
       "   'acc': 0.32909261801286205},\n",
       "  'annual': {'mae': array([0.75587014, 0.77702044, 0.74020801, 0.76954804, 0.81155185]),\n",
       "   'rmse': array([1.30109564, 1.2970383 , 1.2171685 , 1.22326681, 1.31102306]),\n",
       "   'ndcg': array([0.03088219, 0.02123237, 0.02035066, 0.09386946, 0.12103858]),\n",
       "   'mape': array([3.26923626, 3.3395358 , 3.01516713, 6.19913425, 3.24880797]),\n",
       "   'r2': array([0.42958498, 0.41950094, 0.40894916, 0.44554339, 0.40931798]),\n",
       "   'pearson': array([0.66179803, 0.65422372, 0.64943705, 0.67761383, 0.65217729]),\n",
       "   'acc': array([0.3433267 , 0.3131761 , 0.34917541, 0.32542972, 0.31435515])}}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}