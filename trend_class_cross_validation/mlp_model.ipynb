{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "## 仅供交叉验证 前馈神经网络（NNAR）-按趋势分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "from utils import *\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "random.seed(42)\n",
    "n_input = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the gene_editing array: (2643, 17, 10)\n",
      "Shape of the transplant array: (5141, 17, 10)\n"
     ]
    }
   ],
   "source": [
    "# gene_arr_path = r'../output/gene_editing/es_with_decay.array'\n",
    "# transplant_arr_path = r'../output/transplant/es_with_decay.array'\n",
    "\n",
    "# gene_arr = pickle.load(open(gene_arr_path, mode='rb'))\n",
    "# transplant_arr = pickle.load(open(transplant_arr_path, mode='rb'))\n",
    "\n",
    "# print('Shape of the gene_editing array:',gene_arr.shape)\n",
    "# print('Shape of the transplant array:',transplant_arr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 截断数据\n",
    "2019年为无效数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the gene_editing array: (2643, 16, 10)\n",
      "Shape of the transplant array: (5141, 16, 10)\n"
     ]
    }
   ],
   "source": [
    "# gene_arr = gene_arr[:, :-1, :]\n",
    "# transplant_arr = transplant_arr[:, :-1, :]\n",
    "\n",
    "# print('Shape of the gene_editing array:',gene_arr.shape)\n",
    "# print('Shape of the transplant array:',transplant_arr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 规范数据并获取5折交叉检验所需的训练集和验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler, data = scale_data(transplant_arr, 'standard')\n",
    "\n",
    "# # 用预测第二年的类别变量作为分成Kfold的依据，不支持浮点数\n",
    "# X, y, y_cat = data[:, :n_input, :], data[:, n_input:, -2],transplant_arr[:, n_input, -1]\n",
    "# kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 按趋势划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_by_trend(data, targets):\n",
    "    up_data = []\n",
    "    down_data = []\n",
    "    up_target = []\n",
    "    down_target = []\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        a, b = np.polyfit(range(len(data[i])), data[i, :, -2].reshape(-1), 1)\n",
    "        if a > 0:\n",
    "            up_data.append(data[i])\n",
    "            up_target.append(targets[i])\n",
    "        else:\n",
    "            down_data.append(data[i])\n",
    "            down_target.append(targets[i])\n",
    "    return np.array(up_data), np.array(up_target), np.array(down_data), np.array(down_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return keras.backend.sqrt(keras.backend.mean(keras.backend.square(y_pred - y_true), axis=-1)) \n",
    "\n",
    "# def build_direct_dnn_model():\n",
    "#     model = keras.models.Sequential()\n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dense(256, activation='relu'))\n",
    "#     model.add(Dense(256, activation='relu'))\n",
    "#     model.add(Dense(5))\n",
    "    \n",
    "#     optimizer=keras.optimizers.Adam(learning_rate=1e-4)\n",
    "#     model.compile(loss=root_mean_squared_error, optimizer=optimizer)\n",
    "#     return model\n",
    "def build_direct_dnn_model(n_layers=2, n_units=256):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(Flatten())\n",
    "    for i in range(n_layers):\n",
    "        model.add(Dense(n_units, activation='relu'))\n",
    "    model.add(Dense(5))\n",
    "    \n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-4)\n",
    "    model.compile(loss=root_mean_squared_error, optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 进行训练和评估\n",
    "使用EarlyStopping和Checkpoint做训练停止方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(X, y, y_cat, kfold, scaler, n_layers, n_units):\n",
    "    overall_metrics = {\n",
    "        'mae':[],\n",
    "        'rmse':[],\n",
    "        'ndcg':[],\n",
    "        'mape':[],\n",
    "        'r2':[],\n",
    "        'pearson':[],\n",
    "        'acc':[]\n",
    "    }\n",
    "\n",
    "    annual_metrics = {\n",
    "        'mae':[],\n",
    "        'rmse':[],\n",
    "        'ndcg':[],\n",
    "        'mape':[],\n",
    "        'r2':[],\n",
    "        'pearson':[],\n",
    "        'acc':[]\n",
    "    }\n",
    "    \n",
    "    for train, test in kfold.split(X, y_cat):\n",
    "        X_train = X[train]\n",
    "        y_train = y[train]\n",
    "        X_test = X[test]\n",
    "        y_test = y[test]\n",
    "        models = []\n",
    "        \n",
    "        # 按总量划分数据集\n",
    "        X_train1, y_train1, X_train2, y_train2 = split_data_by_trend(X_train, y_train)\n",
    "        train_xs = [X_train1, X_train2]\n",
    "        train_ys = [y_train1, y_train2]\n",
    "        \n",
    "        X_test1, y_test1, X_test2, y_test2 = split_data_by_trend(X_test, y_test)\n",
    "        test_xs = [X_test1, X_test2]\n",
    "        test_ys = [y_test1, y_test2]\n",
    "        i_s = [1, 2]\n",
    "        \n",
    "        # 训练\n",
    "        for i in range(len(i_s)):\n",
    "            model = build_direct_dnn_model(n_layers, n_units)\n",
    "            history = model.fit(train_xs[i], train_ys[i], epochs=300, batch_size=16, verbose=1, validation_data=(test_xs[i], test_ys[i]),\n",
    "                           callbacks=[\n",
    "                               EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='auto', restore_best_weights=True)\n",
    "                           ])\n",
    "            models.append(model)\n",
    "        \n",
    "        # 预测\n",
    "        y_test = []\n",
    "        y_pred = []\n",
    "        for i in range(len(i_s)):\n",
    "            y_test.append(test_ys[i])\n",
    "            y_pred.append(models[i].predict(test_xs[i]).reshape(test_ys[i].shape))\n",
    "        \n",
    "        y_test = np.concatenate(y_test)\n",
    "        y_pred = np.concatenate(y_pred)\n",
    "\n",
    "        metrics = ['mae', 'rmse','ndcg', 'mape', 'r2', 'pearson', 'acc']\n",
    "        for m in metrics:\n",
    "            overall, annual = eval_model(m, y_test, y_pred, scaler)\n",
    "            overall_metrics[m].append(overall)\n",
    "            annual_metrics[m].append(annual)\n",
    "    \n",
    "    return overall_metrics, annual_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_pipeline():\n",
    "    gene_arr_path = r'../output/gene_editing/es_with_decay.array'\n",
    "    transplant_arr_path = r'../output/transplant/es_with_decay.array'\n",
    "\n",
    "    gene_arr = pickle.load(open(gene_arr_path, mode='rb'))\n",
    "    transplant_arr = pickle.load(open(transplant_arr_path, mode='rb'))\n",
    "    \n",
    "    gene_arr = gene_arr[:, :-1, :]\n",
    "    transplant_arr = transplant_arr[:, :-1, :]\n",
    "\n",
    "    print('Shape of the gene_editing array:',gene_arr.shape)\n",
    "    print('Shape of the transplant array:',transplant_arr.shape)\n",
    "    \n",
    "    metrics = {\n",
    "        'gene':{\n",
    "            'overall':{},\n",
    "            'annual':{}\n",
    "        },\n",
    "        'transplant':{\n",
    "            'overall':{},\n",
    "            'annual':{}\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for name, dataset in zip(['gene', 'transplant'], [gene_arr, transplant_arr]):\n",
    "        scaler, data = scale_data(dataset, 'standard')\n",
    "\n",
    "        # 用预测第二年的类别变量作为分成Kfold的依据，不支持浮点数\n",
    "        X, y, y_cat = data[:, :n_input, :], data[:, n_input:, -2], dataset[:, n_input, -1]\n",
    "        kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        \n",
    "#         overall_metrics, annual_metrics = cross_validation(X, y, y_cat, kfold, scaler)\n",
    "        if name == 'gene':\n",
    "            overall_metrics, annual_metrics = cross_validation(X, y, y_cat, kfold, scaler, 3, 128)\n",
    "        elif name == 'transplant':\n",
    "            overall_metrics, annual_metrics = cross_validation(X, y, y_cat, kfold, scaler, 5, 256)\n",
    "        \n",
    "        for metric, value in overall_metrics.items():\n",
    "            metrics[name]['overall'][metric] = np.mean(value)\n",
    "        \n",
    "        for metric, value in annual_metrics.items():\n",
    "            metrics[name]['annual'][metric] = np.mean(np.array(value), axis=0)\n",
    "    \n",
    "    pickle.dump(metrics, open('mlp_metrics.dict', 'wb'))\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the gene_editing array: (2643, 16, 10)\n",
      "Shape of the transplant array: (5141, 16, 10)\n",
      "Train on 988 samples, validate on 256 samples\n",
      "Epoch 1/300\n",
      "988/988 [==============================] - 1s 1ms/sample - loss: 1.2336 - val_loss: 1.1642\n",
      "Epoch 2/300\n",
      "988/988 [==============================] - 0s 199us/sample - loss: 1.1086 - val_loss: 1.1015\n",
      "Epoch 3/300\n",
      "988/988 [==============================] - 0s 203us/sample - loss: 1.0722 - val_loss: 1.0859\n",
      "Epoch 4/300\n",
      "988/988 [==============================] - 0s 197us/sample - loss: 1.0604 - val_loss: 1.0798\n",
      "Epoch 5/300\n",
      "988/988 [==============================] - 0s 199us/sample - loss: 1.0541 - val_loss: 1.0770\n",
      "Epoch 6/300\n",
      "988/988 [==============================] - 0s 216us/sample - loss: 1.0499 - val_loss: 1.0745\n",
      "Epoch 7/300\n",
      "988/988 [==============================] - 0s 212us/sample - loss: 1.0470 - val_loss: 1.0750\n",
      "Epoch 8/300\n",
      "988/988 [==============================] - 0s 224us/sample - loss: 1.0433 - val_loss: 1.0750\n",
      "Epoch 9/300\n",
      "988/988 [==============================] - 0s 245us/sample - loss: 1.0405 - val_loss: 1.0733\n",
      "Epoch 10/300\n",
      "988/988 [==============================] - 0s 216us/sample - loss: 1.0382 - val_loss: 1.0745\n",
      "Epoch 11/300\n",
      "988/988 [==============================] - 0s 211us/sample - loss: 1.0356 - val_loss: 1.0744\n",
      "Epoch 12/300\n",
      "988/988 [==============================] - 0s 214us/sample - loss: 1.0327 - val_loss: 1.0744\n",
      "Epoch 13/300\n",
      "988/988 [==============================] - 0s 212us/sample - loss: 1.0305 - val_loss: 1.0731\n",
      "Epoch 14/300\n",
      "988/988 [==============================] - 0s 201us/sample - loss: 1.0291 - val_loss: 1.0729\n",
      "Epoch 15/300\n",
      "988/988 [==============================] - 0s 198us/sample - loss: 1.0268 - val_loss: 1.0728\n",
      "Epoch 16/300\n",
      "988/988 [==============================] - 0s 199us/sample - loss: 1.0259 - val_loss: 1.0731\n",
      "Epoch 17/300\n",
      "988/988 [==============================] - 0s 226us/sample - loss: 1.0231 - val_loss: 1.0732\n",
      "Epoch 18/300\n",
      "988/988 [==============================] - 0s 203us/sample - loss: 1.0205 - val_loss: 1.0750\n",
      "Epoch 19/300\n",
      "988/988 [==============================] - 0s 209us/sample - loss: 1.0195 - val_loss: 1.0739\n",
      "Epoch 20/300\n",
      "988/988 [==============================] - 0s 339us/sample - loss: 1.0168 - val_loss: 1.0730\n",
      "Epoch 21/300\n",
      "988/988 [==============================] - 0s 234us/sample - loss: 1.0154 - val_loss: 1.0743\n",
      "Epoch 22/300\n",
      "988/988 [==============================] - 0s 234us/sample - loss: 1.0138 - val_loss: 1.0751\n",
      "Epoch 23/300\n",
      "988/988 [==============================] - 0s 226us/sample - loss: 1.0112 - val_loss: 1.0745\n",
      "Epoch 24/300\n",
      "988/988 [==============================] - 0s 213us/sample - loss: 1.0085 - val_loss: 1.0736\n",
      "Epoch 25/300\n",
      "988/988 [==============================] - 0s 227us/sample - loss: 1.0068 - val_loss: 1.0754\n",
      "Train on 1124 samples, validate on 275 samples\n",
      "Epoch 1/300\n",
      "1124/1124 [==============================] - 1s 903us/sample - loss: 0.8568 - val_loss: 0.7210\n",
      "Epoch 2/300\n",
      "1124/1124 [==============================] - 0s 217us/sample - loss: 0.7646 - val_loss: 0.7110\n",
      "Epoch 3/300\n",
      "1124/1124 [==============================] - 0s 221us/sample - loss: 0.7627 - val_loss: 0.7105\n",
      "Epoch 4/300\n",
      "1124/1124 [==============================] - 0s 217us/sample - loss: 0.7625 - val_loss: 0.7119\n",
      "Epoch 5/300\n",
      "1124/1124 [==============================] - 0s 212us/sample - loss: 0.7618 - val_loss: 0.7112\n",
      "Epoch 6/300\n",
      "1124/1124 [==============================] - 0s 227us/sample - loss: 0.7614 - val_loss: 0.7110\n",
      "Epoch 7/300\n",
      "1124/1124 [==============================] - 0s 211us/sample - loss: 0.7615 - val_loss: 0.7131\n",
      "Epoch 8/300\n",
      "1124/1124 [==============================] - 0s 219us/sample - loss: 0.7616 - val_loss: 0.7114\n",
      "Epoch 9/300\n",
      "1124/1124 [==============================] - 0s 215us/sample - loss: 0.7608 - val_loss: 0.7110\n",
      "Epoch 10/300\n",
      "1124/1124 [==============================] - 0s 222us/sample - loss: 0.7605 - val_loss: 0.7111\n",
      "Epoch 11/300\n",
      "1124/1124 [==============================] - 0s 213us/sample - loss: 0.7602 - val_loss: 0.7106\n",
      "Epoch 12/300\n",
      "1124/1124 [==============================] - 0s 221us/sample - loss: 0.7599 - val_loss: 0.7118\n",
      "Epoch 13/300\n",
      "1124/1124 [==============================] - 0s 228us/sample - loss: 0.7593 - val_loss: 0.7111\n",
      "Train on 998 samples, validate on 246 samples\n",
      "Epoch 1/300\n",
      "998/998 [==============================] - 1s 1ms/sample - loss: 1.2617 - val_loss: 1.2147\n",
      "Epoch 2/300\n",
      "998/998 [==============================] - 0s 244us/sample - loss: 1.1176 - val_loss: 1.1570\n",
      "Epoch 3/300\n",
      "998/998 [==============================] - 0s 245us/sample - loss: 1.0752 - val_loss: 1.1248\n",
      "Epoch 4/300\n",
      "998/998 [==============================] - 0s 233us/sample - loss: 1.0549 - val_loss: 1.1091\n",
      "Epoch 5/300\n",
      "998/998 [==============================] - 0s 243us/sample - loss: 1.0457 - val_loss: 1.0980\n",
      "Epoch 6/300\n",
      "998/998 [==============================] - 0s 230us/sample - loss: 1.0421 - val_loss: 1.0955\n",
      "Epoch 7/300\n",
      "998/998 [==============================] - 0s 233us/sample - loss: 1.0370 - val_loss: 1.0952\n",
      "Epoch 8/300\n",
      "998/998 [==============================] - 0s 249us/sample - loss: 1.0338 - val_loss: 1.0930\n",
      "Epoch 9/300\n",
      "998/998 [==============================] - 0s 218us/sample - loss: 1.0314 - val_loss: 1.0917\n",
      "Epoch 10/300\n",
      "998/998 [==============================] - 0s 216us/sample - loss: 1.0282 - val_loss: 1.0912\n",
      "Epoch 11/300\n",
      "998/998 [==============================] - 0s 217us/sample - loss: 1.0267 - val_loss: 1.0886\n",
      "Epoch 12/300\n",
      "998/998 [==============================] - 0s 220us/sample - loss: 1.0247 - val_loss: 1.0886\n",
      "Epoch 13/300\n",
      "998/998 [==============================] - 0s 236us/sample - loss: 1.0211 - val_loss: 1.0896\n",
      "Epoch 14/300\n",
      "998/998 [==============================] - 0s 237us/sample - loss: 1.0200 - val_loss: 1.0876\n",
      "Epoch 15/300\n",
      "998/998 [==============================] - 0s 226us/sample - loss: 1.0169 - val_loss: 1.0876\n",
      "Epoch 16/300\n",
      "998/998 [==============================] - 0s 225us/sample - loss: 1.0161 - val_loss: 1.0871\n",
      "Epoch 17/300\n",
      "998/998 [==============================] - 0s 217us/sample - loss: 1.0141 - val_loss: 1.0871\n",
      "Epoch 18/300\n",
      "998/998 [==============================] - 0s 224us/sample - loss: 1.0109 - val_loss: 1.0862\n",
      "Epoch 19/300\n",
      "998/998 [==============================] - 0s 215us/sample - loss: 1.0088 - val_loss: 1.0890\n",
      "Epoch 20/300\n",
      "998/998 [==============================] - 0s 218us/sample - loss: 1.0090 - val_loss: 1.0879\n",
      "Epoch 21/300\n",
      "998/998 [==============================] - 0s 216us/sample - loss: 1.0054 - val_loss: 1.0870\n",
      "Epoch 22/300\n",
      "998/998 [==============================] - 0s 235us/sample - loss: 1.0038 - val_loss: 1.0869\n",
      "Epoch 23/300\n",
      "998/998 [==============================] - 0s 273us/sample - loss: 1.0011 - val_loss: 1.0882\n",
      "Epoch 24/300\n",
      "998/998 [==============================] - 0s 262us/sample - loss: 0.9991 - val_loss: 1.0862\n",
      "Epoch 25/300\n",
      "998/998 [==============================] - 0s 261us/sample - loss: 0.9969 - val_loss: 1.0868\n",
      "Epoch 26/300\n",
      "998/998 [==============================] - 0s 258us/sample - loss: 0.9963 - val_loss: 1.0894\n",
      "Epoch 27/300\n",
      "998/998 [==============================] - 0s 268us/sample - loss: 0.9932 - val_loss: 1.0888\n",
      "Epoch 28/300\n",
      "998/998 [==============================] - 0s 262us/sample - loss: 0.9912 - val_loss: 1.0874\n",
      "Train on 1115 samples, validate on 284 samples\n",
      "Epoch 1/300\n",
      "1115/1115 [==============================] - 1s 1ms/sample - loss: 0.8189 - val_loss: 0.7799\n",
      "Epoch 2/300\n",
      "1115/1115 [==============================] - 0s 265us/sample - loss: 0.7490 - val_loss: 0.7769\n",
      "Epoch 3/300\n",
      "1115/1115 [==============================] - 0s 257us/sample - loss: 0.7479 - val_loss: 0.7761\n",
      "Epoch 4/300\n",
      "1115/1115 [==============================] - 0s 258us/sample - loss: 0.7473 - val_loss: 0.7761\n",
      "Epoch 5/300\n",
      "1115/1115 [==============================] - 0s 270us/sample - loss: 0.7468 - val_loss: 0.7764\n",
      "Epoch 6/300\n",
      "1115/1115 [==============================] - 0s 262us/sample - loss: 0.7461 - val_loss: 0.7762\n",
      "Epoch 7/300\n",
      "1115/1115 [==============================] - 0s 252us/sample - loss: 0.7453 - val_loss: 0.7776\n",
      "Epoch 8/300\n",
      "1115/1115 [==============================] - 0s 250us/sample - loss: 0.7452 - val_loss: 0.7763\n",
      "Epoch 9/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115/1115 [==============================] - 0s 267us/sample - loss: 0.7455 - val_loss: 0.7761\n",
      "Epoch 10/300\n",
      "1115/1115 [==============================] - 0s 265us/sample - loss: 0.7446 - val_loss: 0.7764\n",
      "Epoch 11/300\n",
      "1115/1115 [==============================] - 0s 243us/sample - loss: 0.7444 - val_loss: 0.7770\n",
      "Epoch 12/300\n",
      "1115/1115 [==============================] - 0s 265us/sample - loss: 0.7443 - val_loss: 0.7762\n",
      "Epoch 13/300\n",
      "1115/1115 [==============================] - 0s 248us/sample - loss: 0.7431 - val_loss: 0.7766\n",
      "Epoch 14/300\n",
      "1115/1115 [==============================] - 0s 264us/sample - loss: 0.7441 - val_loss: 0.7765\n",
      "Train on 995 samples, validate on 249 samples\n",
      "Epoch 1/300\n",
      "995/995 [==============================] - 1s 966us/sample - loss: 1.2050 - val_loss: 1.1160\n",
      "Epoch 2/300\n",
      "995/995 [==============================] - 0s 250us/sample - loss: 1.0893 - val_loss: 1.0674\n",
      "Epoch 3/300\n",
      "995/995 [==============================] - 0s 259us/sample - loss: 1.0698 - val_loss: 1.0587\n",
      "Epoch 4/300\n",
      "995/995 [==============================] - 0s 255us/sample - loss: 1.0633 - val_loss: 1.0528\n",
      "Epoch 5/300\n",
      "995/995 [==============================] - 0s 257us/sample - loss: 1.0570 - val_loss: 1.0512\n",
      "Epoch 6/300\n",
      "995/995 [==============================] - 0s 256us/sample - loss: 1.0535 - val_loss: 1.0479\n",
      "Epoch 7/300\n",
      "995/995 [==============================] - 0s 259us/sample - loss: 1.0504 - val_loss: 1.0481\n",
      "Epoch 8/300\n",
      "995/995 [==============================] - 0s 256us/sample - loss: 1.0467 - val_loss: 1.0479\n",
      "Epoch 9/300\n",
      "995/995 [==============================] - 0s 262us/sample - loss: 1.0450 - val_loss: 1.0458\n",
      "Epoch 10/300\n",
      "995/995 [==============================] - 0s 254us/sample - loss: 1.0415 - val_loss: 1.0468\n",
      "Epoch 11/300\n",
      "995/995 [==============================] - 0s 269us/sample - loss: 1.0386 - val_loss: 1.0462\n",
      "Epoch 12/300\n",
      "995/995 [==============================] - 0s 259us/sample - loss: 1.0362 - val_loss: 1.0470\n",
      "Epoch 13/300\n",
      "995/995 [==============================] - 0s 267us/sample - loss: 1.0336 - val_loss: 1.0466\n",
      "Epoch 14/300\n",
      "995/995 [==============================] - 0s 267us/sample - loss: 1.0309 - val_loss: 1.0454\n",
      "Epoch 15/300\n",
      "995/995 [==============================] - 0s 257us/sample - loss: 1.0286 - val_loss: 1.0473\n",
      "Epoch 16/300\n",
      "995/995 [==============================] - 0s 362us/sample - loss: 1.0265 - val_loss: 1.0450\n",
      "Epoch 17/300\n",
      "995/995 [==============================] - 0s 297us/sample - loss: 1.0252 - val_loss: 1.0472\n",
      "Epoch 18/300\n",
      "995/995 [==============================] - 0s 318us/sample - loss: 1.0220 - val_loss: 1.0471\n",
      "Epoch 19/300\n",
      "995/995 [==============================] - 0s 301us/sample - loss: 1.0184 - val_loss: 1.0460\n",
      "Epoch 20/300\n",
      "995/995 [==============================] - 0s 268us/sample - loss: 1.0175 - val_loss: 1.0461\n",
      "Epoch 21/300\n",
      "995/995 [==============================] - 0s 267us/sample - loss: 1.0146 - val_loss: 1.0477\n",
      "Epoch 22/300\n",
      "995/995 [==============================] - 0s 272us/sample - loss: 1.0125 - val_loss: 1.0464\n",
      "Epoch 23/300\n",
      "995/995 [==============================] - 0s 264us/sample - loss: 1.0117 - val_loss: 1.0463\n",
      "Epoch 24/300\n",
      "995/995 [==============================] - 0s 266us/sample - loss: 1.0085 - val_loss: 1.0459\n",
      "Epoch 25/300\n",
      "995/995 [==============================] - 0s 276us/sample - loss: 1.0062 - val_loss: 1.0483\n",
      "Epoch 26/300\n",
      "995/995 [==============================] - 0s 307us/sample - loss: 1.0037 - val_loss: 1.0488\n",
      "Train on 1120 samples, validate on 279 samples\n",
      "Epoch 1/300\n",
      "1120/1120 [==============================] - 1s 859us/sample - loss: 0.8536 - val_loss: 0.7699\n",
      "Epoch 2/300\n",
      "1120/1120 [==============================] - 0s 237us/sample - loss: 0.7528 - val_loss: 0.7625\n",
      "Epoch 3/300\n",
      "1120/1120 [==============================] - 0s 241us/sample - loss: 0.7509 - val_loss: 0.7623\n",
      "Epoch 4/300\n",
      "1120/1120 [==============================] - 0s 251us/sample - loss: 0.7503 - val_loss: 0.7619\n",
      "Epoch 5/300\n",
      "1120/1120 [==============================] - 0s 240us/sample - loss: 0.7496 - val_loss: 0.7620\n",
      "Epoch 6/300\n",
      "1120/1120 [==============================] - 0s 260us/sample - loss: 0.7495 - val_loss: 0.7620\n",
      "Epoch 7/300\n",
      "1120/1120 [==============================] - 0s 240us/sample - loss: 0.7494 - val_loss: 0.7625\n",
      "Epoch 8/300\n",
      "1120/1120 [==============================] - 0s 240us/sample - loss: 0.7483 - val_loss: 0.7614\n",
      "Epoch 9/300\n",
      "1120/1120 [==============================] - 0s 240us/sample - loss: 0.7479 - val_loss: 0.7618\n",
      "Epoch 10/300\n",
      "1120/1120 [==============================] - 0s 237us/sample - loss: 0.7476 - val_loss: 0.7624\n",
      "Epoch 11/300\n",
      "1120/1120 [==============================] - 0s 235us/sample - loss: 0.7478 - val_loss: 0.7624\n",
      "Epoch 12/300\n",
      "1120/1120 [==============================] - 0s 239us/sample - loss: 0.7475 - val_loss: 0.7616\n",
      "Epoch 13/300\n",
      "1120/1120 [==============================] - 0s 245us/sample - loss: 0.7468 - val_loss: 0.7619\n",
      "Epoch 14/300\n",
      "1120/1120 [==============================] - 0s 238us/sample - loss: 0.7467 - val_loss: 0.7626\n",
      "Epoch 15/300\n",
      "1120/1120 [==============================] - 0s 240us/sample - loss: 0.7467 - val_loss: 0.7608\n",
      "Epoch 16/300\n",
      "1120/1120 [==============================] - 0s 238us/sample - loss: 0.7461 - val_loss: 0.7612\n",
      "Epoch 17/300\n",
      "1120/1120 [==============================] - 0s 362us/sample - loss: 0.7454 - val_loss: 0.7607\n",
      "Epoch 18/300\n",
      "1120/1120 [==============================] - 0s 261us/sample - loss: 0.7448 - val_loss: 0.7602\n",
      "Epoch 19/300\n",
      "1120/1120 [==============================] - 0s 249us/sample - loss: 0.7450 - val_loss: 0.7598\n",
      "Epoch 20/300\n",
      "1120/1120 [==============================] - 0s 248us/sample - loss: 0.7445 - val_loss: 0.7605\n",
      "Epoch 21/300\n",
      "1120/1120 [==============================] - 0s 248us/sample - loss: 0.7441 - val_loss: 0.7599\n",
      "Epoch 22/300\n",
      "1120/1120 [==============================] - 0s 247us/sample - loss: 0.7441 - val_loss: 0.7605\n",
      "Epoch 23/300\n",
      "1120/1120 [==============================] - 0s 264us/sample - loss: 0.7440 - val_loss: 0.7601\n",
      "Epoch 24/300\n",
      "1120/1120 [==============================] - 0s 252us/sample - loss: 0.7432 - val_loss: 0.7602\n",
      "Epoch 25/300\n",
      "1120/1120 [==============================] - 0s 249us/sample - loss: 0.7428 - val_loss: 0.7609\n",
      "Epoch 26/300\n",
      "1120/1120 [==============================] - 0s 253us/sample - loss: 0.7427 - val_loss: 0.7598\n",
      "Epoch 27/300\n",
      "1120/1120 [==============================] - 0s 311us/sample - loss: 0.7429 - val_loss: 0.7618\n",
      "Epoch 28/300\n",
      "1120/1120 [==============================] - 0s 248us/sample - loss: 0.7430 - val_loss: 0.7601\n",
      "Epoch 29/300\n",
      "1120/1120 [==============================] - 0s 303us/sample - loss: 0.7421 - val_loss: 0.7608\n",
      "Train on 991 samples, validate on 253 samples\n",
      "Epoch 1/300\n",
      "991/991 [==============================] - 1s 992us/sample - loss: 1.2780 - val_loss: 1.1508\n",
      "Epoch 2/300\n",
      "991/991 [==============================] - 0s 251us/sample - loss: 1.1267 - val_loss: 1.0709\n",
      "Epoch 3/300\n",
      "991/991 [==============================] - 0s 255us/sample - loss: 1.0845 - val_loss: 1.0537\n",
      "Epoch 4/300\n",
      "991/991 [==============================] - 0s 259us/sample - loss: 1.0654 - val_loss: 1.0567\n",
      "Epoch 5/300\n",
      "991/991 [==============================] - 0s 260us/sample - loss: 1.0571 - val_loss: 1.0584\n",
      "Epoch 6/300\n",
      "991/991 [==============================] - 0s 264us/sample - loss: 1.0513 - val_loss: 1.0618\n",
      "Epoch 7/300\n",
      "991/991 [==============================] - 0s 259us/sample - loss: 1.0475 - val_loss: 1.0630\n",
      "Epoch 8/300\n",
      "991/991 [==============================] - 0s 254us/sample - loss: 1.0444 - val_loss: 1.0662\n",
      "Epoch 9/300\n",
      "991/991 [==============================] - 0s 253us/sample - loss: 1.0413 - val_loss: 1.0660\n",
      "Epoch 10/300\n",
      "991/991 [==============================] - 0s 256us/sample - loss: 1.0385 - val_loss: 1.0732\n",
      "Epoch 11/300\n",
      "991/991 [==============================] - 0s 252us/sample - loss: 1.0362 - val_loss: 1.0716\n",
      "Epoch 12/300\n",
      "991/991 [==============================] - 0s 385us/sample - loss: 1.0329 - val_loss: 1.0690\n",
      "Epoch 13/300\n",
      "991/991 [==============================] - 0s 375us/sample - loss: 1.0300 - val_loss: 1.0728\n",
      "Train on 1125 samples, validate on 274 samples\n",
      "Epoch 1/300\n",
      "1125/1125 [==============================] - 1s 965us/sample - loss: 0.8274 - val_loss: 0.7953\n",
      "Epoch 2/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1125/1125 [==============================] - 0s 248us/sample - loss: 0.7459 - val_loss: 0.7901\n",
      "Epoch 3/300\n",
      "1125/1125 [==============================] - 0s 256us/sample - loss: 0.7449 - val_loss: 0.7901\n",
      "Epoch 4/300\n",
      "1125/1125 [==============================] - 0s 263us/sample - loss: 0.7443 - val_loss: 0.7892\n",
      "Epoch 5/300\n",
      "1125/1125 [==============================] - 0s 247us/sample - loss: 0.7442 - val_loss: 0.7894\n",
      "Epoch 6/300\n",
      "1125/1125 [==============================] - 0s 243us/sample - loss: 0.7433 - val_loss: 0.7899\n",
      "Epoch 7/300\n",
      "1125/1125 [==============================] - 0s 245us/sample - loss: 0.7434 - val_loss: 0.7904\n",
      "Epoch 8/300\n",
      "1125/1125 [==============================] - 0s 246us/sample - loss: 0.7433 - val_loss: 0.7908\n",
      "Epoch 9/300\n",
      "1125/1125 [==============================] - 0s 246us/sample - loss: 0.7426 - val_loss: 0.7897\n",
      "Epoch 10/300\n",
      "1125/1125 [==============================] - 0s 277us/sample - loss: 0.7419 - val_loss: 0.7902\n",
      "Epoch 11/300\n",
      "1125/1125 [==============================] - 0s 282us/sample - loss: 0.7410 - val_loss: 0.7906\n",
      "Epoch 12/300\n",
      "1125/1125 [==============================] - 0s 258us/sample - loss: 0.7414 - val_loss: 0.7897\n",
      "Epoch 13/300\n",
      "1125/1125 [==============================] - 0s 262us/sample - loss: 0.7406 - val_loss: 0.7908\n",
      "Epoch 14/300\n",
      "1125/1125 [==============================] - 0s 271us/sample - loss: 0.7403 - val_loss: 0.7902\n",
      "Train on 1004 samples, validate on 240 samples\n",
      "Epoch 1/300\n",
      "1004/1004 [==============================] - 1s 941us/sample - loss: 1.2488 - val_loss: 1.0883\n",
      "Epoch 2/300\n",
      "1004/1004 [==============================] - 0s 239us/sample - loss: 1.1078 - val_loss: 1.0558\n",
      "Epoch 3/300\n",
      "1004/1004 [==============================] - 0s 250us/sample - loss: 1.0774 - val_loss: 1.0398\n",
      "Epoch 4/300\n",
      "1004/1004 [==============================] - 0s 264us/sample - loss: 1.0663 - val_loss: 1.0344\n",
      "Epoch 5/300\n",
      "1004/1004 [==============================] - 0s 241us/sample - loss: 1.0603 - val_loss: 1.0302\n",
      "Epoch 6/300\n",
      "1004/1004 [==============================] - 0s 245us/sample - loss: 1.0557 - val_loss: 1.0286\n",
      "Epoch 7/300\n",
      "1004/1004 [==============================] - 0s 249us/sample - loss: 1.0519 - val_loss: 1.0280\n",
      "Epoch 8/300\n",
      "1004/1004 [==============================] - 0s 251us/sample - loss: 1.0484 - val_loss: 1.0261\n",
      "Epoch 9/300\n",
      "1004/1004 [==============================] - 0s 245us/sample - loss: 1.0452 - val_loss: 1.0252\n",
      "Epoch 10/300\n",
      "1004/1004 [==============================] - 0s 252us/sample - loss: 1.0428 - val_loss: 1.0241\n",
      "Epoch 11/300\n",
      "1004/1004 [==============================] - 0s 244us/sample - loss: 1.0396 - val_loss: 1.0251\n",
      "Epoch 12/300\n",
      "1004/1004 [==============================] - 0s 244us/sample - loss: 1.0365 - val_loss: 1.0222\n",
      "Epoch 13/300\n",
      "1004/1004 [==============================] - 0s 244us/sample - loss: 1.0344 - val_loss: 1.0235\n",
      "Epoch 14/300\n",
      "1004/1004 [==============================] - 0s 244us/sample - loss: 1.0318 - val_loss: 1.0232\n",
      "Epoch 15/300\n",
      "1004/1004 [==============================] - 0s 241us/sample - loss: 1.0296 - val_loss: 1.0227\n",
      "Epoch 16/300\n",
      "1004/1004 [==============================] - 0s 248us/sample - loss: 1.0268 - val_loss: 1.0221\n",
      "Epoch 17/300\n",
      "1004/1004 [==============================] - 0s 399us/sample - loss: 1.0249 - val_loss: 1.0203\n",
      "Epoch 18/300\n",
      "1004/1004 [==============================] - 0s 251us/sample - loss: 1.0232 - val_loss: 1.0214\n",
      "Epoch 19/300\n",
      "1004/1004 [==============================] - 0s 260us/sample - loss: 1.0207 - val_loss: 1.0225\n",
      "Epoch 20/300\n",
      "1004/1004 [==============================] - 0s 267us/sample - loss: 1.0181 - val_loss: 1.0219\n",
      "Epoch 21/300\n",
      "1004/1004 [==============================] - 0s 249us/sample - loss: 1.0155 - val_loss: 1.0205\n",
      "Epoch 22/300\n",
      "1004/1004 [==============================] - 0s 253us/sample - loss: 1.0135 - val_loss: 1.0188\n",
      "Epoch 23/300\n",
      "1004/1004 [==============================] - 0s 256us/sample - loss: 1.0114 - val_loss: 1.0207\n",
      "Epoch 24/300\n",
      "1004/1004 [==============================] - 0s 255us/sample - loss: 1.0091 - val_loss: 1.0175\n",
      "Epoch 25/300\n",
      "1004/1004 [==============================] - 0s 261us/sample - loss: 1.0064 - val_loss: 1.0188\n",
      "Epoch 26/300\n",
      "1004/1004 [==============================] - 0s 264us/sample - loss: 1.0038 - val_loss: 1.0166\n",
      "Epoch 27/300\n",
      "1004/1004 [==============================] - 0s 257us/sample - loss: 1.0025 - val_loss: 1.0165\n",
      "Epoch 28/300\n",
      "1004/1004 [==============================] - 0s 289us/sample - loss: 1.0000 - val_loss: 1.0175\n",
      "Epoch 29/300\n",
      "1004/1004 [==============================] - 0s 244us/sample - loss: 0.9972 - val_loss: 1.0177\n",
      "Epoch 30/300\n",
      "1004/1004 [==============================] - 0s 251us/sample - loss: 0.9949 - val_loss: 1.0198\n",
      "Epoch 31/300\n",
      "1004/1004 [==============================] - 0s 248us/sample - loss: 0.9926 - val_loss: 1.0175\n",
      "Epoch 32/300\n",
      "1004/1004 [==============================] - 0s 255us/sample - loss: 0.9901 - val_loss: 1.0226\n",
      "Epoch 33/300\n",
      "1004/1004 [==============================] - 0s 245us/sample - loss: 0.9890 - val_loss: 1.0180\n",
      "Epoch 34/300\n",
      "1004/1004 [==============================] - 0s 262us/sample - loss: 0.9868 - val_loss: 1.0154\n",
      "Epoch 35/300\n",
      "1004/1004 [==============================] - 0s 248us/sample - loss: 0.9853 - val_loss: 1.0210\n",
      "Epoch 36/300\n",
      "1004/1004 [==============================] - 0s 247us/sample - loss: 0.9834 - val_loss: 1.0145\n",
      "Epoch 37/300\n",
      "1004/1004 [==============================] - 0s 253us/sample - loss: 0.9811 - val_loss: 1.0157\n",
      "Epoch 38/300\n",
      "1004/1004 [==============================] - 0s 252us/sample - loss: 0.9788 - val_loss: 1.0203\n",
      "Epoch 39/300\n",
      "1004/1004 [==============================] - 0s 256us/sample - loss: 0.9766 - val_loss: 1.0147\n",
      "Epoch 40/300\n",
      "1004/1004 [==============================] - 0s 250us/sample - loss: 0.9760 - val_loss: 1.0184\n",
      "Epoch 41/300\n",
      "1004/1004 [==============================] - 0s 246us/sample - loss: 0.9740 - val_loss: 1.0154\n",
      "Epoch 42/300\n",
      "1004/1004 [==============================] - 0s 244us/sample - loss: 0.9729 - val_loss: 1.0143\n",
      "Epoch 43/300\n",
      "1004/1004 [==============================] - 0s 261us/sample - loss: 0.9694 - val_loss: 1.0170\n",
      "Epoch 44/300\n",
      "1004/1004 [==============================] - 0s 244us/sample - loss: 0.9688 - val_loss: 1.0146\n",
      "Epoch 45/300\n",
      "1004/1004 [==============================] - 0s 244us/sample - loss: 0.9671 - val_loss: 1.0157\n",
      "Epoch 46/300\n",
      "1004/1004 [==============================] - 0s 246us/sample - loss: 0.9678 - val_loss: 1.0159\n",
      "Epoch 47/300\n",
      "1004/1004 [==============================] - 0s 255us/sample - loss: 0.9639 - val_loss: 1.0172\n",
      "Epoch 48/300\n",
      "1004/1004 [==============================] - 0s 244us/sample - loss: 0.9608 - val_loss: 1.0159\n",
      "Epoch 49/300\n",
      "1004/1004 [==============================] - 0s 255us/sample - loss: 0.9608 - val_loss: 1.0138\n",
      "Epoch 50/300\n",
      "1004/1004 [==============================] - ETA: 0s - loss: 0.957 - 0s 398us/sample - loss: 0.9592 - val_loss: 1.0149\n",
      "Epoch 51/300\n",
      "1004/1004 [==============================] - 0s 269us/sample - loss: 0.9568 - val_loss: 1.0164\n",
      "Epoch 52/300\n",
      "1004/1004 [==============================] - 0s 260us/sample - loss: 0.9555 - val_loss: 1.0129\n",
      "Epoch 53/300\n",
      "1004/1004 [==============================] - 0s 254us/sample - loss: 0.9522 - val_loss: 1.0127\n",
      "Epoch 54/300\n",
      "1004/1004 [==============================] - 0s 265us/sample - loss: 0.9526 - val_loss: 1.0122\n",
      "Epoch 55/300\n",
      "1004/1004 [==============================] - 0s 261us/sample - loss: 0.9495 - val_loss: 1.0153\n",
      "Epoch 56/300\n",
      "1004/1004 [==============================] - 0s 266us/sample - loss: 0.9492 - val_loss: 1.0184\n",
      "Epoch 57/300\n",
      "1004/1004 [==============================] - 0s 259us/sample - loss: 0.9475 - val_loss: 1.0137\n",
      "Epoch 58/300\n",
      "1004/1004 [==============================] - 0s 258us/sample - loss: 0.9451 - val_loss: 1.0141\n",
      "Epoch 59/300\n",
      "1004/1004 [==============================] - 0s 265us/sample - loss: 0.9440 - val_loss: 1.0167\n",
      "Epoch 60/300\n",
      "1004/1004 [==============================] - 0s 264us/sample - loss: 0.9434 - val_loss: 1.0161\n",
      "Epoch 61/300\n",
      "1004/1004 [==============================] - 0s 287us/sample - loss: 0.9395 - val_loss: 1.0148\n",
      "Epoch 62/300\n",
      "1004/1004 [==============================] - 0s 255us/sample - loss: 0.9399 - val_loss: 1.0150\n",
      "Epoch 63/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1004/1004 [==============================] - 0s 251us/sample - loss: 0.9372 - val_loss: 1.0135\n",
      "Epoch 64/300\n",
      "1004/1004 [==============================] - 0s 252us/sample - loss: 0.9368 - val_loss: 1.0091\n",
      "Epoch 65/300\n",
      "1004/1004 [==============================] - 0s 246us/sample - loss: 0.9338 - val_loss: 1.0187\n",
      "Epoch 66/300\n",
      "1004/1004 [==============================] - 0s 266us/sample - loss: 0.9323 - val_loss: 1.0141\n",
      "Epoch 67/300\n",
      "1004/1004 [==============================] - 0s 243us/sample - loss: 0.9323 - val_loss: 1.0188\n",
      "Epoch 68/300\n",
      "1004/1004 [==============================] - 0s 255us/sample - loss: 0.9298 - val_loss: 1.0191\n",
      "Epoch 69/300\n",
      "1004/1004 [==============================] - 0s 246us/sample - loss: 0.9288 - val_loss: 1.0144\n",
      "Epoch 70/300\n",
      "1004/1004 [==============================] - 0s 250us/sample - loss: 0.9261 - val_loss: 1.0157\n",
      "Epoch 71/300\n",
      "1004/1004 [==============================] - 0s 251us/sample - loss: 0.9266 - val_loss: 1.0140\n",
      "Epoch 72/300\n",
      "1004/1004 [==============================] - 0s 273us/sample - loss: 0.9240 - val_loss: 1.0133\n",
      "Epoch 73/300\n",
      "1004/1004 [==============================] - 0s 254us/sample - loss: 0.9239 - val_loss: 1.0166\n",
      "Epoch 74/300\n",
      "1004/1004 [==============================] - 0s 252us/sample - loss: 0.9216 - val_loss: 1.0185\n",
      "Train on 1112 samples, validate on 287 samples\n",
      "Epoch 1/300\n",
      "1112/1112 [==============================] - 1s 914us/sample - loss: 0.8964 - val_loss: 0.7353\n",
      "Epoch 2/300\n",
      "1112/1112 [==============================] - 0s 248us/sample - loss: 0.7623 - val_loss: 0.7281\n",
      "Epoch 3/300\n",
      "1112/1112 [==============================] - 0s 248us/sample - loss: 0.7599 - val_loss: 0.7296\n",
      "Epoch 4/300\n",
      "1112/1112 [==============================] - 0s 368us/sample - loss: 0.7600 - val_loss: 0.7268\n",
      "Epoch 5/300\n",
      "1112/1112 [==============================] - 0s 277us/sample - loss: 0.7591 - val_loss: 0.7270\n",
      "Epoch 6/300\n",
      "1112/1112 [==============================] - 0s 308us/sample - loss: 0.7587 - val_loss: 0.7287\n",
      "Epoch 7/300\n",
      "1112/1112 [==============================] - 0s 285us/sample - loss: 0.7581 - val_loss: 0.7297\n",
      "Epoch 8/300\n",
      "1112/1112 [==============================] - 0s 255us/sample - loss: 0.7579 - val_loss: 0.7264\n",
      "Epoch 9/300\n",
      "1112/1112 [==============================] - 0s 261us/sample - loss: 0.7574 - val_loss: 0.7265\n",
      "Epoch 10/300\n",
      "1112/1112 [==============================] - 0s 283us/sample - loss: 0.7572 - val_loss: 0.7265\n",
      "Epoch 11/300\n",
      "1112/1112 [==============================] - 0s 257us/sample - loss: 0.7569 - val_loss: 0.7285\n",
      "Epoch 12/300\n",
      "1112/1112 [==============================] - 0s 255us/sample - loss: 0.7560 - val_loss: 0.7275\n",
      "Epoch 13/300\n",
      "1112/1112 [==============================] - 0s 277us/sample - loss: 0.7557 - val_loss: 0.7257\n",
      "Epoch 14/300\n",
      "1112/1112 [==============================] - 0s 259us/sample - loss: 0.7556 - val_loss: 0.7280\n",
      "Epoch 15/300\n",
      "1112/1112 [==============================] - 0s 250us/sample - loss: 0.7557 - val_loss: 0.7264\n",
      "Epoch 16/300\n",
      "1112/1112 [==============================] - 0s 252us/sample - loss: 0.7548 - val_loss: 0.7282\n",
      "Epoch 17/300\n",
      "1112/1112 [==============================] - 0s 265us/sample - loss: 0.7547 - val_loss: 0.7285\n",
      "Epoch 18/300\n",
      "1112/1112 [==============================] - 0s 253us/sample - loss: 0.7542 - val_loss: 0.7271\n",
      "Epoch 19/300\n",
      "1112/1112 [==============================] - 0s 254us/sample - loss: 0.7534 - val_loss: 0.7281\n",
      "Epoch 20/300\n",
      "1112/1112 [==============================] - 0s 257us/sample - loss: 0.7530 - val_loss: 0.7279\n",
      "Epoch 21/300\n",
      "1112/1112 [==============================] - 0s 252us/sample - loss: 0.7531 - val_loss: 0.7256\n",
      "Epoch 22/300\n",
      "1112/1112 [==============================] - 0s 245us/sample - loss: 0.7521 - val_loss: 0.7255\n",
      "Epoch 23/300\n",
      "1112/1112 [==============================] - 0s 246us/sample - loss: 0.7517 - val_loss: 0.7277\n",
      "Epoch 24/300\n",
      "1112/1112 [==============================] - 0s 249us/sample - loss: 0.7522 - val_loss: 0.7259\n",
      "Epoch 25/300\n",
      "1112/1112 [==============================] - 0s 247us/sample - loss: 0.7516 - val_loss: 0.7280\n",
      "Epoch 26/300\n",
      "1112/1112 [==============================] - 0s 279us/sample - loss: 0.7517 - val_loss: 0.7289\n",
      "Epoch 27/300\n",
      "1112/1112 [==============================] - 0s 311us/sample - loss: 0.7511 - val_loss: 0.7273\n",
      "Epoch 28/300\n",
      "1112/1112 [==============================] - 0s 269us/sample - loss: 0.7509 - val_loss: 0.7271\n",
      "Epoch 29/300\n",
      "1112/1112 [==============================] - 0s 286us/sample - loss: 0.7508 - val_loss: 0.7273\n",
      "Epoch 30/300\n",
      "1112/1112 [==============================] - 0s 261us/sample - loss: 0.7503 - val_loss: 0.7264\n",
      "Epoch 31/300\n",
      "1112/1112 [==============================] - 0s 265us/sample - loss: 0.7505 - val_loss: 0.7264\n",
      "Epoch 32/300\n",
      "1112/1112 [==============================] - 0s 243us/sample - loss: 0.7499 - val_loss: 0.7296\n",
      "Train on 3531 samples, validate on 884 samples\n",
      "Epoch 1/300\n",
      "3531/3531 [==============================] - 2s 554us/sample - loss: 0.6683 - val_loss: 0.6286\n",
      "Epoch 2/300\n",
      "3531/3531 [==============================] - 1s 314us/sample - loss: 0.6308 - val_loss: 0.6260\n",
      "Epoch 3/300\n",
      "3531/3531 [==============================] - 1s 339us/sample - loss: 0.6230 - val_loss: 0.6267\n",
      "Epoch 4/300\n",
      "3531/3531 [==============================] - 1s 319us/sample - loss: 0.6157 - val_loss: 0.6300\n",
      "Epoch 5/300\n",
      "3531/3531 [==============================] - 1s 319us/sample - loss: 0.6102 - val_loss: 0.6247\n",
      "Epoch 6/300\n",
      "3531/3531 [==============================] - 1s 318us/sample - loss: 0.6040 - val_loss: 0.6296\n",
      "Epoch 7/300\n",
      "3531/3531 [==============================] - 1s 311us/sample - loss: 0.5984 - val_loss: 0.6240\n",
      "Epoch 8/300\n",
      "3531/3531 [==============================] - 1s 320us/sample - loss: 0.5919 - val_loss: 0.6233\n",
      "Epoch 9/300\n",
      "3531/3531 [==============================] - 1s 306us/sample - loss: 0.5848 - val_loss: 0.6264\n",
      "Epoch 10/300\n",
      "3531/3531 [==============================] - 1s 307us/sample - loss: 0.5770 - val_loss: 0.6271\n",
      "Epoch 11/300\n",
      "3531/3531 [==============================] - 1s 306us/sample - loss: 0.5695 - val_loss: 0.6249\n",
      "Epoch 12/300\n",
      "3531/3531 [==============================] - 1s 349us/sample - loss: 0.5595 - val_loss: 0.6408\n",
      "Epoch 13/300\n",
      "3531/3531 [==============================] - 1s 323us/sample - loss: 0.5511 - val_loss: 0.6332\n",
      "Epoch 14/300\n",
      "3531/3531 [==============================] - 1s 333us/sample - loss: 0.5424 - val_loss: 0.6393\n",
      "Epoch 15/300\n",
      "3531/3531 [==============================] - 1s 328us/sample - loss: 0.5320 - val_loss: 0.6417\n",
      "Epoch 16/300\n",
      "3531/3531 [==============================] - 1s 339us/sample - loss: 0.5209 - val_loss: 0.6401\n",
      "Epoch 17/300\n",
      "3531/3531 [==============================] - 1s 313us/sample - loss: 0.5103 - val_loss: 0.6465\n",
      "Epoch 18/300\n",
      "3531/3531 [==============================] - 1s 308us/sample - loss: 0.4974 - val_loss: 0.6473\n",
      "Train on 580 samples, validate on 146 samples\n",
      "Epoch 1/300\n",
      "580/580 [==============================] - 1s 2ms/sample - loss: 0.5194 - val_loss: 0.5031\n",
      "Epoch 2/300\n",
      "580/580 [==============================] - 0s 347us/sample - loss: 0.4272 - val_loss: 0.4709\n",
      "Epoch 3/300\n",
      "580/580 [==============================] - 0s 605us/sample - loss: 0.4040 - val_loss: 0.4690\n",
      "Epoch 4/300\n",
      "580/580 [==============================] - 0s 385us/sample - loss: 0.3911 - val_loss: 0.4618\n",
      "Epoch 5/300\n",
      "580/580 [==============================] - 0s 404us/sample - loss: 0.3800 - val_loss: 0.4603\n",
      "Epoch 6/300\n",
      "580/580 [==============================] - 0s 382us/sample - loss: 0.3749 - val_loss: 0.4573\n",
      "Epoch 7/300\n",
      "580/580 [==============================] - 0s 382us/sample - loss: 0.3697 - val_loss: 0.4572\n",
      "Epoch 8/300\n",
      "580/580 [==============================] - 0s 385us/sample - loss: 0.3605 - val_loss: 0.4581\n",
      "Epoch 9/300\n",
      "580/580 [==============================] - 0s 394us/sample - loss: 0.3529 - val_loss: 0.4561\n",
      "Epoch 10/300\n",
      "580/580 [==============================] - 0s 373us/sample - loss: 0.3466 - val_loss: 0.4598\n",
      "Epoch 11/300\n",
      "580/580 [==============================] - 0s 409us/sample - loss: 0.3426 - val_loss: 0.4580\n",
      "Epoch 12/300\n",
      "580/580 [==============================] - 0s 378us/sample - loss: 0.3394 - val_loss: 0.4600\n",
      "Epoch 13/300\n",
      "580/580 [==============================] - 0s 387us/sample - loss: 0.3290 - val_loss: 0.4561\n",
      "Epoch 14/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "580/580 [==============================] - 0s 382us/sample - loss: 0.3274 - val_loss: 0.4579\n",
      "Epoch 15/300\n",
      "580/580 [==============================] - 0s 382us/sample - loss: 0.3179 - val_loss: 0.4565\n",
      "Epoch 16/300\n",
      "580/580 [==============================] - 0s 383us/sample - loss: 0.3116 - val_loss: 0.4562\n",
      "Epoch 17/300\n",
      "580/580 [==============================] - 0s 380us/sample - loss: 0.3050 - val_loss: 0.4557\n",
      "Epoch 18/300\n",
      "580/580 [==============================] - 0s 385us/sample - loss: 0.3045 - val_loss: 0.4570\n",
      "Epoch 19/300\n",
      "580/580 [==============================] - 0s 444us/sample - loss: 0.2965 - val_loss: 0.4573\n",
      "Epoch 20/300\n",
      "580/580 [==============================] - 0s 375us/sample - loss: 0.2924 - val_loss: 0.4550\n",
      "Epoch 21/300\n",
      "580/580 [==============================] - 0s 418us/sample - loss: 0.2864 - val_loss: 0.4611\n",
      "Epoch 22/300\n",
      "580/580 [==============================] - 0s 370us/sample - loss: 0.2837 - val_loss: 0.4591\n",
      "Epoch 23/300\n",
      "580/580 [==============================] - 0s 373us/sample - loss: 0.2776 - val_loss: 0.4586\n",
      "Epoch 24/300\n",
      "580/580 [==============================] - 0s 368us/sample - loss: 0.2725 - val_loss: 0.4583\n",
      "Epoch 25/300\n",
      "580/580 [==============================] - 0s 373us/sample - loss: 0.2698 - val_loss: 0.4615\n",
      "Epoch 26/300\n",
      "580/580 [==============================] - 0s 365us/sample - loss: 0.2671 - val_loss: 0.4596\n",
      "Epoch 27/300\n",
      "580/580 [==============================] - 0s 430us/sample - loss: 0.2601 - val_loss: 0.4604\n",
      "Epoch 28/300\n",
      "580/580 [==============================] - 0s 370us/sample - loss: 0.2587 - val_loss: 0.4604\n",
      "Epoch 29/300\n",
      "580/580 [==============================] - 0s 366us/sample - loss: 0.2525 - val_loss: 0.4616\n",
      "Epoch 30/300\n",
      "580/580 [==============================] - 0s 392us/sample - loss: 0.2544 - val_loss: 0.4661\n",
      "Train on 3542 samples, validate on 873 samples\n",
      "Epoch 1/300\n",
      "3542/3542 [==============================] - 2s 569us/sample - loss: 0.6652 - val_loss: 0.6389\n",
      "Epoch 2/300\n",
      "3542/3542 [==============================] - 1s 329us/sample - loss: 0.6279 - val_loss: 0.6379\n",
      "Epoch 3/300\n",
      "3542/3542 [==============================] - 1s 317us/sample - loss: 0.6189 - val_loss: 0.6360\n",
      "Epoch 4/300\n",
      "3542/3542 [==============================] - 1s 326us/sample - loss: 0.6125 - val_loss: 0.6323\n",
      "Epoch 5/300\n",
      "3542/3542 [==============================] - 1s 321us/sample - loss: 0.6034 - val_loss: 0.6322\n",
      "Epoch 6/300\n",
      "3542/3542 [==============================] - 1s 339us/sample - loss: 0.5977 - val_loss: 0.6335\n",
      "Epoch 7/300\n",
      "3542/3542 [==============================] - 1s 393us/sample - loss: 0.5918 - val_loss: 0.6345\n",
      "Epoch 8/300\n",
      "3542/3542 [==============================] - 1s 344us/sample - loss: 0.5831 - val_loss: 0.6336\n",
      "Epoch 9/300\n",
      "3542/3542 [==============================] - 1s 373us/sample - loss: 0.5772 - val_loss: 0.6351\n",
      "Epoch 10/300\n",
      "3542/3542 [==============================] - 1s 338us/sample - loss: 0.5665 - val_loss: 0.6406\n",
      "Epoch 11/300\n",
      "3542/3542 [==============================] - 1s 338us/sample - loss: 0.5590 - val_loss: 0.6388\n",
      "Epoch 12/300\n",
      "3542/3542 [==============================] - 1s 348us/sample - loss: 0.5471 - val_loss: 0.6489\n",
      "Epoch 13/300\n",
      "3542/3542 [==============================] - 1s 349us/sample - loss: 0.5387 - val_loss: 0.6445\n",
      "Epoch 14/300\n",
      "3542/3542 [==============================] - 1s 325us/sample - loss: 0.5283 - val_loss: 0.6466\n",
      "Epoch 15/300\n",
      "3542/3542 [==============================] - 1s 324us/sample - loss: 0.5171 - val_loss: 0.6492\n",
      "Train on 570 samples, validate on 156 samples\n",
      "Epoch 1/300\n",
      "570/570 [==============================] - 1s 2ms/sample - loss: 0.5441 - val_loss: 0.4535\n",
      "Epoch 2/300\n",
      "570/570 [==============================] - 0s 362us/sample - loss: 0.4480 - val_loss: 0.4284\n",
      "Epoch 3/300\n",
      "570/570 [==============================] - 0s 483us/sample - loss: 0.4122 - val_loss: 0.4215\n",
      "Epoch 4/300\n",
      "570/570 [==============================] - 0s 404us/sample - loss: 0.3987 - val_loss: 0.4190\n",
      "Epoch 5/300\n",
      "570/570 [==============================] - 0s 448us/sample - loss: 0.3881 - val_loss: 0.4176\n",
      "Epoch 6/300\n",
      "570/570 [==============================] - 0s 500us/sample - loss: 0.3783 - val_loss: 0.4190\n",
      "Epoch 7/300\n",
      "570/570 [==============================] - 0s 432us/sample - loss: 0.3732 - val_loss: 0.4184\n",
      "Epoch 8/300\n",
      "570/570 [==============================] - 0s 633us/sample - loss: 0.3663 - val_loss: 0.4197\n",
      "Epoch 9/300\n",
      "570/570 [==============================] - 0s 402us/sample - loss: 0.3586 - val_loss: 0.4177\n",
      "Epoch 10/300\n",
      "570/570 [==============================] - 0s 373us/sample - loss: 0.3533 - val_loss: 0.4201\n",
      "Epoch 11/300\n",
      "570/570 [==============================] - 0s 390us/sample - loss: 0.3514 - val_loss: 0.4166\n",
      "Epoch 12/300\n",
      "570/570 [==============================] - 0s 369us/sample - loss: 0.3441 - val_loss: 0.4180\n",
      "Epoch 13/300\n",
      "570/570 [==============================] - 0s 360us/sample - loss: 0.3361 - val_loss: 0.4189\n",
      "Epoch 14/300\n",
      "570/570 [==============================] - 0s 366us/sample - loss: 0.3326 - val_loss: 0.4218\n",
      "Epoch 15/300\n",
      "570/570 [==============================] - 0s 378us/sample - loss: 0.3219 - val_loss: 0.4213\n",
      "Epoch 16/300\n",
      "570/570 [==============================] - 0s 366us/sample - loss: 0.3236 - val_loss: 0.4278\n",
      "Epoch 17/300\n",
      "570/570 [==============================] - 0s 380us/sample - loss: 0.3143 - val_loss: 0.4242\n",
      "Epoch 18/300\n",
      "570/570 [==============================] - 0s 366us/sample - loss: 0.3078 - val_loss: 0.4247\n",
      "Epoch 19/300\n",
      "570/570 [==============================] - 0s 369us/sample - loss: 0.3055 - val_loss: 0.4306\n",
      "Epoch 20/300\n",
      "570/570 [==============================] - 0s 383us/sample - loss: 0.3000 - val_loss: 0.4300\n",
      "Epoch 21/300\n",
      "570/570 [==============================] - 0s 373us/sample - loss: 0.2913 - val_loss: 0.4358\n",
      "Train on 3535 samples, validate on 880 samples\n",
      "Epoch 1/300\n",
      "3535/3535 [==============================] - 2s 524us/sample - loss: 0.6671 - val_loss: 0.6379\n",
      "Epoch 2/300\n",
      "3535/3535 [==============================] - 1s 293us/sample - loss: 0.6269 - val_loss: 0.6375\n",
      "Epoch 3/300\n",
      "3535/3535 [==============================] - 1s 293us/sample - loss: 0.6186 - val_loss: 0.6344\n",
      "Epoch 4/300\n",
      "3535/3535 [==============================] - 1s 344us/sample - loss: 0.6109 - val_loss: 0.6350\n",
      "Epoch 5/300\n",
      "3535/3535 [==============================] - 1s 317us/sample - loss: 0.6044 - val_loss: 0.6341\n",
      "Epoch 6/300\n",
      "3535/3535 [==============================] - 1s 313us/sample - loss: 0.5986 - val_loss: 0.6343\n",
      "Epoch 7/300\n",
      "3535/3535 [==============================] - 1s 326us/sample - loss: 0.5911 - val_loss: 0.6363\n",
      "Epoch 8/300\n",
      "3535/3535 [==============================] - 1s 309us/sample - loss: 0.5824 - val_loss: 0.6352\n",
      "Epoch 9/300\n",
      "3535/3535 [==============================] - 1s 340us/sample - loss: 0.5756 - val_loss: 0.6385\n",
      "Epoch 10/300\n",
      "3535/3535 [==============================] - 1s 306us/sample - loss: 0.5693 - val_loss: 0.6408\n",
      "Epoch 11/300\n",
      "3535/3535 [==============================] - 1s 318us/sample - loss: 0.5590 - val_loss: 0.6438\n",
      "Epoch 12/300\n",
      "3535/3535 [==============================] - 1s 371us/sample - loss: 0.5479 - val_loss: 0.6481\n",
      "Epoch 13/300\n",
      "3535/3535 [==============================] - 1s 324us/sample - loss: 0.5396 - val_loss: 0.6535\n",
      "Epoch 14/300\n",
      "3535/3535 [==============================] - 1s 372us/sample - loss: 0.5283 - val_loss: 0.6492\n",
      "Epoch 15/300\n",
      "3535/3535 [==============================] - 1s 394us/sample - loss: 0.5163 - val_loss: 0.6567\n",
      "Train on 577 samples, validate on 149 samples\n",
      "Epoch 1/300\n",
      "577/577 [==============================] - 2s 3ms/sample - loss: 0.5298 - val_loss: 0.4659\n",
      "Epoch 2/300\n",
      "577/577 [==============================] - 0s 534us/sample - loss: 0.4443 - val_loss: 0.4249\n",
      "Epoch 3/300\n",
      "577/577 [==============================] - 0s 384us/sample - loss: 0.4172 - val_loss: 0.4143\n",
      "Epoch 4/300\n",
      "577/577 [==============================] - 0s 382us/sample - loss: 0.4052 - val_loss: 0.4222\n",
      "Epoch 5/300\n",
      "577/577 [==============================] - 0s 671us/sample - loss: 0.3975 - val_loss: 0.4107\n",
      "Epoch 6/300\n",
      "577/577 [==============================] - 0s 643us/sample - loss: 0.3865 - val_loss: 0.4099\n",
      "Epoch 7/300\n",
      "577/577 [==============================] - 0s 392us/sample - loss: 0.3803 - val_loss: 0.4097\n",
      "Epoch 8/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "577/577 [==============================] - 0s 455us/sample - loss: 0.3757 - val_loss: 0.4098\n",
      "Epoch 9/300\n",
      "577/577 [==============================] - 0s 432us/sample - loss: 0.3668 - val_loss: 0.4099\n",
      "Epoch 10/300\n",
      "577/577 [==============================] - 0s 446us/sample - loss: 0.3606 - val_loss: 0.4112\n",
      "Epoch 11/300\n",
      "577/577 [==============================] - 0s 442us/sample - loss: 0.3555 - val_loss: 0.4110\n",
      "Epoch 12/300\n",
      "577/577 [==============================] - 0s 434us/sample - loss: 0.3495 - val_loss: 0.4112\n",
      "Epoch 13/300\n",
      "577/577 [==============================] - 0s 365us/sample - loss: 0.3436 - val_loss: 0.4155\n",
      "Epoch 14/300\n",
      "577/577 [==============================] - 0s 370us/sample - loss: 0.3375 - val_loss: 0.4139\n",
      "Epoch 15/300\n",
      "577/577 [==============================] - 0s 373us/sample - loss: 0.3349 - val_loss: 0.4159\n",
      "Epoch 16/300\n",
      "577/577 [==============================] - 0s 418us/sample - loss: 0.3270 - val_loss: 0.4136\n",
      "Epoch 17/300\n",
      "577/577 [==============================] - 0s 370us/sample - loss: 0.3226 - val_loss: 0.4216\n",
      "Train on 3515 samples, validate on 900 samples\n",
      "Epoch 1/300\n",
      "3515/3515 [==============================] - 2s 622us/sample - loss: 0.6697 - val_loss: 0.6194\n",
      "Epoch 2/300\n",
      "3515/3515 [==============================] - 1s 307us/sample - loss: 0.6323 - val_loss: 0.6188\n",
      "Epoch 3/300\n",
      "3515/3515 [==============================] - 1s 313us/sample - loss: 0.6220 - val_loss: 0.6162\n",
      "Epoch 4/300\n",
      "3515/3515 [==============================] - 1s 341us/sample - loss: 0.6166 - val_loss: 0.6158\n",
      "Epoch 5/300\n",
      "3515/3515 [==============================] - 1s 308us/sample - loss: 0.6112 - val_loss: 0.6116\n",
      "Epoch 6/300\n",
      "3515/3515 [==============================] - 1s 304us/sample - loss: 0.6031 - val_loss: 0.6148\n",
      "Epoch 7/300\n",
      "3515/3515 [==============================] - 1s 308us/sample - loss: 0.5969 - val_loss: 0.6110\n",
      "Epoch 8/300\n",
      "3515/3515 [==============================] - 1s 310us/sample - loss: 0.5903 - val_loss: 0.6149\n",
      "Epoch 9/300\n",
      "3515/3515 [==============================] - 1s 295us/sample - loss: 0.5812 - val_loss: 0.6145\n",
      "Epoch 10/300\n",
      "3515/3515 [==============================] - 1s 296us/sample - loss: 0.5724 - val_loss: 0.6183\n",
      "Epoch 11/300\n",
      "3515/3515 [==============================] - 1s 297us/sample - loss: 0.5651 - val_loss: 0.6288\n",
      "Epoch 12/300\n",
      "3515/3515 [==============================] - 1s 295us/sample - loss: 0.5551 - val_loss: 0.6194\n",
      "Epoch 13/300\n",
      "3515/3515 [==============================] - 1s 339us/sample - loss: 0.5462 - val_loss: 0.6203\n",
      "Epoch 14/300\n",
      "3515/3515 [==============================] - 1s 311us/sample - loss: 0.5354 - val_loss: 0.6292\n",
      "Epoch 15/300\n",
      "3515/3515 [==============================] - 1s 303us/sample - loss: 0.5260 - val_loss: 0.6316\n",
      "Epoch 16/300\n",
      "3515/3515 [==============================] - 1s 317us/sample - loss: 0.5136 - val_loss: 0.6333\n",
      "Epoch 17/300\n",
      "3515/3515 [==============================] - 1s 296us/sample - loss: 0.5038 - val_loss: 0.6325\n",
      "Train on 599 samples, validate on 127 samples\n",
      "Epoch 1/300\n",
      "599/599 [==============================] - 1s 2ms/sample - loss: 0.5233 - val_loss: 0.4363\n",
      "Epoch 2/300\n",
      "599/599 [==============================] - 0s 336us/sample - loss: 0.4380 - val_loss: 0.4088\n",
      "Epoch 3/300\n",
      "599/599 [==============================] - 0s 346us/sample - loss: 0.4156 - val_loss: 0.3997\n",
      "Epoch 4/300\n",
      "599/599 [==============================] - 0s 350us/sample - loss: 0.4064 - val_loss: 0.3964\n",
      "Epoch 5/300\n",
      "599/599 [==============================] - 0s 378us/sample - loss: 0.3971 - val_loss: 0.3977\n",
      "Epoch 6/300\n",
      "599/599 [==============================] - 0s 363us/sample - loss: 0.3906 - val_loss: 0.3878\n",
      "Epoch 7/300\n",
      "599/599 [==============================] - 0s 350us/sample - loss: 0.3835 - val_loss: 0.4009\n",
      "Epoch 8/300\n",
      "599/599 [==============================] - 0s 351us/sample - loss: 0.3770 - val_loss: 0.3881\n",
      "Epoch 9/300\n",
      "599/599 [==============================] - 0s 356us/sample - loss: 0.3691 - val_loss: 0.4034\n",
      "Epoch 10/300\n",
      "599/599 [==============================] - 0s 351us/sample - loss: 0.3642 - val_loss: 0.3888\n",
      "Epoch 11/300\n",
      "599/599 [==============================] - 0s 355us/sample - loss: 0.3573 - val_loss: 0.3995\n",
      "Epoch 12/300\n",
      "599/599 [==============================] - 0s 345us/sample - loss: 0.3529 - val_loss: 0.3949\n",
      "Epoch 13/300\n",
      "599/599 [==============================] - 0s 343us/sample - loss: 0.3456 - val_loss: 0.3997\n",
      "Epoch 14/300\n",
      "599/599 [==============================] - 0s 594us/sample - loss: 0.3403 - val_loss: 0.4037\n",
      "Epoch 15/300\n",
      "599/599 [==============================] - 0s 360us/sample - loss: 0.3330 - val_loss: 0.4078\n",
      "Epoch 16/300\n",
      "599/599 [==============================] - 0s 381us/sample - loss: 0.3283 - val_loss: 0.3982\n",
      "Train on 3537 samples, validate on 878 samples\n",
      "Epoch 1/300\n",
      "3537/3537 [==============================] - 2s 609us/sample - loss: 0.6672 - val_loss: 0.6508\n",
      "Epoch 2/300\n",
      "3537/3537 [==============================] - 1s 301us/sample - loss: 0.6266 - val_loss: 0.6460\n",
      "Epoch 3/300\n",
      "3537/3537 [==============================] - 1s 296us/sample - loss: 0.6168 - val_loss: 0.6468\n",
      "Epoch 4/300\n",
      "3537/3537 [==============================] - 1s 340us/sample - loss: 0.6128 - val_loss: 0.6522\n",
      "Epoch 5/300\n",
      "3537/3537 [==============================] - 1s 317us/sample - loss: 0.6076 - val_loss: 0.6423\n",
      "Epoch 6/300\n",
      "3537/3537 [==============================] - 1s 307us/sample - loss: 0.5991 - val_loss: 0.6517\n",
      "Epoch 7/300\n",
      "3537/3537 [==============================] - 1s 330us/sample - loss: 0.5937 - val_loss: 0.6431\n",
      "Epoch 8/300\n",
      "3537/3537 [==============================] - 1s 304us/sample - loss: 0.5853 - val_loss: 0.6496\n",
      "Epoch 9/300\n",
      "3537/3537 [==============================] - 1s 307us/sample - loss: 0.5801 - val_loss: 0.6527\n",
      "Epoch 10/300\n",
      "3537/3537 [==============================] - 1s 314us/sample - loss: 0.5718 - val_loss: 0.6485\n",
      "Epoch 11/300\n",
      "3537/3537 [==============================] - 1s 308us/sample - loss: 0.5650 - val_loss: 0.6519\n",
      "Epoch 12/300\n",
      "3537/3537 [==============================] - 1s 396us/sample - loss: 0.5550 - val_loss: 0.6520\n",
      "Epoch 13/300\n",
      "3537/3537 [==============================] - 1s 323us/sample - loss: 0.5432 - val_loss: 0.6531\n",
      "Epoch 14/300\n",
      "3537/3537 [==============================] - 1s 331us/sample - loss: 0.5340 - val_loss: 0.6560\n",
      "Epoch 15/300\n",
      "3537/3537 [==============================] - 1s 337us/sample - loss: 0.5248 - val_loss: 0.6589\n",
      "Train on 578 samples, validate on 148 samples\n",
      "Epoch 1/300\n",
      "578/578 [==============================] - 1s 2ms/sample - loss: 0.5210 - val_loss: 0.4664\n",
      "Epoch 2/300\n",
      "578/578 [==============================] - 0s 469us/sample - loss: 0.4525 - val_loss: 0.4239\n",
      "Epoch 3/300\n",
      "578/578 [==============================] - 0s 430us/sample - loss: 0.4272 - val_loss: 0.4130\n",
      "Epoch 4/300\n",
      "578/578 [==============================] - 0s 416us/sample - loss: 0.4125 - val_loss: 0.4076\n",
      "Epoch 5/300\n",
      "578/578 [==============================] - 0s 387us/sample - loss: 0.4038 - val_loss: 0.4063\n",
      "Epoch 6/300\n",
      "578/578 [==============================] - 0s 378us/sample - loss: 0.3938 - val_loss: 0.4040\n",
      "Epoch 7/300\n",
      "578/578 [==============================] - 0s 380us/sample - loss: 0.3867 - val_loss: 0.3954\n",
      "Epoch 8/300\n",
      "578/578 [==============================] - 0s 376us/sample - loss: 0.3803 - val_loss: 0.4001\n",
      "Epoch 9/300\n",
      "578/578 [==============================] - 0s 376us/sample - loss: 0.3748 - val_loss: 0.3995\n",
      "Epoch 10/300\n",
      "578/578 [==============================] - 0s 383us/sample - loss: 0.3674 - val_loss: 0.3994\n",
      "Epoch 11/300\n",
      "578/578 [==============================] - 0s 378us/sample - loss: 0.3626 - val_loss: 0.4027\n",
      "Epoch 12/300\n",
      "578/578 [==============================] - 0s 381us/sample - loss: 0.3572 - val_loss: 0.4005\n",
      "Epoch 13/300\n",
      "578/578 [==============================] - 0s 435us/sample - loss: 0.3504 - val_loss: 0.4067\n",
      "Epoch 14/300\n",
      "578/578 [==============================] - 0s 376us/sample - loss: 0.3461 - val_loss: 0.4051\n",
      "Epoch 15/300\n",
      "578/578 [==============================] - 0s 400us/sample - loss: 0.3395 - val_loss: 0.4041\n",
      "Epoch 16/300\n",
      "578/578 [==============================] - 0s 424us/sample - loss: 0.3446 - val_loss: 0.3958\n",
      "Epoch 17/300\n",
      "578/578 [==============================] - 0s 399us/sample - loss: 0.3328 - val_loss: 0.4030\n"
     ]
    }
   ],
   "source": [
    "metrics = full_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gene': {'overall': {'mae': 0.7291975904675908,\n",
       "   'rmse': 1.2871031342506778,\n",
       "   'ndcg': 0.4673787741071691,\n",
       "   'mape': 4.422155277734545,\n",
       "   'r2': 0.24244890230809224,\n",
       "   'pearson': 0.524190051690691,\n",
       "   'acc': 0.3317444264831185},\n",
       "  'annual': {'mae': array([0.41338131, 0.57355205, 0.74987375, 0.88439481, 1.02478603]),\n",
       "   'rmse': array([0.75304537, 0.95914567, 1.31467579, 1.48858523, 1.6651921 ]),\n",
       "   'ndcg': array([0.4623395 , 0.43362874, 0.14620785, 0.19055866, 0.15106367]),\n",
       "   'mape': array([3.42139588, 2.99739742, 3.24076012, 6.14907401, 6.30214895]),\n",
       "   'r2': array([0.42885064, 0.29498014, 0.18880622, 0.05274659, 0.02198769]),\n",
       "   'pearson': array([0.66843551, 0.56805361, 0.47478036, 0.32031706, 0.26847979]),\n",
       "   'acc': array([0.58150882, 0.18466888, 0.23798611, 0.33935394, 0.31520438])}},\n",
       " 'transplant': {'overall': {'mae': 0.7658888161691443,\n",
       "   'rmse': 1.271241089721329,\n",
       "   'ndcg': 0.4850424174440864,\n",
       "   'mape': 3.8585934696807804,\n",
       "   'r2': 0.42290244232094354,\n",
       "   'pearson': 0.6603118550304665,\n",
       "   'acc': 0.33569364692126913},\n",
       "  'annual': {'mae': array([0.75418793, 0.76327487, 0.74002489, 0.76087136, 0.81108503]),\n",
       "   'rmse': array([1.30537114, 1.29968065, 1.21378021, 1.21386833, 1.30992777]),\n",
       "   'ndcg': array([0.03374312, 0.06154707, 0.02096447, 0.09038326, 0.11627429]),\n",
       "   'mape': array([3.22998848, 3.54199572, 3.09160878, 6.11270357, 3.3166708 ]),\n",
       "   'r2': array([0.42537388, 0.4173462 , 0.41210099, 0.45416014, 0.41052196]),\n",
       "   'pearson': array([0.66411839, 0.65997545, 0.65123859, 0.68539273, 0.65389094]),\n",
       "   'acc': array([0.3473911 , 0.34253525, 0.34333486, 0.33611766, 0.30908937])}}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gene': {'overall': {'mae': 0.7297824076242991,\n",
       "   'rmse': 1.288492394560198,\n",
       "   'ndcg': 0.450425276583966,\n",
       "   'mape': 4.379720797710389,\n",
       "   'r2': 0.24096271379741033,\n",
       "   'pearson': 0.5239841606169792,\n",
       "   'acc': 0.33765443023983865},\n",
       "  'annual': {'mae': array([0.41629491, 0.57294854, 0.75242516, 0.88069069, 1.02655274]),\n",
       "   'rmse': array([0.75623629, 0.96455199, 1.31755807, 1.48652634, 1.66621578]),\n",
       "   'ndcg': array([0.42948659, 0.36199947, 0.15691518, 0.21310819, 0.14353297]),\n",
       "   'mape': array([3.27338697, 2.92294838, 3.34444352, 6.13713268, 6.22069244]),\n",
       "   'r2': array([0.42536017, 0.28595323, 0.18519127, 0.05494907, 0.02053747]),\n",
       "   'pearson': array([0.66756681, 0.55964269, 0.47305132, 0.33346943, 0.2686703 ]),\n",
       "   'acc': array([0.57891309, 0.20543948, 0.25955221, 0.3371119 , 0.30725548])}},\n",
       " 'transplant': {'overall': {'mae': 0.7741190799377897,\n",
       "   'rmse': 1.2768263814311376,\n",
       "   'ndcg': 0.521502225186101,\n",
       "   'mape': 3.5752431968110856,\n",
       "   'r2': 0.4178778432693182,\n",
       "   'pearson': 0.6561407872776719,\n",
       "   'acc': 0.3288899995545448},\n",
       "  'annual': {'mae': array([0.75810398, 0.77755318, 0.74153353, 0.77196721, 0.8214375 ]),\n",
       "   'rmse': array([1.30168783, 1.30867047, 1.21911946, 1.21964258, 1.32129062]),\n",
       "   'ndcg': array([0.03279925, 0.06055282, 0.02106838, 0.08558031, 0.14537103]),\n",
       "   'mape': array([3.172992  , 3.58421186, 3.25485806, 4.72289323, 3.14126083]),\n",
       "   'r2': array([0.42892717, 0.4091857 , 0.40697638, 0.44880369, 0.40009249]),\n",
       "   'pearson': array([0.66510822, 0.6508333 , 0.64805261, 0.67886051, 0.64439019]),\n",
       "   'acc': array([0.34565412, 0.33086168, 0.35500877, 0.31784123, 0.2950842 ])}}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
