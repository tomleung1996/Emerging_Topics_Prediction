from bs4 import BeautifulSoup
from model import get_engine, get_session
from model.wos_document import *
from collections import defaultdict, Counter
from tqdm import tqdm
import pickle
import numpy as np


def cal_basic_info(termolator_output_file: str):
    """

    **THIS SHOULD BE CALLED BEFORE FURTHER CALCULATING ANY OTHER PROPERTIES**

    Parse and get basic information from the Termolator output,
    including the name, variants, document frequency, term frequency,
    and the source documents of technical terms

    :param termolator_output_file: The term_instance_map file generated by Termolator
    :return: The basic information of each technical terms
    """
    term_dict = {}
    print('Parsing the basic information from Termolator output file......')

    with open(termolator_output_file, mode='r', encoding='utf-8') as term_map:
        single_term = ''
        for line in term_map:
            single_term += line
            if line.startswith('<term '):
                single_term = line
            if line.startswith('</term>'):
                soup = BeautifulSoup(single_term, 'lxml')
                term = soup.find('term')

                term_name = term['string']
                term_rank = term['rank']
                term_tf = term['total_frequency']
                term_df = term['number_of_files_containing_term']
                term_variants = term['variants'].split('|')
                term_docs = Counter([i['file'].replace('foreground/', '') for i in term.find_all('instance')])

                term_dict[term_name] = {
                    'rank': term_rank,
                    'tf': term_tf,
                    'df': term_df,
                    'variants': term_variants,
                    'docs': term_docs
                }

    print('Done!\n')
    return term_dict


def cal_doc_info(term_dict: dict, session):
    """

    Fetch the document-related information of each technical terms,
    and store them annually in hierarchical dictionaries

    :param term_dict: The term_dict generated by cal_basic_info function,
                      containing basic information of the technical terms
    :param session: The database connection session
    :return: term_dict: The term_dict that enhanced with document-related
                        annual information
    """

    print('Calculating the annual document-related information......')

    for term, info in tqdm(term_dict.items()):
        cited_times = defaultdict(int)
        df = defaultdict(int)
        tf = defaultdict(int)

        authors = defaultdict(set)
        refs = defaultdict(set)
        funds = defaultdict(set)
        affs = defaultdict(set)
        keywords = defaultdict(set)
        keyword_plus = defaultdict(set)
        cats = defaultdict(set)

        for unique_id, term_freq in info['docs'].items():
            doc = session.query(WosDocument).filter(WosDocument.unique_id == unique_id)[0]
            pub_year = doc.pub_year

            cited_times[pub_year] += doc.cited_times
            df[pub_year] += 1
            tf[pub_year] += term_freq

            single_doc_authors = [(i.first_name.strip() + i.last_name.strip()).lower() for i in doc.authors]
            authors[pub_year] = authors[pub_year].union(set(single_doc_authors))

            single_doc_refs = [i.document_md5 for i in doc.references]
            refs[pub_year] = refs[pub_year].union(set(single_doc_refs))

            single_doc_funds = [(i.agent + i.funding_number if i.funding_number else '').lower() for i in doc.fundings]
            funds[pub_year] = funds[pub_year].union(set(single_doc_funds))

            single_doc_affs = [i.address.lower() for j in doc.authors for i in j.affiliations]
            affs[pub_year] = affs[pub_year].union(set(single_doc_affs))

            single_doc_keywords = [i.keyword.lower() for i in doc.keywords]
            keywords[pub_year] = keywords[pub_year].union(set(single_doc_keywords))

            single_doc_keyword_plus = [i.keyword_plus.lower() for i in doc.keyword_plus]
            keyword_plus[pub_year] = keyword_plus[pub_year].union(set(single_doc_keyword_plus))

            single_doc_cats = [i.category.lower() for i in doc.categories]
            cats[pub_year] = cats[pub_year].union(set(single_doc_cats))

        authors, refs, funds, affs, keywords, keyword_plus, cats = map(lambda d: defaultdict(int, {k: len(v) for k, v in d.items()})
                                                                , (authors, refs, funds, affs, keywords, keyword_plus, cats))
        info['tf'] = tf
        info['df'] = df
        info['author_num'] = authors
        info['ref_num'] = refs
        info['fund_num'] = funds
        info['aff_num'] = affs
        info['kw_num'] = keywords
        info['kp_num'] = keyword_plus
        info['cat_num'] = cats
        info['cited_times'] = cited_times

    print('Done!\n')
    return term_dict


def build_np_array(term_dict: dict, delta: float):
    """

    Convert the term_dict to the corresponding numpy array.
    The Emerging-Score is calculated and added in this function.

    :param term_dict: The enhanced term_dict generated by cal_doc_info function
    :param delta: The smoothing factor to avoid divided by zero
    :return: The corresponding numpy array
    """
    features = ['df', 'tf', 'author_num', 'ref_num', 'fund_num', 'aff_num', 'kw_num', 'kp_num', 'cat_num', 'cited_times']
    years = list(range(2003, 2019 + 1))

    term_num = len(term_dict)  # batch size
    feature_num = len(features) + 1  # input_dim (plus Emerging-Score)
    time_span = len(years)  # timesteps

    arr = np.zeros((term_num, time_span, feature_num))

    for i, (term, feats) in enumerate(term_dict.items()):
        for j in range(time_span):
            for k in range(feature_num - 1):
                arr[i][j][k] = feats[features[k]][years[j]]

            # Calculating the Emerging-Score
            if j != 0:  # Emerging-Score is not defined for the first year
                df_pos = features.index('df')
                arr[i][j][-1] = np.log(arr[i][j][df_pos] + delta) * ((arr[i][j][df_pos] + delta) / (arr[i][j-1][df_pos] + delta))

    return arr


if __name__ == '__main__':
    # engine = get_engine(db_url='sqlite:///../data/gene_editing.db')
    # session = get_session(engine)
    #
    # term_dict = cal_basic_info(r'..\data\Termolator_result\gene_editing\gene.term_instance_map')
    # term_dict = cal_doc_info(term_dict, session)
    #
    # print('Dumping term_dict......')
    # pickle.dump(term_dict, open(r'term.dict', mode='wb'))
    # print('Done!')
    #
    # session.close()

    print('Loading term_dict......')
    term_dict = pickle.load(open(r'term.dict', mode='rb'))
    print('Done!\n')

    print('Converting dictionary to numpy array......')
    arr = build_np_array(term_dict, 1.0)
    print('Done!\n')

    print('Dumping numpy array......')
    pickle.dump(arr, open(r'result.array', mode='wb'))
    print('Done!')

    # print(next(iter(term_dict.values())))
    # print(arr[0])
