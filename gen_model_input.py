from bs4 import BeautifulSoup
from model import get_engine, get_session
from model.wos_document import *
from collections import defaultdict, Counter
from tqdm import tqdm
import pickle
import numpy as np
import math
from sqlalchemy.orm import selectinload


def cal_basic_info(termolator_output_file: str, df_threshold:int):
    """

    **THIS SHOULD BE CALLED BEFORE FURTHER CALCULATING ANY OTHER PROPERTIES**

    Parse and get basic information from the Termolator output,
    including the name, variants, document frequency, term frequency,
    and the source documents of technical terms

    :param termolator_output_file: The term_instance_map file generated by Termolator
    :return: The basic information of each technical terms
    """
    term_dict = {}
    print('Parsing the basic information from Termolator output file......')

    with open(termolator_output_file, mode='r', encoding='utf-8') as term_map:
        single_term = ''
        for line in term_map:
            single_term += line
            if line.startswith('<term '):
                single_term = line
            if line.startswith('</term>'):
                soup = BeautifulSoup(single_term, 'lxml')
                term = soup.find('term')

                term_name = term['string']
                term_rank = term['rank']
                term_tf = term['total_frequency']
                term_df = term['number_of_files_containing_term']

                if int(term_df) < df_threshold:
                    continue

                term_variants = term['variants'].split('|')
                term_docs = Counter([i['file'].replace('foreground/', '') for i in term.find_all('instance')])

                term_dict[term_name] = {
                    'rank': term_rank,
                    'tf': term_tf,
                    'df': term_df,
                    'variants': term_variants,
                    'docs': term_docs
                }

    print('Done!\n')
    return term_dict


def cal_doc_info(term_dict: dict, session):
    """

    Fetch the document-related information of each technical terms,
    and store them annually in hierarchical dictionaries

    :param term_dict: The term_dict generated by cal_basic_info function,
                        containing basic information of the technical terms
    :param session: The database connection session
    :return: term_dict: The term_dict that enhanced with document-related
                        annual information
    """

    print('Calculating the annual document-related information......')

    for term, info in tqdm(term_dict.items()):

        unique_ids = list(info['docs'].keys())
        term_freqs = list(info['docs'].values())

        cited_times = defaultdict(int)
        df = defaultdict(int)
        tf = defaultdict(int)

        authors = defaultdict(set)
        refs = defaultdict(set)
        funds = defaultdict(set)
        # affs = defaultdict(set)
        keywords = defaultdict(set)
        keyword_plus = defaultdict(set)
        cats = defaultdict(set)

        n = math.ceil(len(unique_ids) / 999)
        docs = []
        for i in range(n):
            docs += session.query(WosDocument). \
                options(selectinload('authors')). \
                options(selectinload('references')). \
                options(selectinload('fundings')). \
                options(selectinload('keywords')). \
                options(selectinload('keyword_plus')). \
                options(selectinload('categories')). \
                filter(WosDocument.unique_id.in_(unique_ids[i * 999: (i + 1) * 999])).all()

        for i, doc in enumerate(docs):
            pub_year = doc.pub_year

            cited_times[pub_year] += doc.cited_times
            df[pub_year] += 1
            tf[pub_year] += term_freqs[i]

            single_doc_authors = [(i.first_name.strip() + i.last_name.strip()).lower() for i in doc.authors]
            authors[pub_year] = authors[pub_year].union(set(single_doc_authors))

            single_doc_refs = [i.document_md5 for i in doc.references]
            refs[pub_year] = refs[pub_year].union(set(single_doc_refs))

            single_doc_funds = [(i.agent + i.funding_number if i.funding_number else '').lower() for i in doc.fundings]
            funds[pub_year] = funds[pub_year].union(set(single_doc_funds))

            # single_doc_affs = [i.address.lower() for j in doc.authors for i in j.affiliations]
            # affs[pub_year] = affs[pub_year].union(set(single_doc_affs))

            single_doc_keywords = [i.keyword.lower() for i in doc.keywords]
            keywords[pub_year] = keywords[pub_year].union(set(single_doc_keywords))

            single_doc_keyword_plus = [i.keyword_plus.lower() for i in doc.keyword_plus]
            keyword_plus[pub_year] = keyword_plus[pub_year].union(set(single_doc_keyword_plus))

            single_doc_cats = [i.category.lower() for i in doc.categories]
            cats[pub_year] = cats[pub_year].union(set(single_doc_cats))

        authors, refs, funds, keywords, keyword_plus, cats = map(
            lambda d: defaultdict(int, {k: len(v) for k, v in d.items()})
            , (authors, refs, funds, keywords, keyword_plus, cats))
        info['tf'] = tf
        info['df'] = df
        info['author_num'] = authors
        info['ref_num'] = refs
        info['fund_num'] = funds
        # info['aff_num'] = affs
        info['kw_num'] = keywords
        info['kp_num'] = keyword_plus
        info['cat_num'] = cats
        info['cited_times'] = cited_times

    print('Done!\n')
    return term_dict


def build_np_array(term_dict: dict, threshold_dict: dict, delta: float, decay: float):
    """

    Convert the term_dict to the corresponding numpy array.
    The Emerging-Score is calculated and added in this function.
    Also, the classes are assigned according to the percentile
    of Emerging-Score.

    :param term_dict: The enhanced term_dict generated by cal_doc_info function
    :param delta: The smoothing factor to avoid divided by zero
    :return: The corresponding numpy array
    """
    assert decay < 1.0

    # no aff_num for now, no cited_times for reality reason
    features = ['df', 'tf', 'author_num', 'ref_num', 'fund_num', 'kw_num', 'kp_num', 'cat_num']
    years = list(range(2003, 2019 + 1))

    # term_num = len(term_dict)  # batch size
    term_num = len(threshold_dict)
    feature_num = len(features) + 2  # input_dim (plus Emerging-Score and category)
    time_span = len(years)  # timesteps

    arr = np.zeros((term_num, time_span, feature_num))
    ordered_list = []

    new_term_dict = {}
    for term, feats in term_dict.items():
        if term not in threshold_dict:
            continue
        new_term_dict[term] = feats

    for i, (term, feats) in enumerate(new_term_dict.items()):
        ordered_list.append((i, term))
        # if term == 'clustered interspaced short palindromic repeats':
        #     print(i, feats)

        for j in range(time_span):
            for k in range(feature_num - 2):
                arr[i][j][k] = feats[features[k]][years[j]]

    for i, (term, feats) in enumerate(new_term_dict.items()):
        for j in range(time_span):
            for k in range(feature_num - 2):
                # Calculating the Emerging-Score
                df_pos = features.index('df')
                if j != 0:  # Emerging-Score is not defined for the first year (fix: the first year also has Emerging-Score)

                    if arr[i][j-1][df_pos] == 0:
                        # arr[i][j][-2] = np.log(arr[i][j][df_pos] + delta)
                        # To avoid successive zeros differ in Emerging-Score
                        if arr[i][j][df_pos] == 0:
                            arr[i][j][-2] = arr[i][j-1][-2] * decay
                        else:
                            arr[i][j][-2] = np.log(np.sum(arr[i, :j, df_pos] * np.logspace(j, 1, j, base=decay)) + arr[i, j, df_pos] + delta)
                    else:
                        # arr[i][j][-2] = np.log(arr[i][j][df_pos] + delta) * ((arr[i][j][df_pos] + delta) / (arr[i][j - 1][df_pos] + delta))
                        arr[i][j][-2] = np.log(np.sum(arr[i, :j, df_pos] * np.logspace(j, 1, j, base=decay)) + arr[i, j, df_pos] + delta) * ((arr[i][j][df_pos] + delta) / (arr[i][j - 1][df_pos] + delta))


                    # New cumulative method
                    # 用衰减存量增长也不合适，因为后期总体的趋势是下降，越来越难增长
                    # if np.sum(arr[i, :j, df_pos]) == 0:
                    #     # 说明还没增长过
                    #     arr[i][j][-2] = np.log(arr[i, j, df_pos] + delta)
                    # else:
                    #     arr[i][j][-2] = np.log(np.sum(arr[i, :j, df_pos] * np.logspace(j, 1, j, base=decay)) + arr[i, j, df_pos] + delta) * \
                    #                     ((arr[i, j, df_pos]) / (arr[i, j-1, df_pos] + delta))

                    # New relative method
                    # arr[i][j][-2] = (arr[i, j, df_pos] / np.max(arr[:, j, df_pos])) * ((arr[i, j, df_pos]) / (arr[i, j - 1, df_pos] + delta))

                    # Special treatment
                    # Add decay to prevent the Emerging-Score decrease sharply to zero
                    # if arr[i][j][-2] == 0:
                    #     arr[i][j][-2] = arr[i][j - 1][-2] * decay
                else:
                    arr[i][j][-2] = np.log(arr[i, j, df_pos] + delta)
                    # arr[i][j][-2] = (arr[i, j, df_pos] / np.max(arr[:, j, df_pos]))

    pickle.dump(ordered_list, open(r'output/ordered_list.list', mode='wb'))

    es = arr[:, :, -2]
    ps = np.percentile(es, [70, 85, 95], axis=0)

    # Assign classes
    for i in range(term_num):
        for j in range(time_span):
            cur_es = arr[i][j][-2]
            # if cur_es == 0:
            #     arr[i][j][-1] = 0
            # elif 0 < cur_es < ps[0][j]:
            if cur_es < ps[0][j]:
                arr[i][j][-1] = 1  # Below the 70th percentile
            elif ps[0][j] <= cur_es < ps[1][j]:
                arr[i][j][-1] = 2  # Below the 85th percentile
            elif ps[1][j] <= cur_es < ps[2][j]:
                arr[i][j][-1] = 3  # Below the 95th percentile
            else:
                arr[i][j][-1] = 4  # Top classes

    return arr


if __name__ == '__main__':
    # engine = get_engine(db_url='sqlite:///../data/transplant.db')
    # session = get_session(engine)
    #
    threshold_dict = cal_basic_info(r'..\data\Termolator_result\gene_editing\gene.term_instance_map', 0)
    # term_dict = cal_doc_info(term_dict, session)

    # print('Dumping term_dict......')
    # pickle.dump(term_dict, open(r'term.dict', mode='wb'))
    # print('Done!')

    # session.close()

    print('Loading term_dict......')
    term_dict = pickle.load(open(r'output/gene_editing/term.dict', mode='rb'))
    print('Done!\n')

    print('Converting dictionary to numpy array......')
    arr = build_np_array(term_dict, threshold_dict, delta=1.0, decay=0.9)
    print('Done!\n')

    print(arr.shape)

    print('Dumping numpy array......')
    pickle.dump(arr, open(r'output/es_with_decay.array', mode='wb'))
    print('Done!')

    # print(next(iter(term_dict.values())))
    # print(arr[0])
    pass